bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Biological structure and function emerge from scaling unsupervised learning to

250 million protein sequences

Alexander Rives * 1 2 Joshua Meier * 1 Tom Sercu * 1 Siddharth Goyal * 1 Zeming Lin 2 Jason Liu 1

Demi Guo 3 † Myle Ott 1 C. Lawrence Zitnick 1 Jerry Ma 4 † Rob Fergus 2

Abstract

In the ﬁeld of artiﬁcial intelligence, a combination
of scale in data and model capacity enabled by un-
supervised learning has led to major advances in
representation learning and statistical generation.
In the life sciences, the anticipated growth of se-
quencing promises unprecedented data on natural
sequence diversity. Protein language modeling
at the scale of evolution is a logical step toward
predictive and generative artiﬁcial intelligence for
biology. To this end we use unsupervised learn-
ing to train a deep contextual language model on
86 billion amino acids across 250 million protein
sequences spanning evolutionary diversity. The
resulting model contains information about bio-
logical properties in its representations. The rep-
resentations are learned from sequence data alone.
The learned representation space has a multi-scale
organization reﬂecting structure from the level
of biochemical properties of amino acids to re-
mote homology of proteins. Information about
secondary and tertiary structure is encoded in the
representations and can be identiﬁed by linear pro-
jections. Representation learning produces fea-
tures that generalize across a range of applications,
enabling state-of-the-art supervised prediction of
mutational effect and secondary structure, and im-
proving state-of-the-art features for long-range
contact prediction.

1. Introduction
Growth in the number of protein sequences in public
databases has followed an exponential trend over decades,
*Equal contribution †Work performed while at Facebook AI
Research 1Facebook AI Research 2Dept. of Computer Science,
New York University 3Harvard University 4Booth School of Busi-
ness, University of Chicago & Yale Law School. Correspondence
to: Alexander Rives <arives@cs.nyu.edu>. Pre-trained models
available at: <https://github.com/facebookresearch/esm>.

creating a deep view into the breadth and diversity of protein
sequences across life. This data is a promising ground for
studying predictive and generative models for biology using
artiﬁcial intelligence. Our focus here will be to ﬁt a single
model to many diverse sequences from across evolution.
Accordingly we study high-capacity neural networks, inves-
tigating what can be learned about the biology of proteins
from modeling evolutionary data at scale.
The idea that biological function and structure are recorded
in the statistics of protein sequences selected through evolu-
tion has a long history (Yanofsky et al., 1964; Altschuh et al.,
1987; 1988). Out of the possible random perturbations to a
sequence, evolution is biased toward selecting those that are
consistent with ﬁtness (Göbel et al., 1994). The unobserved
variables that determine a protein’s ﬁtness, such as structure,
function, and stability, leave a record in the distribution of
observed natural sequences (Göbel et al., 1994).
Unlocking the information encoded in protein sequence
variation is a longstanding problem in biology. An analo-
gous problem in the ﬁeld of artiﬁcial intelligence is natural
language understanding, where the distributional hypothe-
sis posits that a word’s semantics can be derived from the
contexts in which it appears (Harris, 1954).
Recently, techniques based on self-supervision, a form of
unsupervised learning in which context within the text is
used to predict missing words, have been shown to materi-
alize representations of word meaning that can generalize
across natural language tasks (Collobert & Weston, 2008;
Dai & Le, 2015; Peters et al., 2018; Devlin et al., 2018). The
ability to learn such representations improves signiﬁcantly
with larger training datasets (Baevski et al., 2019; Radford
et al., 2019).
Protein sequences result from a process greatly dissimilar
to natural language. It is uncertain whether the models and
objective functions effective for natural language transfer
across differences between the domains. We explore this
question by training high-capacity Transformer language
models on evolutionary data. We investigate the resulting
unsupervised representations for the presence of biologi-
cal organizing principles, and information about intrinsic

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

biological properties. We ﬁnd metric structure in the repre-
sentation space that accords with organizing principles at
scales from physicochemical to remote homology. We also
ﬁnd that secondary and tertiary protein structure can be iden-
tiﬁed in representations. The structural properties captured
by the representations generalize across folds. We apply the
representations to a range of prediction tasks and ﬁnd that
they improve state-of-art features across the applications.

2. Background
Sequence alignment and search is a longstanding basis
for comparative and statistical analysis of biological se-
quence data. (Altschul et al., 1990; Altschul & Koonin,
1998; Eddy, 1998; Remmert et al., 2011). Search across
large databases containing evolutionary diversity assem-
bles related sequences into a multiple sequence alignment
(MSA). Within sequence families, mutational patterns con-
vey information about functional sites, stability, tertiary con-
tacts, binding, and other properties (Altschuh et al., 1987;
1988; Göbel et al., 1994). Conserved sites correlate with
functional and structural importance (Altschuh et al., 1987).
Local biochemical and structural contexts are reﬂected in
preferences for distinct classes of amino acids (Levitt, 1978).
Covarying mutations have been associated with function,
tertiary contacts, and binding (Göbel et al., 1994).
The prospect of inferring biological structure and function
from evolutionary statistics has motivated development of
machine learning on individual sequence families. Direct
coupling analysis (Lapedes et al., 1999; Thomas et al., 2008;
Weigt et al., 2009) infers constraints on the structure of a
protein by ﬁtting a generative model in the form of a Markov
Random Field (MRF) to the sequences in the protein’s MSA.
Various methods have been developed to ﬁt the MRF (Mor-
cos et al., 2011; Jones et al., 2011; Balakrishnan et al., 2011;
Ekeberg et al., 2013b). The approach can also be used to in-
fer functional constraints (Figliuzzi et al., 2016; Hopf et al.,
2017), and the generative picture can be extended to include
latent variables (Riesselman et al., 2018).
Recently, self-supervision has emerged as a core direction in
artiﬁcial intelligence research. Unlike supervised learning
which requires manual annotation of each datapoint, self-
supervised methods use unlabeled datasets and thus can
exploit far larger amounts of data. Self-supervised learning
uses proxy tasks for training, such as predicting the next
word in a sentence given all previous words (Bengio et al.,
2003; Dai & Le, 2015; Peters et al., 2018; Radford et al.,
2018; 2019) or predicting words that have been masked from
their context (Devlin et al., 2018; Mikolov et al., 2013).
Increasing the dataset size and the model capacity has shown
improvements in the learned representations. In recent work,
self-supervision methods used in conjunction with large data

and high-capacity models produced new state-of-the-art
results approaching human performance on various ques-
tion answering and semantic reasoning benchmarks (Devlin
et al., 2018), and coherent natural text generation (Radford
et al., 2019).
This paper explores self-supervised language modeling ap-
proaches that have demonstrated state-of-the-art perfor-
mance on a range of natural language processing tasks, ap-
plying them to protein data in the form of unlabeled amino
acid sequences. Since protein sequences use a small vocabu-
lary of twenty canonical elements, the modeling problem is
more similar to character-level language models (Mikolov
et al., 2012; Kim et al., 2016) than word-level models. Like
natural language, protein sequences also contain long-range
dependencies, motivating use of architectures that detect
and model distant context (Vaswani et al., 2017).

3. Scaling language models to 250 million

diverse protein sequences

Large protein sequence databases contain diverse sequences
sampled across life.
In our experiments we explore
datasets with up to 250 million sequences of the Uniparc
database (The UniProt Consortium, 2007) which has 86
billion amino acids. This data is comparable in size to large
text datasets that are being used to train high-capacity neu-
ral network architectures on natural language (Devlin et al.,
2018; Radford et al., 2019). To model the data of evolu-
tion with ﬁdelity, neural network architectures must have
capacity and inductive biases to represent its breadth and
diversity.
We investigate the Transformer (Vaswani et al., 2017), which
has emerged as a powerful general-purpose model architec-
ture for representation learning and generative modeling,
outperforming recurrent and convolutional architectures in
natural language settings. We use a deep Transformer (De-
vlin et al., 2018), taking as input amino acid character se-
quences.
The Transformer processes inputs through a series of blocks
that alternate self-attention with feed-forward connections.
Self-attention allows the network to build up complex repre-
sentations that incorporate context from across the sequence.
Since self-attention explicitly constructs pairwise interac-
tions between all positions in the sequence, the Transformer
architecture directly represents residue-residue interactions.
We train models using the masked language modeling objec-
tive (Devlin et al., 2018). Each input sequence is corrupted
by replacing a fraction of the amino acids with a special
mask token. The network is trained to predict the missing
tokens from the corrupted sequence:

2

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Params Training

Uniform Random

Model
(a) Oracle
(b) n-gram
(c) LSTM
LSTM
(d) Transformer
Transformer
Transformer
Transformer
Transformer
Transformer
Transformer
Transformer

(e)

(f)

4-gram
Small
Large
6-layer
12-layer
34-layer
34-layer
34-layer
10% data
1% data
0.1% data

UR50/S
28.4M UR50/S
113.4M UR50/S
42.6M UR50/S
85.1M UR50/S
669.2M UR100
669.2M UR50/S
669.2M UR50/D
669.2M UR50/S
669.2M UR50/S
669.2M UR50/S

ECE
1
25
17.18
14.42
13.54
11.79
10.45
10.32
8.54
8.46
10.99
15.01
17.50

Table 1. Evaluation of language models for generalization to held-out UniRef50 clusters. (a) Exponentiated cross-entropy (ECE) ranges
from 25 for a random model to 1 for a perfect model. (b) Best n-gram model across range of context sizes and Laplace-smoothing settings.
(c) State-of-the-art LSTM bidirectional language models (Peters et al., 2018). (d) Transformer model baselines with 6 and 12 layers.
Small Transformer models have better performance than LSTMs despite having fewer parameters. (e) 34-layer Transformer models
trained on datasets of differing sequence diversity. Increasing the diversity of the training set improves generalization. High-capacity
Transformer models outperform LSTMs and smaller Transformers. (f) 34-layer Transformer models trained on reduced fractions of data.
Increasing training data improves generalization.

LMLM = Ex∼X EM

(cid:88)

i∈M

− log p(xi|x/M )

(1)

For each sequence x we sample a set of indices M to mask,
replacing the true token at each index i with the mask token.
For each masked token, we independently minimize the
negative log likelihood of the true amino acid xi given the
masked sequence x/M as context. Intuitively, to make a
prediction for a masked position, the model must identify
dependencies between the masked site and the unmasked
parts of the sequence.

Evaluation of language models We begin by training a
series of Transformers on all the sequences in UniParc (The
UniProt Consortium, 2007), holding out a random sample of
1M sequences for validation. We use these models through-
out to investigate properties of the representations and the
information learned during pre-training.
To comparatively evaluate generalization performance of
different language models we use UniRef50 (Suzek et al.,
2015), a clustering of UniParc at 50% sequence identity. For
evaluation, a held-out set of 10% of the UniRef50 clusters
is randomly sampled. The evaluation dataset consists of the
representative sequences of these clusters. All sequences
belonging to the held-out clusters are removed from the
pre-training datasets.
We explore the effect of the underlying sequence diversity in
the pre-training data. Clustering UniParc shows a power-law

distribution of cluster sizes (Suzek et al., 2007), implying
the majority of sequences belong to a small fraction of clus-
ters. Training using a clustering of the sequences results in a
re-weighting of the masked language modeling loss toward
a more diverse set of sequences. We use UniRef (Suzek
et al., 2015) to create three pre-training datasets with differ-
ing levels of diversity: (i) the low-diversity dataset (UR100)
uses the UniRef100 representative sequences; (ii) the high-
diversity sparse dataset (UR50/S) uses the UniRef50 rep-
resentative sequences; (iii) the high-diversity dense dataset
(UR50/D) samples the UniRef100 sequences evenly across
the UniRef50 clusters.
Table 1 presents modeling performance on the held-out
UniRef50 sequences across a series of experiments explor-
ing different model classes, number of parameters, and pre-
training datasets. Models are compared using the exponenti-
ated cross entropy (ECE) metric, which is the exponential of
the model’s loss averaged per token. In the case of the Trans-
former this is 2LMLM. ECE describes the mean uncertainty
of the model among its set of options for every prediction:
ranging from 1 for an ideal model to 25 (the number of
unique amino acid tokens in the data) for a completely ran-
dom prediction. To measure the difﬁculty of generalization
to the evaluation set, we train a series of n-gram models
across a range of context lengths and settings of Laplace
smoothing on UR50/S. The best n-gram model has an ECE
of 17.18 with context size of 4.
As a baseline we train recurrent LSTM bidirectional lan-
guage models (Peters et al., 2018), which are state-of-the-art

3

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

for recurrent models in the text domain. Unlike standard
left-to-right autoregressive LSTMs, these models use the
whole sequence context, making them comparable to the
Transformers we study. We evaluate a small model with
approximately 25M parameters, and a large model with
approximately 113M parameters. Trained on the UR50/S
dataset, the small and large LSTM models have an ECE of
14.4 and 13.5 respectively.
We also train two small Transformers, a 12-layer (85.1M
parameters) and 6-layer Transformer (42.6M parameters) on
the UR50/S dataset. Both Transformer models have better
ECE values (10.45, and 11.79 respectively) than the small
and large LSTM models, despite the large LSTM having
more parameters. These results show the Transformer en-
ables higher ﬁdelity modeling of protein sequences for a
comparable number of parameters.
We train high-capacity 34-layer Transformers (approx 670M
parameters) across the three datasets of differing diver-
sity. The high-capacity Transformer model trained on the
UR50/S dataset outperforms the smaller Transformers in-
dicating an improvement in language modeling with in-
creasing model capacity. Transformers trained on the two
high-diversity datasets, UR50/S and UR50/D, improve gen-
eralization over the UR100 low-diversity dataset. The best
Transformer trained on the most diverse and dense dataset
reaches an ECE of 8.46, meaning that intuitively the is
model choosing among approximately 8.46 amino acids for
each prediction.
We also train a series of 34-layer Transformer models on
0.1%, 1%, and 10% of the UR50/S dataset, seeing the ex-
pected relationship between increased data and improved
generalization performance. Underﬁtting is observed even
for the largest models trained on 100% of UR50/S sug-
gesting potential for additional improvements with higher
capacity models.

ESM-1b Transformer Finally we perform a systematic
optimization of model hyperparameters on 100M parameter
models to identify a robust set of hyperparameters. The
hyperparameter search is described in detail in Appendix B.
We scale the hyperparameters identiﬁed by this search to
train a model with approximately 650M parameters (33
layers) on the UR50/S dataset, resulting in the ESM-1b
Transformer.

4. Multi-scale organization in sequence

representations

The variation observed in large protein sequence datasets
is inﬂuenced by processes at many scales, including prop-
erties that affect ﬁtness directly, such as activity, stability,
structure, binding, and other properties under selection (Hor-

Figure 1. Biochemical properties of amino acids are represented
in the Transformer model’s output embeddings, visualized here
with t-SNE. Through unsupervised learning, residues are clustered
into hydrophobic, polar, and aromatic groups, and reﬂect overall
organization by molecular weight and charge. Visualization of
36-layer Transformer trained on UniParc.

moz, 2013; Hopf et al., 2017) as well as by contributions
from phylogenetic bias (Gabaldon, 2007), experimental and
selection biases (Wang et al., 2019; Overbaugh & Bang-
ham, 2001), and sources of noise such as random genetic
drift (Kondrashov et al., 2003).
Unsupervised learning may encode underlying factors that,
while unobserved, are useful for explaining the variation in
sequences seen by the model during pre-training. We inves-
tigate the representation space of the network at multiple
scales from biochemical to evolutionary homology to look
for signatures of biological organization.
Neural networks contain inductive biases that impart struc-
ture to representations. Randomly initialized networks can
produce features that perform well without any learning (Jar-
rett et al., 2009). To understand how the process of learning
shapes the representations, it is necessary to compare repre-
sentations before and after they have been trained. Further-
more, a basic level of intrinsic organization is expected in
the sequence data itself as a result of biases in amino acid
composition. To disentangle the role of frequency bias in
the data we also compare against a baseline that maps each
sequence to a vector of normalized amino acid counts.

Learning encodes biochemical properties The Trans-
former neural network represents the identity of each amino
acid in its input and output embeddings. The input em-
beddings project the input amino acid tokens into the ﬁrst
Transformer block. The output embeddings project the ﬁ-
nal hidden representations back to logarithmic probabilities.
The interchangeability of amino acids within a given struc-
tural or functional context in a protein depends on their
biochemical properties (Hormoz, 2013). Self-supervision
can be expected to capture these patterns to build a repre-
sentation space that reﬂects biochemical knowledge.

4

Small (<120 Da)Medium (120-150 Da)Large (>150 Da)Negatively chargedPositively chargedAromaticHydrophobicPolarSizeUniqueBiological propertybioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Transformer (trained)

Transformer (untrained)

Unigram

(a) Sequence representations (t-SNE)

Transformer (trained)

Transformer (untrained)

Unigram

(b) Sequence representations (PCA)

Figure 2. Protein sequence representations encode and organize biological variations. (a) Each point represents a gene, and each gene is
colored by the orthologous group it belongs to (dimensionality is reduced by t-SNE). Orthologous groups of genes are densely clustered
in the trained representation space. By contrast, the untrained representation space and unigram representations do not reﬂect strong
organization by evolutionary relationships. (b) Genes corresponding to a common biological variation are related linearly in the trained
representation space. Genes are colored by their orthologous group, and their species is indicated by a character label. PCA recovers
a species axis (horizontal) and orthology axis (vertical) in the trained representation space, but not in the untrained or unigram spaces.
Representations are from the 36-layer Transformer model trained on UniParc.

To investigate if the network has learned to encode physico-
chemical properties in representations, we project the weight
matrix of the ﬁnal embedding layer of the network into
two dimensions with t-SNE (Maaten & Hinton, 2008). In
Figure 1 the structure of the embedding space reﬂects bio-
chemical interchangeability with distinct clustering of hy-
drophobic and polar residues, aromatic amino acids, and
organization by molecular weight and charge.

Biological variations are encoded in representation
space Each protein can be represented as a single vec-
tor by averaging across the hidden representation at each
position in its sequence. Protein embeddings represent se-
quences as points in a high dimensional space. Each se-
quence is represented as a single point and sequences as-
signed to similar representations by the network are mapped
to nearby points. We investigate how homologous genes are
represented in this space.
The structure and function of orthologous genes is likely to
be retained despite divergence of their sequences (Huerta-
Cepas et al., 2018). We ﬁnd in Figure 2a that training shapes
the representation space so that orthologous genes are clus-
tered. Figure 2a shows a two-dimensional projection of the
model’s representation space using t-SNE. Prior to training
the organization of orthologous proteins in the model’s rep-
resentation space is diffuse. Orthologous genes are clustered

in the learned representation space.
We examine whether unsupervised learning encodes bio-
logical variations into the structure of the representation
space. We apply principal component analysis (PCA), to
recover principal directions of variation in the representa-
tions, selecting 4 orthologous genes across 4 species to look
for directions of variation. Figure 2b indicates that linear
dimensionality reduction recovers species and orthology as
primary axes of variation in the representation space after
training. This form of structure is absent from the represen-
tations prior to training.
To quantitatively investigate the structure of the representa-
tion space, we assess nearest neighbor recovery under vector
similarity queries. If biological properties are encoded along
independent directions in the representation space, then pro-
teins corresponding with a unique biological variation are
related by linear vector arithmetic. In Figure S1 we ﬁnd that
learning improves recovery of target proteins under queries
encoded as linear transformations along the species or gene
axes.

Learning encodes remote homology Remotely homolo-
gous proteins have underlying structural similarity despite
divergence of their sequences. If structural homology is
encoded in the metric structure of the representation space,

5

abcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdabcdbioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Pre-
training

Hit-10
Fold
.584

HHblits†
.558
LSTM (S) UR50/S
.574
LSTM (L) UR50/S
.653
UR50/S
Transf-6
.639
UR50/S
Transf-12
.481
Transf-34
None
UR100
Transf-34
.599
UR50/D .617
Transf-34
.639
UR50/S
Transf-34
ESM-1b
UR50/S
.532

AUC
Fold
.831

.801
.805
.768
.778
.755
.753
.822
.825
.770

SF
.951

.863
.880
.901
.942
.807
.876
.932
.933
.880

SF
.965

.760
.813
.878
.915
.527
.841
.932
.931
.913

Table 2. Remote homology detection. Structural homology at the
fold and superfamily (SF) level is encoded in the metric structure
of the representation space. Results for unsupervised classiﬁer
based on distance between vector sequence embeddings. Hit-10
reports the probability that a remote homolog is included in the
ten nearest neighbors of the query sequence. Area under the ROC
curve (AUC) is reported for classiﬁcation by distance from the
query in representation space. Transformer models have higher
performance than LSTMs and similar performance to HMMs at
the fold level. Best neural models are indicated in bold. † HH-
blits (Remmert et al., 2011), a state-of-the-art HMM-based method
for remote homology detection, using 3 iterations of sequence
search.

then the distance between proteins reﬂects their degree of
structural relatedness.
We investigate whether the representation space enables de-
tection of remote homology at the superfamily (proteins that
belong to different families but are in the same superfamily)
and fold (proteins that belong to different superfamilies but
have the same fold) level. We construct a dataset to evaluate
remote homology detection using SCOPe (Fox et al., 2014).
Following standard practices (Söding & Remmert, 2011)
we exclude Rossman-like folds (c.2-c.5, c.27 and 28, c.30
and 31) and four- to eight-bladed β-propellers (b.66-b.70).
An unsupervised classiﬁer on distance from the query mea-
sures the density of homologous proteins in the neighbor-
hood of a query sequence. For each domain, a vector simi-
larity query is performed against all other domains, ranking
them by distance to the query domain. For evaluation at
the fold level, any domain with the same fold is a posi-
tive; any domain with a different fold is a negative; and
domains belonging to the same superfamily are excluded.
For evaluation at the superfamily level, any domain with the
same superfamily is a positive; any domain with a different
superfamily is a negative; and domains belonging to the
same family are excluded. We report AUC for the classiﬁer,
and Hit-10 (Ma et al., 2014) which gives the probability

(a) Overall

(b) Identical A.A. pairs

(c) Distinct A.A. pairs

Figure 3. Final representations from trained models implicitly
align sequences. Cosine similarity distributions are depicted for
the ﬁnal representations of residues from sequences within PFAM
family PF01010. The differences between the aligned (dark blue)
and unaligned (light blue) distributions imply that the trained
Transformer representations are a powerful discriminator between
aligned and unaligned positions in the sequences. In contrast rep-
resentations prior to training do not separate the aligned (dark
red) and unaligned positions (light red). AUCs across 128 PFAM
families are reported in Table S1.

of recovering a remote homolog in the ten highest ranked
results.
Table 2 indicates that vector nearest neighbor queries us-
ing the representations can detect remote homologs that are
distant at the fold level with similar performance to HH-
blits (Remmert et al., 2011) a state-of-the-art HMM-HMM
alignment-based method. At the superfamily level, where
sequence similarity is higher, HMM performance is better,
but Transformer embeddings are close. Fast vector nearest
neighbor ﬁnding methods allow billions of sequences to be
searched for similarity to a query protein within millisec-
onds (Johnson et al., 2017).

Learning encodes alignment within a protein family
An MSA identiﬁes corresponding sites across a family of
related sequences (Ekeberg et al., 2013a). These correspon-
dences give a picture of evolutionary variation at different
sites within the sequence family. The model receives as
input individual sequences and is given no access to the
family of related sequences except via learning. We investi-
gate whether the ﬁnal hidden representations of a sequence
encode information about the family it belongs to.
Family information could appear in the network through
assignment of similar representations to positions in differ-
ent sequences that are aligned in the family’s MSA. Using
the collection of MSAs of structurally related sequences in
Pfam (Bateman et al., 2013), we compare the distribution
of cosine similarities of representations between pairs of
residues that are aligned in the family’s MSA to a back-
ground distribution of cosine similarities between unaligned
pairs of residues. A large difference between the aligned
and unaligned distributions implies that the representations
use shared features for related sites within all the sequences
of the family.

6

Transformer (untrained)Transformer (trained)Aligned pairsUnaligned pairs0.00.20.40.60.81.0Cosine similarityDensity0.00.20.40.60.81.0Cosine similarityDensity0.00.20.40.60.81.0Cosine similarityDensitybioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Pre-
training

-

-
-

Family
47.9 ± 1.1

Model
HMM Proﬁle
CCMpred
62.4 ± 0.7
Transformer-6
UR50/S
65.5 ± 0.7
Transformer-12 UR50/S
41.5 ± 0.9
Transformer-34
(None)
65.4 ± 0.7
Transformer-34 UR100
69.4 ± 0.6
Transformer-34 UR50/S
Transformer-34 UR50/D 69.5 ± 0.6
71.1 ± 0.5
ESM-1b

UR50/S

SSP

Superfamily
47.8 ± 1.3

Fold

48.0 ± 1.6

-

61.8 ± 0.7
65.0 ± 1.0
41.3 ± 0.8
64.9 ± 0.9
68.8 ± 0.9
68.9 ± 0.9
70.5 ± 0.9

-

61.8 ± 1.0
65.2 ± 1.0
41.3 ± 1.3
64.9 ± 0.9
68.9 ± 0.9
69.0 ± 0.9
70.6 ± 0.9

Family
14.8 ± 0.6
32.7 ± 1.0
24.4 ± 2.2
32.8 ± 2.9
8.7 ± 0.3
28.3 ± 2.5
43.9 ± 2.8
43.9 ± 2.8
49.2 ± 2.5

Contact

Superfamily
14.5 ± 1.4
32.5 ± 2.3
19.4 ± 3.1
25.8 ± 3.7
8.4 ± 0.7
22.5 ± 3.3
36.4 ± 4.2
37.1 ± 4.6
43.5 ± 4.8

Fold

14.6 ± 1.6
32.6 ± 0.4
18.6 ± 0.5
25.2 ± 0.9
8.4 ± 0.3
22.0 ± 0.8
35.3 ± 1.7
36.3 ± 2.0
42.8 ± 2.3

Table 3. Linear projections. Five-fold cross validation experiment for generalization at the family, superfamily, and fold level. 8-class
accuracy (secondary structure), Top-L long-range precision (contacts), mean and standard deviation across test sets for the ﬁve partitions.
Minimal information about structure is present in representations prior to training. Information about secondary and tertiary structure
emerges in representations as a result of unsupervised learning on sequences with the language modeling objective. Increasing diversity
of sequences improves learning of structure. (Higher diversity UR50 datasets improve over UR100). Learned representations enable
linear projections to generalize to held-out folds, outperforming projections of the sequence proﬁle, and contacts identiﬁed by the
CCMpred (Seemayer et al., 2014) implementation of direct coupling analysis.

Figure 3a depicts the distribution of cosine similarity values
between aligned and unaligned positions within a repre-
sentative family for the trained model and baselines. Un-
supervised learning produces a marked shift between the
distributions of aligned and unaligned pairs. Figure 3b and
Figure 3c indicate that these trends hold under the con-
straints that the residue pairs (1) share the same amino acid
identity or (2) have different amino acid identities.
We estimate differences between the aligned and unaligned
distributions across 128 Pfam families using the area under
the ROC curve (AUC) as a metric of discriminative power
between aligned and unaligned pairs. Table S1 shows a
quantitative improvement in average AUC after unsuper-
vised training, supporting the idea that self-supervision en-
codes information about the MSA of a sequence into its
representation of the sequence.

5. Prediction of secondary structure and

tertiary contacts

There is reason to believe that unsupervised learning will
cause the model’s representations to contain structural in-
formation. The underlying structure of a protein is a hidden
variable that inﬂuences the patterns observed in sequence
data. For example local sequence variation depends on
secondary structure (Levitt, 1978); and tertiary structure in-
troduces higher order dependencies in the choices of amino
acids at different sites within a protein (Marks et al., 2011;
Anishchenko et al., 2017). While the model cannot ob-
serve protein structure directly, it observes patterns in the
sequences of its training data that are determined by struc-

ture. In principle, the network could compress sequence
variations by capturing commonality in structural elements
across the data, thereby encoding structural information into
the representations.

Linear projections We begin by identifying information
about protein structure that is linearly encoded within the
representations. The use of linear projections ensures that
the information originates in the Transformer representa-
tions, enabling a direct inspection of the structural content
of representations. By comparing representations of the
Transformer before and after pre-training, we can identify
the information that emerges as a result of the unsupervised
learning.
We perform a ﬁve-fold cross validation experiment to study
generalization of structural information at the family, su-
perfamily, and fold level. For each of the three levels, we
construct a dataset of 15,297 protein structures using the
SCOPe database. We partition the structures into ﬁve parts,
splitting by family, superfamily, and fold accordingly. Five-
fold cross validation is performed independently for each of
the levels of structural hold-out.
To detect information about secondary structure we ﬁt a
logistic regression to the hidden representations using the
8-class secondary structure labels. To detect information
about tertiary structure, we ﬁt two separate linear projec-
tions to the hidden representations of pairs of positions in
the sequence, taking their dot product to regress a binary
variable indicating whether the positions are in contact in
the protein’s 3-dimensional structure. The neural representa-
tions are compared to (i) projections of the sequence proﬁle;

7

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

With pre-training
8-class Acc: 70.6%

No pre-training

8-Class Acc: 36.6%

(a) d1nt4a_ (Phosphoglycerate mutase-like fold)

With pre-training
8-class Acc: 82.4%

8-class Acc: 32.4%
(b) d3wr7a_ (Acyl-CoA N-acyltransferases fold)

No pre-training

Figure 4. Secondary structure (linear projections). Example pre-
dictions for held-out folds. Unsupervised pre-training encodes
secondary structure into representations. Following pre-training,
linear projections recover secondary structure (left column). With-
out pre-training little information is recovered (right column). Col-
ors indicate secondary structure class identiﬁed by the projection:
helix (red), strand (green), and coil (blue). Color intensities indi-
cate conﬁdence. Representations from ESM-1b Transformer are
used.

(ii) unsupervised contacts predicted by the CCMpred im-
plementation (Seemayer et al., 2014) of direct coupling
analysis. MSAs for the baselines are generated from the
UniClust30 (Mirdita et al., 2017) database using 3 iterations
of search by Hhblits. For secondary structure, we report
8-class accuracies. For contact precision we report Top-L
long-range precision, i.e. the precision of the L (length of
the protein) highest ranked predictions for contacts with
sequence separation of at least 24 residues.
Table 3 shows results of the cross validation experiments.
Prior to pre-training, minimal information about secondary
structure and contacts can be detected. After pre-training,
projections recover information about secondary structure
and long-range contacts that generalizes across families,
superfamilies, and folds. Secondary structure prediction
8-class accuracy distributions (Figure S2), and long-range
contact prediction Top-L precision distributions (Figure S3)
demonstrate that pre-training produces an increase in struc-
tural information across the entire distribution of test do-
mains. Table 3 shows that projections of the Transformer
representations recover more structure than projections of
the sequence proﬁle. For long-range contacts, projections of
the best Transformer models have higher precision than con-

Figure 5. Residue-residue contacts (linear projections). Left: Top-
L predictions for fold level held-out example d1n3ya_, with vWA-
like fold. True positives in blue, false positives in red, superim-
posed on ground-truth contact map in grey. ESM-1b Transformer
projections below the diagonal, CCMpred predictions above the
diagonal. Right: Precision distribution (Top-L long-range) com-
paring ESM-1b projections with CCMpred across all domains in
the ﬁve test partitions with structural hold-out at the fold level.
Visualized domain marked by x.

tacts predicted by CCMpred across all levels of structural
generalization. As the level of structural split becomes more
remote, there is little degradation for secondary structure,
with performance at the family level similar to the fold level.
For long-range contacts, while generalization is reduced at
the fold level in comparison to the family level, the best
models still capture more structure than the unsupervised
baseline. Training with higher-diversity sequences (UR50
datasets) improves learning of both secondary structure and
long-range contacts, with a more pronounced effect on long-
range contacts.
Figure 4 visualizes three-class secondary structure projec-
tions for two domains belonging to held-out folds. Prior
to pre-training, projections produce an incoherent predic-
tion of secondary structure. After pre-training projections
recover a coherent prediction with most errors occurring at
the boundaries of secondary structure regions. Figure 5 com-
pares a projected contact map to predictions from CCMpred.
Transformer projections recover complex contact patterns,
including long-range contacts. Further visualizations of pro-
jected contacts for eight randomly selected test proteins are
shown in Figure S7.

Deep neural network We train deep neural networks to
predict secondary structure and contacts from the represen-
tations. We choose state-of-the-art neural architectures for
both tasks. These downstream models are trained with a
supervised loss to predict either the secondary structure or
contact map from the pre-trained representations. The ar-
chitecture of the downstream model is kept ﬁxed across
experiments with different representations and baselines to
enable comparison.

8

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Pre-
Training CB513

Model
HMM Proﬁle
LSTM (S)
LSTM (L)
Transf-6
Transf-12
Transf-34
Transf-34
Transf-34
Transf-34
ESM-1B

UR50/S
UR50/S
UR50/S
UR50/S
None
UR100
UR50/S
UR50/D
UR50/S

71.2 ± 0.1
60.4 ± 0.1
62.4 ± 0.2
62.0 ± 0.2
65.4 ± 0.1
56.8 ± 0.3
64.3 ± 0.2
69.1 ± 0.2
69.2 ± 0.1
71.6 ± 0.1

CASP13
72.3 ± 0.9
63.2 ± 0.6
64.1 ± 0.7
64.2 ± 1.2
67.2 ± 0.3
60.0 ± 0.5
66.5 ± 0.3
70.7 ± 0.8
70.9 ± 0.5
72.5 ± 0.2

Table 4. Eight-class secondary structure prediction accuracy on
the CB513 and CASP13 test sets. A ﬁxed neural architecture is
trained to predict the secondary structure label from the language
model representation of the input sequence. The Transformer has
higher performance than the comparable LSTM baselines. Pre-
training with the high-diversity UR50 datasets increases accuracy
signiﬁcantly. Features from ESM-1b Transformer are competitive
with HMM proﬁles for supervised secondary structure prediction.

To predict secondary structure we replace the linear layer
with a deep neural network, using the model architecture
introduced by the Netsurf method (Klausen et al., 2019).
For tertiary structure, we predict the binary contact map
from the hidden representation of the sequence. We use
a dilated convolutional residual network similar to recent
state-of-the-art methods for tertiary structure prediction (Xu,
2018; Jones & Kandathil, 2018; Senior et al., 2018).
Table 4 compares the representations for secondary structure
prediction. We evaluate models on the CB513 test set (Cuff
& Barton, 1999) and the CASP13 domains (Kryshtafovych
et al., 2019). For comparison we also re-implement the
Netsurf method. The models are trained on the Netsurf
training dataset which applies a 25% sequence identity hold-
out with CB513, and a temporal hold-out with CASP13.
The Transformer features are compared before and after
unsupervised pre-training to features from the LSTM base-
lines. They are also compared to the HMM proﬁles used
by Netsurf. The best Transformer features (71.6%) match
the performance of the HMM proﬁles (71.2%), and ex-
ceed the published performance of RaptorX (70.6)% on the
same benchmark (Klausen et al., 2019), implying that pro-
tein langauge models can produce features that are directly
competitive with sequence proﬁles for secondary structure
prediction.
Table 5 shows performance of the various representations
for predicting Top-L long-range contacts across a panel
of benchmarks using the RaptorX train set (Wang et al.,
2017). For comparison we train the same architecture us-

Pre-
Model
training Test
LSTM (S) UR50/S
24.1
LSTM (L) UR50/S
27.8
Transf-6
UR50/S
30.2
UR50/S
Transf-12
37.7
(None)
Transf-34
16.3
UR100
Transf-34
32.7
Transf-34
UR50/S
50.2
UR50/D 50.0
Transf-34
56.9
ESM-1b
UR50/S

CASP

12
19.9
24.0
25.3
27.8
14.8
24.3
34.7
33.6
42.7

13
15.3
16.4
19.8
20.7
13.3
19.1
30.1
28.0
35.9

11
23.6
26.4
29.9
33.6
17.7
28.9
42.8
43.0
47.4

Table 5. Top-L long-range contact precision. A deep dilated con-
volutional residual network is trained to predict contacts using
the representations from the pre-trained language model. The
pre-trained Transformer representations outperform the LSTM rep-
resentations in all cases. Pre-training on the high-diversity UR50
datasets boosts precision of representations over pre-training on
UR100. High-capacity Transformers (34 layer) outperform lower
capacity models (6/12 layer).

ing features from RaptorX (Wang et al., 2017; Xu, 2018).
The Test (Wang et al., 2017) and CASP11 (Moult et al.,
2016) test sets evaluate with sequence identity hold-out at
25%; the CASP12 (Moult et al., 2018) test set implements
a temporal hold-out with the structural training data; and
the CASP13 (Kryshtafovych et al., 2019) experiment imple-
ments a full temporal hold-out of both the pre-training and
training data. For contact prediction, the best features from
representation learning do not achieve comparable perfor-
mance to the state-of-the-art RaptorX features (50.2 vs 59.4
respectively on the RaptorX test set).
In the secondary structure benchmarks Transformer repre-
sentations produce higher accuracy than the LSTM base-
lines with comparable numbers of parameters. For contact
prediction Transformer reprentations yield higher precision
than LSTMs, with even the smallest Transformer represen-
tations exceeding LSTMs with more parameters. Diversity
in the pre-training data also has a strong effect, with the
high-diversity datasets providing signiﬁcant improvements
over the low-diversity dataset. Relative performance of the
representations is consistent across all four of the contact
benchmarks using different hold-out methodology.

Relationship between language modeling and structure
learning To investigate the relationship between the lan-
guage modeling objective and information about structure
in the model, linear projections for secondary structure and
contacts are ﬁt using the representations from Transformer
models taken from checkpoints across their pre-training tra-
jectories. We use the Transformers trained on UR50/S. We

9

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Features

CB513

CASP13

RaptorX
Netsurf
(a) Netsurf (reimpl.)
(b) +direct
(c) +avg

70.6
72.1
71.2 ± 0.1
72.1 ± 0.1
73.7 ± 0.2

74
72.3 ± 0.9
72.2 ± 0.5
75.1 ± 0.4

Table 6. Feature combination (secondary structure prediction).
Eight-class accuracy. The language model improves state-of-the-
art features for secondary structure prediction. Features from a
reimplementation of Netsurf (Klausen et al., 2019) are combined
with 34-layer Transformer (UR50/S) embeddings using a two layer
BiLSTM architecture. (a) Performance of Netsurf features alone.
(b) Direct adds the Transformer representation of the input se-
quence. (c) Avg adds the average of Transformer features for each
position in the MSA of the input sequence. Results exceed those
for state-of-the-art methods RaptorX (70.6%) and Netsurf (72.1%)
on the CB513 test set, and for Netsurf (74.0%) on the CASP13
evaluation set used here.

formation within-family and out-of-family by evaluating
on sequences with ground truth labels from the family the
model was trained on or from the alternate families. The
models are evaluated using linear projections. In all cases,
the model trained on within-family sequences has higher
accuracy than models trained on out-of-family sequences
(Table S2), indicating poor generalization when training on
single MSA families. More signiﬁcantly, the model trained
across the full UniParc sequence diversity has a higher ac-
curacy than the single-family model accuracies, even on the
same-family evaluation dataset. This suggests that the rep-
resentations learned from the full dataset are generalizing
information about secondary structure learned outside the
sequence family.

6. Feature combination
Features discovered by unsupervised protein language mod-
eling can be combined with state-of-the-art features to im-
prove them further. Current state-of-the-art methods use
information derived from MSAs. We combine this informa-
tion with features from the Transformer model.
We explore three approaches for incorporating information
from representation learning. For each input sequence x: (i)
direct uses the ﬁnal hidden representation from the Trans-
former directly; (ii) avg takes the average of the ﬁnal hidden
representation at each position across the sequences from
the MSA of x; (iii) cov produces features for each pair of
positions, using the uncentered covariance across sequences
from the MSA of x, after dimensionality reduction of the
ﬁnal hidden representations by PCA. Note that (i) and (ii)
produce features for each position in x, while (iii) produces

10

Figure 6. Relationship between the language modeling objective
and structure learning. Eight-class secondary structure prediction
accuracy (left) and contact prediction Top-L long-range precision
(right) both as a function of pre-training ECE. Performance is
evaluated on held-out folds. Linear projections are ﬁt using model
checkpoints over the course of pre-training on UR50/S. The linear
relationship for each model indicates that for a given model and
pre-training dataset, the language modeling ECE is a good proxy
for the structural content of the representations. Improvement of
the model’s ECE leads to an increase in information about structure.
This establishes a link between the language modeling objective
and unsupervised structure learning.

ﬁt the projections and evaluate with the train and test split
implemented by the ﬁrst partition of the fold level struc-
tural hold-out dataset. For each model, Figure 6 shows a
linear relationship between the language modeling objec-
tive and information about structure, which is maintained
over the course of pre-training. The linear ﬁt is close to
ideal for both secondary structure and contacts. A simi-
lar experiment is also performed for secondary structure
with a deep neural network instead of linear projection, us-
ing the Netsurf training sequences and CB513 test set. A
linear relationship between secondary structure accuracy
and language modeling ECE is also observed for the deep
prediction head (Figure S4). Thus, for a given model and
pre-training dataset, language modeling ﬁdelity measured
by ECE is a good proxy for the structural content of the
representations. Since performance on the language model-
ing objective improves with model capacity, this suggests
further scale may improve results on structure prediction
tasks.

Single versus multi-family pre-training We compare
training across evolutionary statistics to training on sin-
gle protein families. We pre-train separate 12-layer Trans-
former models on the Pfam multiple sequence alignments
of the three most common domains in nature longer than
100 amino acids, the ATP-binding domain of the ABC trans-
porters, the protein kinase domain, and the response regula-
tor receiver domain. We test the ability of models trained
on one protein family to generalize secondary structure in-

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

# domains
(a) RaptorX
(b) +direct
(c) +avg
(d) +cov

Test
500
59.4 ± .2
61.7 ± .4
62.9 ± .4
63.3 ± .2

CASP11
105
53.8 ± .3
55.0 ± .1
56.6 ± .4
56.8 ± .2

CASP12
55
51.1 ± .2
51.5 ± .5
52.4 ± .5
53.0 ± .3

CASP13
34
43.4 ± .4
43.7 ± .4
44.8 ± .8
45.2 ± .5

Table 7. Feature combination (contact prediction). Top-L long-range contact precision. The language model improves state-of-the-art
features for contact prediction. A deep ResNet with ﬁxed architecture is trained on each feature set to predict binary contacts. (a)
performance of state-of-the-art RaptorX (Xu, 2018) features including PSSM, predicted secondary structure, predicted accessibility,
pairwise APC-corrected Potts model couplings and mutual information, and a pairwise contact potential.
(b) Adds Transformer
representation of the input sequence to the feature set. (c) Adds the average Transformer representation at each position of the MSA. (d)
Adds the uncentered covariance over the MSA of a low-dimenensional projection of the Transformer features. Features are from the
34-layer Transformer pre-trained on UR50/S.

features for each pair of positions.

Secondary structure Current state-of-the-art methods for
secondary structure prediction have high accuracies for the
eight-class prediction problem. We investigate whether per-
formance can be improved by combining Transformer fea-
tures with sequence proﬁles. Table 6 shows that combining
the representations with proﬁles further boosts accuracy,
resulting in state-of-the-art performance on secondary struc-
ture prediction.
We establish a baseline of performance by reimplementing
the Klausen et al. (2019) method using the same features,
resulting in an accuracy of 71.2% (vs. published perfor-
mance of 72.1%) on the CB513 test set. Then we add
the the Transformer features using the direct and avg com-
bination methods; these achieve 0.9% and 2.5% absolute
improvement in accuracy respectively. This suggests that
the Transformer features contain information not present in
the MSA-derived features.

Residue-residue contacts Deep neural networks have en-
abled recent breakthroughs in the prediction of protein con-
tacts and tertiary structure (Xu, 2018; Senior et al., 2018).
State-of-the-art neural networks for tertiary structure and
contact prediction use deep residual architectures with two-
dimensional convolutions over pairwise feature maps to
output a contact prediction or distance potential for each
pair of residues (Wang et al., 2017; Xu, 2018; Senior et al.,
2018).
A variety of input features, training datasets, and supervision
signals are used in state-of-the-art methods. To make a con-
trolled comparison, we ﬁx a standard architecture, training
dataset, multiple sequence alignments, and set of base input
features for all experiments, to which we add pre-trained
features from the Transformer model. For the base features
we use the RaptorX feature set which includes PSSM, 3-
state secondary structure prediction, one-hot embedding of

sequence, APC corrected Potts model couplings, mutual
information, pairwise contact potential, and predicted ac-
cessibility. RaptorX was the winning method for contact
prediction in the CASP12 and CASP13 competitions (Xu,
2018). The training and evaluation sets are the same as used
in the previous section.
Table 7 indicates that addition of Transformer features from
the 34-layer model trained on UR50/S consistently produces
an improvement across the test sets. The table shows Top-L
long-range precisions reporting mean and standard devia-
tion over 5 different model seeds. Direct gives a modest
improvement on some test sets. Avg improves over direct,
and cov provides further gains. For example, cov produces
an absolute improvement of 3.9% on the RaptorX test set,
and 1.8% improvement on the CASP13 test set evaluated
with temporal hold-outs on both ﬁne-tuning and pre-training
data. Additional results and metrics for contact prediction
are reported in Table S3.

7. Prediction of mutational effects
The mutational ﬁtness landscape provides deep insight into
biology. Coupling next generation sequencing with a muta-
genesis screen allows parallel readout of tens of thousands
of variants of a single protein (Fowler & Fields, 2014). The
detail and coverage of these experiments provides a view
into the mutational ﬁtness landscape of individual proteins,
giving quantitative relationships between sequence and pro-
tein function. We adapt the Transformer protein language
model to predict the quantitative effect of mutations.
First we investigate intra-protein variant effect prediction,
where a limited sampling of mutations is used to predict
the effect of unobserved mutations. This setting has util-
ity in protein engineering applications (Yang et al., 2019).
We evaluate the representations on two deep mutational
scanning datasets used by recent state-of-the-art methods
for variant effect prediction, Envision (Gray et al., 2018)

11

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Figure 7. Representation learning enables state-of-the-art supervised prediction of the quantitative effect of mutations. Left panel: Envision
dataset (Gray et al., 2018); right panel: DeepSequence dataset (Riesselman et al., 2018). Transformer representations (34-layer, UR50/S)
are compared to the LSTM bidirectional language model (large model, UR50/S). The result of ﬁve-fold cross validation is reported
for each protein. For each partition, supervised ﬁne-tuning is performed on 80% of the mutational data for the protein, and results are
evaluated on the remaining 20%. Transformer representations outperform baseline LSTM representations on both datasets. State-of-the-art
methods are also shown for each dataset. Gray et al. (2018) is a supervised method using structural, evolutionary, and biochemical
features, trained with the same protocol as used for the Transformer. Riesselman et al. (2018) is an unsupervised method trained on the
MSA of each protein.

and DeepSequence (Riesselman et al., 2018). Collectively
the data includes over 700,000 variant effect measure-
ments from over 100 large-scale experimental mutagenesis
datasets.
Fine-tuning the Transformer yields a mutational effect pre-
dictor that is comparable to the results of Envision. En-
vision (Gray et al., 2018) relies on protein structural and
evolutionary features to generalize. We assess whether the
Transformer can achieve similar generalization results, with-
out direct access to structural features. The same method-
ology for partitioning data for training and evaluation is
used as in Gray et al. (2018) to allow a comparison of the
results. We use the 34-layer Transformer trained on UR50/S.
Figure 7 shows the ﬁne-tuned Transformer exceeds the per-
formance of Envision on 10 of the 12 proteins. For each
protein a fraction p = 0.8 of the data is used for training
and the remaining data is used for testing. We report mean
and standard deviations for ﬁve-fold cross-validation in Ta-
ble S5. Results varying the fraction of data that is used for
training are reported in Figure S5.
We also evaluate using the same ﬁve-fold cross validation
methodology on the deep mutational scanning experiments
assembled for DeepSequence (Riesselman et al., 2018). The
ﬁne-tuned Transformer model outperforms the ﬁne-tuned
LSTM baselines. While not directly comparable, we also in-

clude the performance of the original DeepSequence method
which is unsupervised, and represents state-of-the-art for
this dataset.

Generalization to a new ﬁtness landscape We analyze
the Transformer’s ability to generalize to the ﬁtness land-
scape of a new protein. Following the protocol introduced
in Envision, we use a leave-one-out analysis: to evaluate
performance on a given protein, we train on data from the
remaining n − 1 proteins and test on the held-out protein.
Figure S6 shows that the Transformer’s predictions from
raw sequences perform better than Envision on 5 of the 9
tasks.

8. Related Work
Contemporaneously with the preprint of this work, Rives
et al. (2019), related preprints Alley et al. (2019) and
Heinzinger et al. (2019), also proposed protein language
modeling, albeit at a smaller scale. These works, along with
Rao et al. (2019), evaluated on a variety of downstream tasks.
Rives et al. (2019) ﬁrst proposed protein language model-
ing with Transformers. Alley et al. (2019) and Heinzinger
et al. (2019) train LSTMs on UniRef50. Rao et al. (2019)
trained a 12-layer Transformer model (38M parameters) on
Pfam (Bateman et al., 2013). The baselines in this paper are

12

POL_HV1N5-CA_Ndungu2014  -lactamase [Ranganathan 2015]-lactamase [Ostermeier 2014]TIM Barrell (T. thermophilus)-lactamase [Palzkill 2012]Ubiquitin [Bolon 2013]HrasLevoglucosan kinase (stabilized)Influenza hemagglutinin -glucosidase Yeast tRNA (CCU, Arg) TIM Barrell (S. solfataricus)Aliphatic amidaseHIV env protein (BG505)PABP singles (RRM domain)TIM Barrell (T. maritima)Kanamycin kinase APH(3')-II HIV env protein (BF520)YAP1 (WW domain 1) Mitogen-activated protein kinase 1SUMO-conjugating enzyme UBC9PSD95 (PDZ domain) Ubiquitin [Bolon 2014]-lactamase [Tenaillon 2013]HSP90 (ATPase domain) Ubiquitin [Fraser 2016]PTEN_HUMAN_Fowler2018 GAL4 (DNA-binding domain) DNA methylase HaeIII Translation initiation factor IF-1Small ubiquitin-related modifier 1Thiopurine S-methyltransferaseInfluenza polymerase PA subunit BRCA 1 (BRCT domain)UBE4B (U-box domain) Imidazoleglycerol-phosphate dehydrataseBRCA 1 (RING domain) Levoglucosan kinaseThiamin pyrophosphokinase 1Toxin-antitoxin complex Calmodulin-1TransformerLSTM biLM (large)Riesselman, et al. 2018Gray, et al. 2018Protein GBeta LactamasePab1 (RRM Domain)Yap65 (WW domain)Aminoglycoside kinasePSD95 (pdz3 domain)Hsp90Ubiquitin (E1 activation)E4B (U-box domain)BRCA1 (E3 ligase activity)BRCA1 (Bard1 binding)0.00.20.40.60.81.0|Spearman |bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Pre-
Training Params RH SSP Contact

Model
UniRep1†
SeqVec2†
Tape3†
UR50/S
LSTM (S)
UR50/S
LSTM (L)
UR50/S
Transformer-6
Transformer-12 UR50/S
Transformer-34 UR100
Transformer-34 UR50/S
ESM-1b
UR50/S

18M .527
93M .545
38M .581
28.4M .558
113.4M .574
42.6M .653
85.1M .639
669.2M .599
669.2M .639
652.4M .532

58.4
62.1
58.0
60.4
62.4
62.0
65.4
64.3
69.2
71.6

21.9
29.0
23.2
24.1
27.8
30.2
37.7
32.7
50.2
56.9

Table 8. Comparison to related protein language models. (RH) Remote Homology at the fold level, using Hit-10 metric on SCOP. (SSP)
Secondary structure Q8 accuracy on CB513. (Contact) Top-L long range contact precision on RaptorX test set from Wang et al. (2017).
Results for additional test sets in Table S6. 1Alley et al. (2019) 2Heinzinger et al. (2019) 3Rao et al. (2019). †The pre-training datasets for
related work have differences from ours.

comparable to these models. The large Transformer models
trained in this paper are considerably larger than in these
related works.
We benchmark against related work in Table 8. Heinzinger
et al. (2019), Alley et al. (2019), and Rao et al. (2019),
evaluate models on differing downstream tasks and test sets.
We retrieve the weights for the above models, evaluating
them directly in our codebase against the panel of test sets
used in this paper for remote homology, secondary structure
prediction, and contact prediction, with the same training
data and model architectures. This allows a direct com-
parison between the representations. Table 8 shows that
high-capacity Transformers have strong performance for
secondary structure and contact prediction signiﬁcantly ex-
ceeding Alley et al. (2019), Heinzinger et al. (2019), and
Rao et al. (2019). The small Transformer models trained as
baselines also have higher performance than the methods
with comparable parameter numbers.
Protein sequence embeddings have been the subject of re-
cent investigation for protein engineering (Yang et al., 2018).
Bepler & Berger (2019) pre-trained LSTMs on protein se-
quences, adding supervision from contacts to produce em-
beddings. Subsequent to our preprint, related works have
built on its exploration of protein sequence modeling, ex-
ploring generative models (Riesselman et al., 2019; Madani
et al., 2020), internal representations of Transformers (Vig
et al., 2020), and applications of representation learning
and generative modeling such as classiﬁcation (Elnaggar
et al., 2019; Strodthoff et al., 2020), mutational effect pre-
diction (Luo et al., 2020), and design of sequences (Repecka
et al., 2019; Hawkins-Hooker et al., 2020; Amimeur et al.,
2020).

9. Discussion
One of the goals for artiﬁcial intelligence in biology could be
the creation of controllable predictive and generative models
that can read and generate biology in its native language.
Accordingly, research will be necessary into methods that
can learn intrinsic biological properties directly from pro-
tein sequences, which can be transferred to prediction and
generation.
We investigated deep learning across evolution at the scale
of the largest protein sequence databases, training contex-
tual language models across 86 billion amino acids from
250 million sequences. The space of representations learned
from sequences by high-capacity networks reﬂects biologi-
cal structure at multiple levels, including that of amino acids,
proteins, and evolutionary homology. Information about sec-
ondary and tertiary structure is internalized and represented
within the network. Knowledge of intrinsic biological prop-
erties emerges without supervision — no learning signal
other than sequences is given during pre-training.
We ﬁnd that networks that have been trained across evolu-
tionary data generalize: information can be extracted from
representations by linear projections, deep neural networks,
or by adapting the model using supervision. Fine-tuning pro-
duces results that match state-of-the-art on variant activity
prediction. Predictions are made directly from the sequence,
using features that have been automatically learned by the
language model, rather than selected by domain knowledge.
We ﬁnd that pre-training discovers information that is not
present in current state-of-the-art features. The learned fea-
tures can be combined with features used by state-of-the-art
structure prediction methods to improve results. Empir-
ically we ﬁnd that features discovered by larger models

13

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

perform better on downstream tasks. The Transformer out-
performs LSTMs with similar capacity across benchmarks.
Increasing diversity of the training data results in signiﬁcant
improvements to the representations.
While the protein language models we study are of com-
parable scale to those used in the text domain, our experi-
ments have not yet reached the limit of scale. We observed
that even the highest capacity models we trained (with ap-
proximately 650-700M parameters) under-ﬁt the sequence
datasets, due to insufﬁcient model capacity. The relation-
ship we ﬁnd between language modeling ﬁdelity and the
information about structure encoded into the representa-
tions suggests that higher capacity models will yield better
representations. These ﬁndings imply potential for further
model scale and data diversity incorporating sequences from
metagenomics.
Combining high-capacity generative models with gene syn-
thesis and high throughput characterization can enable gen-
erative biology. The models we have trained can be used
to generate new sequences (Wang & Cho, 2019). If neural
networks can transfer knowledge learned from protein se-
quences to design functional proteins, this could be coupled
with predictive models to jointly generate and optimize se-
quences for desired functions. The size of current sequence
data and its projected growth point toward the possibility of
a general purpose generative model that can condense the
totality of sequence statistics, internalizing and integrating
fundamental chemical and biological concepts including
structure, function, activity, localization, binding, and dy-
namics, to generate new sequences that have not been seen
before in nature but that are biologically active.

Pre-trained models

Transformer models and baselines are available at:
https://github.com/facebookresearch/esm

Acknowledgments
We thank Tristan Bepler, Richard Bonneau, Yilun Du,
Vladimir Gligorijevic, Anika Gupta, Omer Levy, Ian Peikon,
Hetunandan Kamisetty, Laurens van der Maaten, Ethan
Perez, Oded Regev, Neville Sanjana, and Emily Wang for
feedback on the manuscript and insightful conversations.
We thank Jinbo Xu for sharing RaptorX features and help
with CASP13. We thank Michael Klausen for providing
Netsurf training code. Alexander Rives was supported at
NYU by NSF Grant #1339362.

References
Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M.,
and Church, G. M. Uniﬁed rational protein engineering

with sequence-based deep representation learning. Nature
methods, 16(12):1315–1322, 2019.

Altschuh, D., Lesk, A., Bloomer, A., and Klug, A. Cor-
relation of co-ordinated amino acid substitutions with
function in viruses related to tobacco mosaic virus. Jour-
nal of Molecular Biology, 193(4):693–707, 1987. ISSN
0022-2836. doi: 10.1016/0022-2836(87)90352-4.

Altschuh, D., Vernet, T., Berti, P., Moras, D., and Nagai, K.
Coordinated amino acid changes in homologous protein
families. Protein Engineering, Design and Selection, 2
(3):193–199, 1988.

Altschul, S. F. and Koonin, E. V. Iterated proﬁle searches
with psi-blast – a tool for discovery in protein databases.
Trends in Biochemical Sciences, 23(11):444–447, 11
1998. ISSN 0968-0004. doi: 10.1016/S0968-0004(98)
01298-5.

Altschul, S. F., Gish, W., Miller, W., Myers, E. W., and
Lipman, D. J. Basic local alignment search tool. Journal
of Molecular Biology, 215(3):403–410, 1990. ISSN 0022-
2836. doi: 10.1016/S0022-2836(05)80360-2.

Amimeur, T., Shaver, J. M., Ketchem, R. R., Taylor, J. A.,
Clark, R. H., Smith, J., Van Citters, D., Siska, C. C.,
Smidt, P., Sprague, M., et al. Designing feature-controlled
humanoid antibody discovery libraries using generative
adversarial networks. bioRxiv, 2020.

Anishchenko, I., Ovchinnikov, S., Kamisetty, H., and Baker,
D. Origins of coevolution between residues distant in pro-
tein 3d structures. Proceedings of the National Academy
of Sciences, 114(34):9122–9127, 2017.

Baevski, A., Edunov, S., Liu, Y., Zettlemoyer, L., and Auli,
M. Cloze-driven pretraining of self-attention networks.
CoRR, abs/1903.07785, 2019. URL http://arxiv.
org/abs/1903.07785.

Balakrishnan, S., Kamisetty, H., Carbonell, J. G., Lee, S.-I.,
and Langmead, C. J. Learning generative models for
protein fold families. Proteins: Structure, Function, and
Bioinformatics, 79(4):1061–1078, 2011.

Bateman, A., Heger, A., Sonnhammer, E. L. L., Mistry,
J., Clements, J., Tate, J., Hetherington, K., Holm, L.,
Punta, M., Coggill, P., Eberhardt, R. Y., Eddy, S. R., and
Finn, R. D. Pfam: the protein families database. Nucleic
Acids Research, 42(D1):D222–D230, 11 2013.
ISSN
0305-1048. doi: 10.1093/nar/gkt1223.

Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A
neural probabilistic language model. Journal of machine
learning research, 3(Feb):1137–1155, 2003.

14

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Bepler, T. and Berger, B. Learning protein sequence
In In-
embeddings using information from structure.
ternational Conference on Learning Representations,
2019. URL https://openreview.net/forum?
id=SygLehCqtm.

Collobert, R. and Weston, J. A uniﬁed architecture for natu-
ral language processing: Deep neural networks with mul-
titask learning. In Proceedings of the 25th international
conference on Machine learning, pp. 160–167. ACM,
2008.

Cuff, J. A. and Barton, G. J. Evaluation and improvement of
multiple sequence methods for protein secondary struc-
ture prediction. Proteins: Structure, Function, and Bioin-
formatics, 34(4):508–519, 1999.

Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.
In Advances in Neural Information Processing Systems,
pp. 3079–3087, 2015.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. CoRR, abs/1810.04805, 2018. URL
http://arxiv.org/abs/1810.04805.

Eddy, S. R. Proﬁle hidden markov models. Bioinformatics,
14(9):755–763, 10 1998. ISSN 1367-4803. doi: 10.1093/
bioinformatics/14.9.755.

Ekeberg, M., Lövkvist, C., Lan, Y., Weigt, M., and Au-
rell, E.
Improved contact prediction in proteins: Us-
ing pseudolikelihoods to infer potts models. Phys. Rev.
E, 87:012707, Jan 2013a. doi: 10.1103/PhysRevE.87.
012707. URL https://link.aps.org/doi/10.
1103/PhysRevE.87.012707.

Ekeberg, M., Lövkvist, C., Lan, Y., Weigt, M., and Aurell,
E. Improved contact prediction in proteins: using pseu-
dolikelihoods to infer potts models. Physical Review E,
87(1):012707, 2013b.

Elnaggar, A., Heinzinger, M., Dallago, C., and Rost, B.
End-to-end multitask learning, from protein language to
protein features without alignments. bioRxiv, pp. 864405,
2019.

Figliuzzi, M., Jacquier, H., Schug, A., Tenaillon, O., and
Weigt, M. Coevolutionary landscape inference and the
context-dependence of mutations in beta-lactamase tem-1.
Molecular biology and evolution, 33(1):268–280, 2016.

Fowler, D. M. and Fields, S. Deep mutational scanning: a
new style of protein science. Nature methods, 11(8):801,
2014.

Fox, N. K., Brenner, S. E., and Chandonia, J.-M. SCOPe:
Structural Classiﬁcation of Proteins—extended, inte-
grating SCOP and ASTRAL data and classiﬁcation
of new structures. Nucleic Acids Research, 42(D1):
D304–D309, January 2014.
ISSN 0305-1048, 1362-
4962. doi: 10.1093/nar/gkt1240. URL https://
academic.oup.com/nar/article-lookup/
doi/10.1093/nar/gkt1240.

Gabaldon, T. Evolution of proteins and proteomes: a phy-
logenetics approach. Evol Bioinform Online, 1:51–61,
2007.

Göbel, U., Sander, C., Schneider, R., and Valencia, A. Corre-
lated mutations and residue contacts in proteins. Proteins:
Structure, Function, and Bioinformatics, 18(4):309–317,
1994.

Gray, V. E., Hause, R. J., Luebeck, J., Shendure, J., and
Fowler, D. M. Quantitative missense variant effect pre-
diction using large-scale mutagenesis data. Cell systems,
6(1):116–124, 2018.

Harris, Z. S. Distributional structure. Word, 10(2-3):146–

162, 1954.

Hawkins-Hooker, A., Depardieu, F., Baur, S., Couairon, G.,
Chen, A., and Bikard, D. Generating functional protein
variants with variational autoencoders. BioRxiv, 2020.

Heinzinger, M., Elnaggar, A., Wang, Y., Dallago, C.,
Nechaev, D., Matthes, F., and Rost, B. Modeling aspects
of the language of life through transfer-learning protein
sequences. BMC bioinformatics, 20(1):723, 2019.

Hopf, T., Ingraham, J., Poelwijk, F., Scharfe, C., Springer,
M., Sander, C., and Marks, D. Mutation effects predicted
from sequence co-variation. Nature Biotechnology, 35:
128–135, 1 2017.

Hormoz, S. Amino acid composition of proteins reduces
deleterious impact of mutations. Scientiﬁc reports, 3:
2919, 2013.

Huerta-Cepas, J., Forslund, S. K., Bork, P., Hernández-
Plaza, A., von Mering, C., Szklarczyk, D., Heller, D.,
Cook, H., Jensen, L., Mende, D. R., Letunic, I., and
Rattei, T. eggNOG 5.0: a hierarchical, functionally and
phylogenetically annotated orthology resource based on
5090 organisms and 2502 viruses. Nucleic Acids Re-
search, 47(D1):D309–D314, 11 2018. ISSN 0305-1048.
doi: 10.1093/nar/gky1085.

Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y.
What is the best multi-stage architecture for object recog-
nition? In 2009 IEEE 12th international conference on
computer vision, pp. 2146–2153. IEEE, 2009.

15

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Johnson, J., Douze, M., and Jégou, H. Billion-scale similar-
ity search with gpus. CoRR, abs/1702.08734, 2017. URL
http://arxiv.org/abs/1702.08734.

ﬁelds. In International Conference on Research in Com-
putational Molecular Biology, pp. 173–174. Springer,
2014.

Jones, D. T. and Kandathil, S. M. High precision in pro-
tein contact prediction using fully convolutional neural
networks and minimal sequence features. Bioinformatics,
34(19):3308–3315, 2018.

Jones, D. T., Buchan, D. W., Cozzetto, D., and Pontil, M.
Psicov: precise structural contact prediction using sparse
inverse covariance estimation on large multiple sequence
alignments. Bioinformatics, 28(2):184–190, 2011.

Kim, Y., Jernite, Y., Sontag, D., and Rush, A. M. Character-
aware neural language models. In Proceedings of the Thir-
tieth AAAI Conference on Artiﬁcial Intelligence, Febru-
ary 12-17, 2016, Phoenix, Arizona, USA., pp. 2741–2749,
2016. URL http://www.aaai.org/ocs/index.
php/AAAI/AAAI16/paper/view/12489.

Klausen, M. S., Jespersen, M. C., Nielsen, H., Jensen, K. K.,
Jurtz, V. I., Sonderby, C. K., Sommer, M. O. A., Winther,
O., Nielsen, M., Petersen, B., and Marcatili, P. NetSurfP-
2.0: Improved prediction of protein structural features
by integrated deep learning. Proteins, 87(6):520–527, 06
2019.

Kondrashov, F. A., Bork, P., Sunyaev, S., and Ramensky,
V. Impact of selection, mutation rate and genetic drift
on human genetic variation. Human Molecular Genet-
ics, 12(24):3325–3330, 12 2003. ISSN 0964-6906. doi:
10.1093/hmg/ddg359. URL https://doi.org/10.
1093/hmg/ddg359.

Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K., and
Moult, J. Critical assessment of methods of protein struc-
ture prediction (casp)—round xiii. Proteins: Structure,
Function, and Bioinformatics, 87(12):1011–1020, 2019.

Lapedes, A. S., Giraud, B. G., Liu, L., and Stormo,
G. D. Correlated mutations in models of protein se-
quences: phylogenetic and structural effects. Lecture
Notes-Monograph Series, pp. 236–256, 1999.

Levitt, M. Conformational preferences of amino acids
in globular proteins. Biochemistry, 17(20):4277–4285,
1978.

Maaten, L. v. d. and Hinton, G. Visualizing data using
t-sne. Journal of machine learning research, 9(Nov):
2579–2605, 2008.

Madani, A., McCann, B., Naik, N., Keskar, N. S., Anand,
N., Eguchi, R. R., Huang, P.-S., and Socher, R. Progen:
Language modeling for protein generation. arXiv preprint
arXiv:2004.03497, 2020.

Marks, D. S., Colwell, L. J., Sheridan, R., Hopf, T. A.,
Pagnani, A., Zecchina, R., and Sander, C. Protein 3d
structure computed from evolutionary sequence variation.
PloS one, 6(12):e28766, 2011.

Mikolov, T., Sutskever, I., Deoras, A., Le, H.-S., Kom-
brink, S., and Cernocky, J. Subword language modeling
with neural networks. preprint (http://www. ﬁt. vutbr.
cz/imikolov/rnnlm/char. pdf), 8, 2012.

Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁ-
cient estimation of word representations in vector space.
In 1st International Conference on Learning Represen-
tations, ICLR 2013, Scottsdale, Arizona, USA, May 2-
4, 2013, Workshop Track Proceedings, 2013. URL
http://arxiv.org/abs/1301.3781.

Mirdita, M., von den Driesch, L., Galiez, C., Martin, M. J.,
Söding, J., and Steinegger, M. Uniclust databases of
clustered and deeply annotated protein sequences and
alignments. Nucleic acids research, 45(D1):D170–D176,
2017.

Morcos, F., Pagnani, A., Lunt, B., Bertolino, A., Marks,
D. S., Sander, C., Zecchina, R., Onuchic, J. N., Hwa,
T., and Weigt, M. Direct-coupling analysis of residue
coevolution captures native contacts across many pro-
tein families. Proceedings of the National Academy of
Sciences, 108(49):E1293–E1301, 2011.

Moult, J., Fidelis, K., Kryshtafovych, A., Schwede, T., and
Tramontano, A. Critical assessment of methods of protein
structure prediction: Progress and new directions in round
xi. Proteins: Structure, Function, and Bioinformatics, 84:
4–14, 2016.

Luo, Y., Vo, L., Ding, H., Su, Y., Liu, Y., Qian, W. W.,
Zhao, H., and Peng, J. Evolutionary context-integrated
deep sequence modeling for protein engineering. In In-
ternational Conference on Research in Computational
Molecular Biology, pp. 261–263. Springer, 2020.

Ma, J., Wang, S., Wang, Z., and Xu, J. Mrfalign: protein ho-
mology detection through alignment of markov random

Moult, J., Fidelis, K., Kryshtafovych, A., Schwede, T., and
Tramontano, A. Critical assessment of methods of protein
structure prediction (casp)—round xii. Proteins: Struc-
ture, Function, and Bioinformatics, 86:7–15, 2018.

Overbaugh, J. and Bangham, C. Selection forces and con-
straints on retroviral sequence variation. Science, 292:
1106–1109, 5 2001. doi: 10.1126/science.1059128.

16

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., and Zettlemoyer, L. Deep contextualized
word representations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2018, New Orleans, Louisiana,
USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 2227–
2237, 2018. URL https://aclanthology.info/
papers/N18-1202/n18-1202.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
I. Improving language understanding by generative pre-
training. 2018.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. 2019.

Rao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen,
P., Canny, J., Abbeel, P., and Song, Y. Evaluating pro-
tein transfer learning with tape. In Advances in Neural
Information Processing Systems, pp. 9686–9698, 2019.

Remmert, M., Biegert, A., Hauser, A., and Söding, J. Hh-
blits: lightning-fast iterative protein sequence searching
by hmm-hmm alignment. Nature Methods, 9:173 EP, 12
2011. doi: 10.1038/nmeth.1818.

Repecka, D., Jauniskis, V., Karpus, L., Rembeza, E., Zrimec,
J., Poviloniene, S., Rokaitis, I., Laurynenas, A., Abuajwa,
W., Savolainen, O., et al. Expanding functional protein
sequence space using generative adversarial networks.
bioRxiv, pp. 789719, 2019.

Riesselman, A. J., Ingraham, J. B., and Marks, D. S. Deep
generative models of genetic variation capture the effects
of mutations. Nature Methods, 15(10):816–822, 2018.
ISSN 1548-7105. doi: 10.1038/s41592-018-0138-4.

Riesselman, A. J., Shin, J.-E., Kollasch, A. W., McMahon,
C., Simon, E., Sander, C., Manglik, A., Kruse, A. C., and
Marks, D. S. Accelerating protein design using autore-
gressive generative models. bioRxiv, pp. 757252, 2019.

Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick,
C. L., Ma, J., and Fergus, R. Biological structure and func-
tion emerge from scaling unsupervised learning to 250
million protein sequences. bioRxiv, pp. 622803, 2019.

Seemayer, S., Gruber, M., and Söding, J. Ccmpred—fast
and precise prediction of protein residue–residue contacts
from correlated mutations. Bioinformatics, 30(21):3128–
3130, 2014.

Senior, A., Jumper, J., and Hassabis, D. AlphaFold: Using
AI for scientiﬁc discovery, 12 2018. URL https://
deepmind.com/blog/alphafold/.

Söding, J. and Remmert, M. Protein sequence comparison
and fold recognition: progress and good-practice bench-
marking. Current opinion in structural biology, 21(3):
404–411, 2011.

Strodthoff, N., Wagner, P., Wenzel, M., and Samek, W.
Udsmprot: universal deep sequence models for protein
classiﬁcation. Bioinformatics, 36(8):2401–2409, 2020.

Suzek, B. E., Huang, H., McGarvey, P., Mazumder, R.,
and Wu, C. H. UniRef: comprehensive and non-
redundant UniProt reference clusters. Bioinformatics,
23(10):1282–1288, May 2007.
ISSN 1367-4803.
doi: 10.1093/bioinformatics/btm098. URL https:
//academic.oup.com/bioinformatics/
article/23/10/1282/197795.

Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu,
C. H., and Consortium, U. Uniref clusters: a comprehen-
sive and scalable alternative for improving sequence sim-
ilarity searches. Bioinformatics, 31(6):926–932, 2015.

The UniProt Consortium. The universal protein resource
(uniprot). Nucleic Acids Research, 36(suppl_1):D190–
D195, 11 2007.
ISSN 0305-1048. doi: 10.1093/nar/
gkm895.

Thomas, J., Ramakrishnan, N., and Bailey-Kellogg, C.
Graphical models of residue coupling in protein fami-
lies. IEEE/ACM Transactions on Computational Biology
and Bioinformatics, 5(2):183–197, 2008.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

Vig, J., Madani, A., Varshney, L. R., Xiong, C., Socher, R.,
and Rajani, N. F. Bertology meets biology: Interpreting
attention in protein language models. arXiv preprint
arXiv:2006.15222, 2020.

Wang, A. and Cho, K. BERT has a mouth, and it must
speak: BERT as a markov random ﬁeld language model.
CoRR, abs/1902.04094, 2019. URL http://arxiv.
org/abs/1902.04094.

Wang, S., Sun, S., Li, Z., Zhang, R., and Xu, J. Accu-
rate de novo prediction of protein contact map by ultra-
deep learning model. PLoS computational biology, 13(1):
e1005324, 2017.

Wang, S.-W., Bitbol, A.-F., and Wingreen, N. Revealing
evolutionary constraints on proteins through sequence
analysis. PLoS Comput Biol, 15(4), 2019. doi: 10.1371/
journal.pcbi.1007010.

17

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Weigt, M., White, R. A., Szurmant, H., Hoch, J. A., and
Hwa, T. Identiﬁcation of direct residue contacts in protein–
protein interaction by message passing. Proceedings of
the National Academy of Sciences, 106(1):67–72, 2009.

Xu, J. Distance-based protein folding powered by deep

learning. arXiv preprint arXiv:1811.03481, 2018.

Yang, K. K., Wu, Z., Bedbrook, C. N., and Arnold, F. H.
Learned protein embeddings for machine learning. Bioin-
formatics, 34(15):2642–2648, 2018.

Yang, K. K., Wu, Z., and Arnold, F. H. Machine-learning-
guided directed evolution for protein engineering. Nature
methods, 16(8):687–694, 2019.

Yanofsky, C., Horn, V., and Thorpe, D. Protein structure
relationships revealed by mutational analysis. Science,
146(3651):1593–1594, 1964.

18

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

A. Approach & Data
1. Pre-training datasets
UniParc pre-training dataset. A series of development
models are trained on UniParc (The UniProt Consortium,
2007) Release 2019_01 which contains approximately
250M sequences. 1M sequences are held-out randomly
for validation. These models were used in the preprint of
this paper, and representations from the models are used in
Figures 1, 2, and 3.

UniRef pre-training datasets. Datasets are based on
UniRef (Suzek et al., 2015) dated March 28, 2018 to permit
a temporal hold-out with CASP13. 10% of UniRef50 clus-
ters are randomly selected as a held-out evaluation set, yield-
ing 3.02 million representative sequences for evaluation.
Three training datasets are used, removing all sequences be-
longing to clusters selected for the evaluation set: (i) UR100,
124.9M UniRef100 representative sequences; (ii) UR50/S,
27.1M UniRef50 representative sequences; (iii) UR50/D,
124.9M UniRef50 cluster members sampled evenly by clus-
ter. To ensure a deterministic validation set, we removed
sequences longer than 1024 amino acids from the validation
set.

2. Downstream tasks
Remote Homology. A dataset of remote homolog pairs is
derived from SCOPe (Fox et al., 2014) containing 256,806
pairs of remote homologs at the fold level and 92,944 at
the superfamily level, consisting of 217 unique folds and
366 unique superfamilies. Creation of the dataset is detailed
below in the section on remote homology.

Linear projections. Five-fold cross validation datasets
implementing structural hold-outs at the family, superfam-
ily, and fold level are constructed using SCOPe (Fox et al.,
2014). Independently for each level of structural hold-out,
the domains are split into 5 equal sets, i.e. ﬁve sets of folds,
superfamilies, or families. This ensures that for each of the
ﬁve partitions, structures having the same classiﬁcation do
not appear in both the train and test sets. For a given classiﬁ-
cation level each structure appears in a test set once, so that
in the cross validation experiment each of the structures will
be evaluated exactly once. Scores reported are the mean and
standard deviation over each of the ﬁve test sets. Further
details on construction of the dataset are given below in the
section on linear projections.

Secondary structure prediction. All downstream mod-
els are trained using the Netsurf (Klausen et al., 2019) train-
ing dataset containing 10,837 examples with labels and
HMM proﬁles. Netsurf features are replicated for CASP13
domains using MMseqs2 (Steinegger & Söding, 2017) on

the Uniclust90 (Mirdita et al., 2017) dataset released April
2017. For test sets we use (i) the standard CB513 (Cuff
& Barton, 1999) test set of 513 sequences with sequence
identity hold-out at 25% identity; and (ii) the 34 publicly
available CASP domains, using DSSP (Kabsch & Sander,
1983) to label secondary structure, with temporal hold-out
for both pre-training and downstream data.

Contact prediction. All downstream models are trained
using the training and test sets of Wang et al. (2017). Com-
parisons with RaptorX features use features from Wang et al.
(2017) and Xu (2018). The following test sets are used: (i)
RaptorX Test, 500 domains (25% sequence identity hold-
out); (ii) CASP11, 105 domains (25% sequence identity
hold-out); (iii) CASP12, 55 domains (temporal hold-out
from training data but not pre-training data); (iv) CASP13,
34 publicly released domains (temporal hold-out from train-
ing data and pre-training data). The training set consists
of 6,767 sequences with contact map targets, a subset of
PDB created in February 2015 (Wang et al., 2017). The use
of an earlier version of the PDB ensures a temporal hold-
out w.r.t. both CASP12 and CASP13. Additionally, Wang
et al. (2017) implemented a sequence identity hold-out for
Test and CASP11 by removing proteins from the training
set which share >25% sequence identity or have BLAST
E-value <0.1 with the proteins in these test sets.

Mutational effect prediction. The model is ﬁne-tuned
on deep mutational scanning datasets compiled by Gray
et al. (2018) and Riesselman et al. (2018).

3. The Transformer

We use a deep Transformer encoder model (Vaswani et al.,
2017b; Devlin et al., 2018b), processing input as charac-
ter sequences of amino acids. In contrast to recurrent and
convolutional neural networks, the Transformer makes no
assumptions on the ordering of the input and instead uses
position embeddings. Particularly relevant to protein se-
quences is the Transformer’s ability to model long range
dependencies, which are not effectively captured by RNNs
or LSTMs (Khandelwal et al., 2018). A key factor affecting
the performance of LSTMs on these tasks is the path lengths
that must be traversed by forward activation and backward
gradient signals in the network (Hochreiter et al., 2001).
It is well known that structural properties of protein se-
quences are reﬂected in long-range dependencies. Direct
coupling analysis (Lapedes et al., 1999; Thomas et al., 2008;
Weigt et al., 2009) which aims to detect pairwise depen-
dencies in multiple sequence alignments uses a Markov
Random Field (Potts Model) which models the complete
sequence with pairwise coupling parameters. Similarly, the
Transformer builds up a representation of a sequence by
alternating self-attention with non-linear projections. Self-

19

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

attention structures computation so that each position is
represented by a weighted sum of the other positions in the
sequence. The attention weights are computed dynamically
and allow each position to choose what information from
the rest of the sequence to integrate at every computation
step.
Developed to model large contexts and long range dependen-
cies in language data, self-attention architectures currently
give state-of-the-art performance on various natural lan-
guage tasks, mostly due to the Transformer’s scalability in
parameters and the amount of context it can integrate (De-
vlin et al., 2018b). The tasks include token-level tasks
like part-of-speech tagging, sentence-level tasks such as
textual entailment, and paragraph-level tasks like question-
answering.

Scaled dot-product attention. Self-attention takes a se-
quence of vectors (h1, . . . , hn) and produces a sequence
of vectors (h(cid:48)
n) by computing interactions between
all elements in the sequence. The Transformer model uses
scaled dot-product attention (Vaswani et al., 2017b):

1, . . . , h(cid:48)

√
A(h) = softmax(1/
d Q(h)K(h)T )V (h)

(2)

Here the query Q, key K, and value V , are projections of
the input sequence to n × d matrices where n is the length
of the sequence and d is the inner dimension of the ma-
trix outer product between Q and K. This outer product
parameterizes an n × n map of attention logits, which are
rescaled, and passed through the softmax function row-wise,
thereby representing each position of the sequence in the
output as a convex combination of the sequence of values V .
One step of self-attention directly models possible pairwise
interactions between all positions in the sequence simulta-
neously. Note the contrast to recurrent and convolutional
models which can only represent long-range context through
many steps, and the parallel in inductive bias with the ex-
plicit pairwise parameterization of Markov Random Fields
in widespread use for modeling protein MSAs.
Multi-headed self-attention concatenates the output of t
independent attention heads:

AMH(x) = A1(x) . . . At(x)

(3)

Use of multiple heads enables representation of different
inter-position interaction patterns.

Architecture The Transformer models (Vaswani et al.,
2017b) in this work take a sequence of tokens (x1, . . . , xn)
and output a sequence of log probabilities (y1, . . . , yn)
which are optimized using the masked language modeling
objective. The computation proceeds through a series of
residual blocks producing hidden states, each a sequence of
vectors (h1, . . . , hn) with embedding dimension d.

The Transformer model architecture consists of a series
of encoder blocks interleaving two functions: a multi-
headed self-attention computing position-position interac-
tions across the sequence, and a feed-forward network ap-
plied independently at each position.
The attention unit:

UAT(h) = P (AMH(n(h)))

(4)

Applies one step of multi-headed scaled dot-product atten-
tion to the normalized input, denoted by n(x), projecting
the result into the residual path.
The feed-forward network (with the output state of P1 deﬁn-
ing the “MLP dimension”):

UFF(h) = P2(g(P1(n(h)))

(5)

Passes the normalized input through a position-independent
multi-layered perceptron (MLP) with activation function
g(x).
The full Transformer block:

B(h) :

h ← h + UAT(h)
h ← h + UFF(h)

Successively applies the self-attention unit, and the feed-
forward network on a residual path.
The Transformer model:

Transformer(x) :

h ← E(x) + H(x)
h ← Bk(h) for k ∈ 1 . . . K
y ← W (h)

Consists of an embedding step with token E(x) and posi-
tional H(x) embeddings, followed by K layers of Trans-
former blocks, before a projection W to log probabilities.
The raw input sequence is represented as a sequence of 1-
hot vectors of dimension 25, which is passed through E(x)
the learned embedding layer before being presented to the
ﬁrst Transformer layer.
The models trained in the paper use pre-activation
blocks (He et al., 2016), where the layer normalization (Ba
et al., 2016) is applied prior to the activation as in Radford
et al. (2019b), enabling stable training of deep Transformer
networks. No dropout is used. All projections include bi-
ases, except for the token and positional embeddings. We
use learned token embeddings, and harmonic positional em-
beddings as in (Vaswani et al., 2017b). The feed-forward
network uses the Gaussian error linear unit (Hendrycks &
Gimpel, 2016) activation function. We initialize all lay-
ers from a zero centered normal distribution with standard

20

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

√
deviation 0.02, and re-scale the initialization of the projec-
L where L is the number
tions into the residual path by 1/
of residual layers. All biases are initialized to zero. The
query, key, and value projections are to d dimensions, and
the hidden dimension of the feed-forward network is 4d.

4. Pre-trained Transformer Models
UniParc development models We experimented with
Transformer models of various depths, including a 36-layer
Transformer with 708.6 million parameters, and a 12-layer
model with 85.1M parameters trained. Development models
were trained on UniParc. Details are in Table A.1.

UniRef models We train 34-layer models with 669.2M
parameters across different datasets and fractions of training
data. Additionally we train 6 and 12-layer models. These
models are detailed in Table A.2.

ESM-1b The ESM-1b hyperparameter sweep and model
is described in detail in Appendix B. In brief, ESM-1b is
the result of an extensive hyperparameter sweep that was
performed on smaller 12-layer models. ESM-1b is the result
of scaling up that model to 33 layers. Compared to the
Uniref models, the main changes in ESM-1b are: higher
learning rate; dropout after word embedding; learned posi-
tional embeddings; ﬁnal layer norm before the output; and
tied input/output word embedding.

Pre-training task The masked language modeling pre-
training task follows Devlin et al. (2018b). Speciﬁcally,
we select as supervision 15% of tokens randomly sampled
from the sequence. For those 15% of tokens, we change
the input token to a special “masking” token with 80%
probability, a randomly-chosen alternate amino acid token
with 10% probability, and the original input token (i.e. no
change) with 10% probability. We take the loss to be the
whole batch average cross entropy loss between the model’s
predictions and the true token for these 15% of amino acid
tokens. In contrast to Devlin et al. (2018b), we do not use
any additional auxiliary prediction losses. The ESM-1b
models, as well as the UniParc development models used
in visualizations and in the supplemental results are trained
with the masking procedure above. The UniRef models used
across the experiments of the main text are trained similarly,
except that for the 15% of tokens selected as prediction
targets, all are replaced by the mask token.

Pre-training details Our model was pre-trained using a
context size of 1024 tokens. As most Uniparc sequences
(96.7%) contain fewer than 1024 amino acids, the Trans-
former is able to model the entire context in a single model
pass. For those sequences that are longer than 1024 to-
kens, we sampled a random crop of 1024 tokens during

each training epoch. The model was optimized using Adam
(β1 = 0.9, β2 = 0.999) with learning rate 10−4. We trained
with 131,072 tokens per batch (128 gpus x 1024 tokens).
The models follow a warm-up period of 16000 updates, dur-
ing which the learning rate increases linearly. Afterwards,
the learning rate follows an inverse square root decay sched-
ule. All models were trained using the fairseq toolkit (Ott
et al., 2019) on 128 NVIDIA V100 GPUs.

5. Evaluating the models for downstream tasks

After pre-training the model with unsupervised learning, we
can adapt the parameters to supervised tasks. By passing the
input sequence (x1, . . . , xn) through our pre-trained model,
we obtain a ﬁnal vector representation of the input sequence
(h1, . . . , hn). During pre-training, this representation is pro-
jected to log probabilities (y1, . . . , yn). Recall that a soft-
max over yi represents the model’s posterior for the amino
acid at position i. These ﬁnal representations (h1, . . . , hn)
are used directly, or ﬁne-tuned in a task-dependent way by
adding additional layers to the model and allowing the gradi-
ents to backpropagate through the weights of the pre-trained
model to adapt them to the new task. Hidden representations
from intermediate layers rather than the ﬁnal layer can also
be used.

6. Language Modeling Baselines

In addition to comparing to past work, we also implemented
deep learning baselines for our experiments.

Frequency (n-gram) models To establish a meaningful
performance baseline on the sequence modeling task (Sec-
tion 3), we construct n-gram frequency-based models for
context sizes 1 ≤ n ≤ 104, applying optimal Laplace
smoothing for each context size. The Laplace smoothing
hyperparameter in each case was tuned on the validation
set. ECE is reported for the best left-conditioning n-gram
model.

Bidirectional LSTM language models We trained state-
of-the-art LSTM (Hochreiter & Schmidhuber, 1997) lan-
guage models on the UR50 dataset. We use the ELMo
model of Peters et al. (2018) which concatenates two inde-
pendent autoregressive language models with left-to-right
and right-to-left factorization. Unlike standard LSTM lan-
guage models, the ELMo model receives context in both
directions and is therefore comparable to the Transformers
we train that also use the whole context of the sequence. We
train two models: (i) the small model has approximately
28.4M parameters across 3 layers, with an embedding di-
mension of 512 and a hidden dimension of 1024; (ii) the
large model has approximately 113.4M parameters across 3
layers, with an embedding dimension of 512 and a hidden
dimension of 4096. The models are trained with a nomi-

21

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

# Layers

# Heads

Embedding Dim MLP Dim # Params

Steps

12
24
36

12
12
20

768
768
1280

3072
3072
5120

85.1M 1.6M
170.2M 300k
708.6M 300k

Table A.1. Hyperparameters for development Transformer models trained on UniParc. Embedding dim is the dimension of the hidden
states at the output of each transformer block. MLP Dim refers to the width of hidden layer P1 in the Transformer’s MLPs.

# Layers

# Heads

Embedding Dim MLP Dim # Params

Data

Steps

6
12
34
34
34

12
12
20
20
20

768
768
1280
1280
1280

3072
3072
5120
5120
5120

840K
42.6M UR50/S
840K
85.1M UR50/S
275K
669.2M UR100
669.2M UR50/S
840K
669.2M UR50/D 906K

Table A.2. Hyperparameters for UniRef Transformer models. Note: UR100 model stopped making signiﬁcant progress on valid loss and
was stopped at 275K updates.

nal batch size of 32,768, with truncated backpropagation
to 100 tokens, dropout of 0.1, learning rate of 8e-4, using
the Adam optimizer with betas of (0.9, 0.999), clip norm
0.1 and warmup of 1500 updates using an inverse square
root learning rate schedule. We searched across a range of
learning rates and found 8e-4 to be optimal.

7. Metric structure experiments
Dataset An orthologous group dataset was constructed
from eggNOG 5.0 (Huerta-Cepas et al., 2018) by selecting
25 COG orthologous groups toward maximizing the size of
the intersected set of species within each orthologous group.
Through a greedy algorithm, we selected 25 COG groups
with an intersecting set of 2,609 species. We shrank the
dataset above by selecting only one species from each of 24
phyla in order to ensure species-level diversity.

8. Remote Homology
Dataset We used the database of SCOP 2.07e ﬁltered
to 40% sequence similarity, provided by the ASTRAL
compendium (Fox et al., 2014). Following standard prac-
tices (Söding & Remmert, 2011), we exclude folds that are
known to be related, speciﬁcally Rossman-like folds (c.2-
c.5, c.27 and 28, c.30 and 31) and four- to eight-bladed β-
propellers (b.66-b.70). This yields 256,806 pairs of remote
homologs at the fold level and 92,944 at the superfamily
level, consisting of 217 unique folds and 366 unique su-
perfamilies. We then perform an 80-20 split, and tune our
hyperparameters on the “training set” and report results on
the held out 20% of the data.

Metrics Given a protein sequence x, with ﬁnal hidden
representation (h1, . . . , hn), we deﬁne the embedding of
the sequence to be a vector e which is the average of the
hidden representations across the positions in the sequence:

n(cid:88)

i=1

hi

e =

1
n

We can compare the similarity of two protein sequences, x
and x(cid:48) having embeddings e and e(cid:48) using a metric in the
embedding space.
We evaluate the L2 distance (cid:107)e−e(cid:48)(cid:107)2 and the cosine distance
e · e(cid:48)/(cid:107)e(cid:107)(cid:107)e(cid:48)(cid:107). Additionally we evaluated the L2 distance
after projecting the e vectors to the unit sphere.

Evaluation To evaluate HHblits (Remmert et al., 2011),
ﬁrst we construct HMM proﬁles for each sequence using
default parameters for ‘hhblits‘, except we use 3 iterations.
Then, we do an all-to-all alignment using ‘hhalign‘ with de-
fault parameters, and use the resulting E-value as a measure
of similarity. Given a query sequence, a sequence is more
similar with a smaller E-value.
The two metrics reported are Hit-10 as introduced in Ma
et al. (2014) and AUC. For both metrics, for each sequence,
we treat it as a query and we rank each other sequence
according to the distance metric used. Following Ma et al.
(2014), for evaluation at the fold level, any domain with the
same fold is a positive; any domain with a different fold is a
negative; and domains belonging to the same superfamily
are excluded. For evaluation at the superfamily level, any
domain with the same superfamily is a positive; any domain
with a different superfamily is a negative; and domains
belonging to the same family are excluded. This ensures

22

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

we speciﬁcally measure how well our models do on ﬁnding
remote homologs.
For Hit-10, we consider the query a success if any of the top
10 sequences is a remote homolog. We report the proportion
of successes averaged across all queries. For AUC, we ﬁrst
compute the AUC under the ROC curve when classifying se-
quences by vector similarity to the query. Then, we average
the AUC across all query sequences.
We found that cosine similarity results in the best Hit-10
scores, while the L2 with unnormalized vectors result in the
best AUC scores, so we report this in Table 2.

Implementation We used the FAISS similarity search
engine (Johnson et al., 2017).

9. Representational similarity-based alignment of

sequences within MSA families

Family selection We use the Pfam database (Bateman
et al., 2013). We ﬁrst ﬁltered out any families whose
longest sequence is less than 32 residues or greater than
1024 residues in length. We then ranked the families by the
number of sequences contained in each family and selected
the 128 largest families and associated MSAs. Finally, we
reduced the size of each family to 128 sequences by uniform
random sampling.

Aligned pair distribution For each family, we construct
an empirical distribution of aligned residue pairs by enu-
merating all pairs of positions and indices that are aligned
within the MSA and uniformly sampling 50,000 pairs.

Unaligned pair distribution We also construct for each
family a background empirical distribution of unaligned
residue pairs. This background distribution needs to con-
trol for within-sequence position, since the residues of two
sequences that have been aligned in an MSA are likely to
occupy similar positions within their respective unaligned
source sequences. Without controlling for this bias, a differ-
ence in the distributions of aligned and unaligned pairs could
arise from representations encoding positional information
rather than actual context. We control for this effect by
sampling from the unaligned-pair distribution in proportion
to the observed positional differences from the aligned-pair
distribution. Speciﬁcally, the following process is repeated
for each pair in the empirical aligned distribution:

1. Calculate the absolute value of the difference of each
residue’s within-sequence positions in the aligned pair.

2. Select a pair of sequences at random.

3. For that pair of sequences, select a pair of residues at
random whose absolute value of positional difference

equals the one calculated above.

4. Verify that the residues are unaligned in the MSA; if so,
add the pair to the empirical background distribution.

5. Otherwise, return to step 2.

This procedure sufﬁces to compute a empirical background
distribution of 50,000 unaligned residue pairs.

Similarity distributions Finally, for each family and
each distribution, we apply the cosine similarity operator to
each pair of residues to obtain the per-family aligned and
unaligned distribution of representational cosine similari-
ties.

10. Linear projections
Dataset We construct ﬁve-fold cross validation datasets
with structural hold-outs at the family, superfamily, and fold
level using SCOPe 2.07 (Fox et al., 2014). We use the full
version of SCOPe 2.07, clustered at 90% sequence iden-
tity, generated on January 23, 2020, and extract the domain
annotations with labels. There are 19,695 domains. Then,
independently for each hold-out level, we split the domains
at the hold-out level into 5 equal sets, i.e. ﬁve sets of folds,
superfamilies, or families. This ensures that for each parti-
tion, structures having the same classiﬁcation do not appear
in both the train and test sets. For a given classiﬁcation
level each structure appears in a test set once, so that in the
cross validation experiment each of the structures will be
evaluated once. Scores reported are the mean and standard
deviation over each of the ﬁve test sets.
For each domain, we ﬁrst obtain the sequence x whose
residues align with the domain speciﬁcation. To construct
the secondary structure labels, we take each CIF ﬁle (pulled
from PDB), and run DSSP (Joosten et al., 2010; Kabsch
& Sander, 1983). If x has any residues where DSSP has
not provided a secondary structure label, we mark them as
missing data and do not supervise for those positions.
To construct the contact map, we obtain Cβ coordinates
from the structure portion of the CIF ﬁle (Cα in the case of
glycine), defaulting to NaN where information is missing,
and ﬁnally calculating pairwise distances and thresholding
at 8Å. Similar to secondary structure, we do not supervise
over NaNs.
We discard any domains where (1) DSSP fails, (2) we are
unable to align the sequence to the structure, or (3) the
domain is longer than 1023 residues. This leaves 15,297
domains.
For the MSA baselines, we query each sequence against the
Uniclust30, 2017 database (Mirdita et al., 2017) with HH-
blits (Remmert et al., 2011) using the default settings with

23

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

additional parameters (n=3, 1e-3). For secondary structure
prediction, we construct HMM proﬁles using HHmake (de-
fault settings). For contact prediction, we apply CCMpred
(Seemayer et al., 2014) implementation of pseudolikelihood
maximization (Balakrishnan et al., 2011; Ekeberg et al.,
2013) using default settings with (n=500) to each MSA,
from which we extract both an output matrix (RL×L), as
well as a sequence proﬁle (RL×K) where L is the length of
the sequence and K is the size of the amino acid vocab, i.e.
25.

Representations To obtain sequence representations, we
provide the sequence of the domain as input to a forward
pass of the Transformer model. We retrieve the activations
into the ﬁnal multi-head attention (after the layer normaliza-
tion), using this as the matrix of sequence representations
(RL×d) where d is the hidden dimension of the model.

Secondary structure projections For secondary struc-
ture, we ﬁt a multi-class logistic regression taking as input
an individual representation hi and as output the secondary
structure label from DSSP. We observe that the logistic re-
gression model’s performance does not change with penalty
settings (L1, L2, no penalty); therefore we report the result
where the L2 penalty is applied during training.

Contact projections For contacts, we ﬁt two linear pro-
jections. Given two representations hi, hj at positions i and
j, we regress to whether residues at positions i and j are in
contact:

P (i,j contact) = sigmoid[(P hi + p) · (Qhj + q) + b]

With learned projections P , Q, vector biases p, q, and scalar
bias b. We use the AdamW optimizer (Loshchilov & Hutter,
2017) to ﬁt the projections and bias term.
For each partition we set aside approximately 12.5% of
the training set as validation. We sweep over a range of
projection dimensions (32 to 512), learning rates (1e-6 to
1e-2) and weight decay values (0 to 0.9). Based on the
best validation Top-L, long-range precision score, we set
the projection dimension to 128, the learning rate to 1e-4,
and the optimizer weight decay to 0.2. We observe that
the precision score does not improve with an increased
projection dimension over 128.
The above setup for contact projections only applies to the
Transformer models and the sequence proﬁle baseline. No
supervision is applied to the CCMpred output.

11. Single-family data and analysis

For each of the three domains used, we extracted all domain
sequences from the Pfam dataset (Bateman et al., 2013)
and located the subset of PDB ﬁles containing the domain,
using the latter to derive ground truth secondary structure
labels (Kabsch & Sander, 1983).
Pre-training is with the masked language modeling ob-
jective, using the same hyperparameters as used to train
the UniParc development models. The domain sequences
were randomly partitioned into training, validation, and
testing datasets. For each family, the training dataset com-
prises 65,536 sequences, the validation dataset comprises
either 16,384 sequences (PF00005 and PF00069) or 8,192
sequences (PF00072), and the test dataset comprises the
remainder.
Each Pfam family also forms an evaluation dataset for linear
projection; from the sequences with corresponding crystal
structures, the training dataset comprises 80 sequences and
the test dataset comprises the remainder.

12. Secondary structure prediction. Deep neural

networks and feature combination

We use features from the ﬁnal hidden representations of
the models. We removed the ﬁnal embedding layer, added
layer norm, and applied a top-level architecture following
(Klausen et al., 2019). In particular, this top-level archi-
tecture consists of two parallel convolution layers and an
identity layer, whose outputs are concatenated in the fea-
ture dimension and fed to a two layer bidirectional LSTM
containing 1024 hidden units and dropout p = 0.5. The
output is then projected to an 8-dimensional feature vector
at each position and the model is trained with a categorical
cross-entropy loss with the Q8 labels. The training data
was obtained from the (Klausen et al., 2019). Secondary
structure labels for the CASP13 test set were constructed
using DSSP.
In feature combination experiments, we used the features
provided by (Klausen et al., 2019) which were generated us-
ing MMseqs2 on the Uniclust90 dataset released April 2017.
For CASP13 experiments, we generated these features us-
ing code provided by (Klausen et al., 2019) on CASP13
domains.
As a baseline, we reimplemented (Klausen et al., 2019)
by replacing the Transformer features with the MMseqs2
features and keeping the top-level architecture. For feature
combination experiments, we projected (a) the features from
this baseline and (b) the features from the Transformer to
the same dimension (512 units), concatenated along the
feature dimension, and fed the resulting tensor to a two
layer bidirectional LSTM with 512 hidden units and dropout
p = 0.3.

24

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

To check our dataset construction, we used the pretrained
weights provided by (Klausen et al., 2019) and evaluated
their model directly in our evaluation pipeline. We were able
to reproduce the values reported in (Klausen et al., 2019).

13. Contact prediction. Deep neural networks and

feature combination

Data We use the datasets and features distributed with
Wang et al. (2017) and Xu (2018). The base features are
those used by RaptorX (Xu, 2018) a state-of-the-art method
in CASP13, including sequence features, PSSM, 3-state
secondary structure prediction, predicted accessibility, one-
hot embedding of sequence, and pairwise features APC-
corrected Potts model couplings, mutual information, pair-
wise contact potential.
We use the training, standard test set, and CASP11 test set
from Wang et al. (2017). We use the CASP12 test set from
Xu (2018). For the CASP13 test set we use the 34 publicly
released domains.
Wang et al. (2017) established training and test sets as fol-
lows. The train (6,367 proteins), valid (400 proteins) and test
(500 proteins) datasets were selected as subsets of PDB25
(each protein having <25% sequence similarity). Proteins
having sequence similarity >25% or BLAST E-value <0.1
with any test or CASP11 protein were excluded from train-
ing data.
All our MSAs (used for the avg and cov combination meth-
ods) are constructed by running HHblits (Remmert et al.,
2011) with 3 iterations and E-value 0.001 against Uniprot20
released on 2016-02; except for CASP12 and CASP13
where we used the four different MSAs released with and
described in Xu (2018). Note that for the Transformer pre-
training UniRef50 from 2018-03 was used; hence no data
which was not already available prior to the start of CASP13
was present during either pre-training or contact prediction
training.

Model architecture On top of the sequence and pairwise
features we use a depth-32 residual network (ResNet) model
to predict binary contacts. The ResNet model architecture
is similar to Wang et al. (2017) and Xu (2018).
The ﬁrst component of the ResNet is a learned sequence
θ (x) which maps sequence features x ∈
pipeline y = f S
RL×d1 to y ∈ RL×d2 with L the length of the protein.
Though f S
θ could be a 1D convolutional network or residual
network as in Wang et al. (2017), we chose our sequence
net to be a simple linear projection from input dimension d1
to d2 = 128 dimensions. The input dimension d1 is either
46 (RaptorX only), 1280 (Transformer hidden state), or
1326 (feature combination). We varied d2 and empirically
determined 128 to be optimal.

The 128-D output y of the sequence net gets converted
to pairwise matrix features z1 with 256 feature maps, by
an outer concatenation operation; i.e. at position i, j we
concatenate yi and yj along the feature dimension, giving
rise to 2 × d2 feature maps. This z1 ∈ R2d2×L×L is then
concatenated in the ﬁrst (feature map or channel) dimension,
with the pairwise features z2 ∈ R6×L×L i.e. the pairwise
RaptorX features described in previous subsection and/or
the msa embedding covariance features (z3 ∈ R256×L×L)
described in the next subsection. As such the concatenated
z ∈ R262×L×L or z ∈ R518×L×L.
The ﬁnal component is the actual 2D residual network op-
erating in z, which computes the binary contact probability
θ (z) with ˆp ∈ RL×L and ˆpij the continuous predicted
ˆp = gR
probability of position i and j of the protein being in con-
tact. The ResNet has an initial 1 × 1 convolutional layer
going to d3 = 128 feature maps, followed by MaxOut over
the feature maps with stride 2, reducing to 64 feature maps.
After this, there are 32 residual blocks. Each residual block
has on its weight path consecutively BatchNorm - ReLU
- Conv 3 × 3 (64 feature maps) - Dropout (0.3) - ReLU -
Conv 3 × 3 (64 feature maps). The residual blocks have
consecutive dilation rates of 1,2,4. This follows Adhikari
(2019). The ﬁnal output is computed with a Conv 3 × 3
(1 output feature map) and sigmoid to produce probability
of contact ˆpij ∈ [0, 1]. As such there are 66 convolutional
layers in the main 2D ResNet.
Note that a number of shortcomings exist from our pipeline
to CASP13 winners (Senior et al., 2018; Xu, 2018); most im-
portantly we use an earlier training dataset of PDB structures
compiled from PDB dated Feb 2015 by Wang et al. (2017),
additionally we do not incorporate more recent develop-
ments like distance distribution prediction, sliding window
on small crops allowing deeper ResNets, auxiliary losses
like torsion angles, or data augmentation.
For reference, the ofﬁcially released AlphaFold (Senior
et al., 2018) predictions achieve a top-L/5,LR and top-L,LR
precision on the same subset of CASP-13 targets of 75.2%
and 52.2% respectively. The discrepancies in the pipeline
explain why our best precisions using RaptorX features are
about 7-9% lower (compare CASP13-AVG (a): 68.0% /
43.4%)

MSA Embedding feature combination. We construct
features based on the embedding of the MSA of a protein
sequence in our training data. We denote the original protein
in our labeled dataset, i.e. query sequence x of length L,
to have corresponding embedding h = Transformer(x) ∈
RL×d, and the embedding of the i-th position to be hi ∈ Rd.
Typically h is the last hidden state from the pre-trained
Transformer model. The mth sequence in the MSA is xm,
with corresponding embedding hm. m ∈ [0, M [ with M

25

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

i = 0 if xm
i

is the gap character; ie hm

the MSA depth. The embeddings are computed by em-
bedding the original sequence xm without inserting gaps
(there is no gap character in our vocabulary), then realign-
ing the embedding according to the alignment between xm
and query sequence x by inserting 0-vectors at position i
if the xm
i,k = 0. We also use
i
i = 1 if xm
indicator variable αm
is non-gap (match state),
i
or αm
is gap. We further compute sequence
weights wm as the commonly used debiasing heuristic to
reduce the inﬂuence of the oversampling of many simi-
lar sequences. The weights are deﬁned in the usual way
with 70% sequence similarity threshold: sequence weight
) > 70%}| which is the in-
wm = 1/|{xm(cid:48)|seqid(xm, xm(cid:48)
verse of the number of sequences xm(cid:48)
that are more than
70% similar to the sequence xm i.e. hamming distance less
than 0.3L.
Now we introduce the average embedding over an MSA:

havg
ik =

1

wmαm

i hm
i,k

m=0

Meﬀ (i)

with per-position denominator Meﬀ (i) = (cid:80)M−1

m=0 wmαm
i
This is effectively a weighted average over the sequence
embeddings in the MSA. Note that if the embeddings were
one-hot encodings of AA identities, we would recover the
position probability matrix (except the absence of a pseudo-
count).
Similarly; we introduce the (uncentered) covariance of the
embeddings, with PCA-projected ¯h:

M−1(cid:88)

M−1(cid:88)

Cijkl =

1

Meﬀ (i, j)

m=0

wmαm
i

¯hm
i,kαm
j

¯hm
j,l

With pairwise position denominator Meﬀ (i, j) =

(cid:80)M−1

m=0 wmαm

j .
i αm

Note that to make above covariance embedding feasible,
we ﬁrst reduce the dimensionality of the embeddings by
projecting onto the ﬁrst 16 PCA directions: ¯hi = P hi
with P ∈ R16×d, giving rise to a covariance per pair
of positions i, j and pair of interacting PCA components
k, l of L × L × 16 × 16. The 256 different k, l pairs of
Cijkl ∈ RL×L×16×16 will now become the feature maps of
z ∈ R256×L×L, such that z16k+l,i,j = Cijkl. We tried train-
ing (rather than ﬁxed PCA) the projection of the features
h → ¯h before covariance (learned linear projection P or
training a 3-layer MLP). We also varied the formulation to
center the embeddings over the MSA (normal covariance)
and to rescale the feature maps with a pre-computed mean
and standard deviation for each feature map corresponding
to a pair of k, l. We found no gains from these variations
over the current formulation. Note that centering with the
average havg as in normal empirical covariance calculation,

(cid:88)

introduces a shift that is independent per protein (because
speciﬁc to the MSA), and independent per position. There-
fore it is not unexpected that the uncentered covariance gives
better (more consistent) features.

14. Mutational Effect
Datasets We used two datasets of variant effect measure-
ments compiled by Gray et al. (2018) and Riesselman et al.
(2018). The ﬁrst dataset is a collection of 21,026 measure-
ments from nine experimental deep mutational scans. The
second dataset contains 712,218 mutations across 42 deep
mutational scans.

Fine-tuning procedure To ﬁne-tune the model to predict
the effect of changing a single amino acid or combination
of amino acids we regress the scaled mutational effect with:

y =

log pi(mt(i)) − log pi(wt(i))

i

Where mt(i) is the mutated amino acid at position i, and
wt(i) is the wildtype amino acid. The sum runs over the
indices of the mutated positions. As an evaluation metric,
we report the Spearman ρ between the model’s predictions
and experimentally measured values.

15. Area under the ROC curve

For a binary classiﬁcation task, the ROC curve plots the true
positive rate against the false positive rate at various clas-
siﬁcation thresholds. The area under the ROC curve gives
a measure that quantiﬁes the model’s ability to distinguish
between classes. Intuitively a perfect classiﬁer has an AUC
of 1, while a uniform random classiﬁer has an AUC of 0.5.

B. ESM-1b Hyperparameter Optimization
Experimental setup We perform a systematic analysis
on Transformer models with 100M parameters. We train
models on the UniRef50 dataset following the same method-
ology described in the rest of this work.
After identifying the best performing settings on 100M pa-
rameter models, we explore scale by training 650M parame-
ter models. All models are trained with a target batch size
of 1M tokens. To accommodate the large batch size, we use
gradient accumulation and distributed data parallel. Under
this setup, each epoch of a 100M parameter model com-
pletes in 1.8 hours on 32 GPUs. Each epoch of a 650M
parameter model completes in 8.5 hours on 64 GPUs.
When studying architectural variants, we assess the quality
of representations from each model after 10 or 12 epochs of
pre-training. We observed that relative performance ranking
of the models does not change after this point. Notably, this

26

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

is still early in training; our best performing model, ESM-1b,
is trained for 56 epochs.

Hyperparameter: Masking and data setup Protein lan-
guage models are trained with a masked language modeling
objective, wherein each input sequence is corrupted by re-
placing a fraction of the tokens with a special mask token.
We train 100M parameter models for 10 epochs and com-
pare their performance on the CB513 test set. We investigate
four masking strategies:

• All masks: following supplemental section 4, 15% of
the input tokens are replaced with a mask token and
predicted.

• All random (uniform): 15% of the input tokens are
replaced with an amino acid selected uniform randomly
and predicted.

• All random (frequency): 15% of the input tokens are
replaced with an amino acid selected according to their
frequency in the dataset and predicted.

• BERT: 15% of the input tokens are selected and pre-
dicted. Of these, 80% are replaced with mask token;
10% with a uniform random amino acid; 10% not
changed.

In all cases, we follow Liu et al. (2019) and dynamically
mask the sequences, such that a new mask is randomly se-
lected at each epoch. Since the input data changes across
runs, language modeling perplexities cannot be fairly com-
pared. Therefore, we evaluate the downstream performance
of the models on a secondary structure benchmark. Table
B.1 ﬁnds that the BERT masking pattern performs better
than the other masking patterns and is therefore used for all
model variations below.

Masking pattern
All masks
All random (uniform)
All random (frequency)
BERT

CB513
60.4
59.3
59.0
60.8

For example, in NLP, Liu et al. (2019) and Devlin et al.
(2018a), use a static batching approach, wherein multiple
proteins are concatenated in the same batch along the se-
quence dimension. This approach is common in NLP, as
sentences that are nearby in a corpus generally relate to
the same topic. As this situation is not the case in protein
language modeling, we analyze the impact of static batch-
ing schemes in protein language models, ﬁnding that they
reduce model performance (Table B.2). 100M parameter
models are evaluated on the secondary structure downstream
task after 10 epochs.

Batching mode
Static
Static [cropping long sequences]
Dynamic

CB513
56.2
57.0
60.8

Table B.2. Comparison of batching modes, 8-class secondary struc-
ture accuracy. Batching modes are investigated on 100M parameter
models and trained for 10 epochs. All models are trained with
a context size of 1024 tokens. In the static batching mode, se-
quences longer than 1024 can span multiple batches. We crop long
sequences for the other batching modes considered. The dynamic
batching scheme we used performs signiﬁcantly better than static
batching modes.

Hyperparameters: further sweeps After ﬁxing the data
distribution to use the BERT masking scheme with dynamic
batching, we next perform a hyperparameter sweep to iden-
tify the best learning rates, initializations and layer norm
placement. We also propose a token dropout scheme which
further improves performance on downstream tasks.

Initializations We compare our initializations to the ini-
tializations presented in Liu et al. (2019), ﬁnding that they
perform similarly (Table B.3).

Initializations
Radford et al. (2019a)
Liu et al. (2019)

CB513
60.9
60.8

Table B.1. Comparison of masking patterns, 8-class secondary
structure prediction accuracy. Models are pre-trained for 10 epochs
on Uniref50. The model and all other hyperparameters remain
ﬁxed between experiments. The BERT masking pattern performs
best and is used for future experiments.

Hyperparameter: Dynamic batching Models in sec-
tion 4 were trained with dynamic batching, which results in
a single sequence in each sample in the batch. This design
choice contrasts with existing language modeling works.

Table B.3. Comparison of initializations, 8-class secondary struc-
ture accuracy. 100M parameter models are trained for 10 epochs
and evaluated on CB513.

Layer norm placement Recent works Child et al. (2019)
and Shoeybi et al. (2020) have suggested that pre-activation
layer norm results in more stable training for larger models.
We investigate the impact of this choice in a smaller con-
trolled setting using 100M parameter transformer models,
ﬁnding that pre-activation layer norm improves performance
on downstream tasks (Table B.4). To account for the lack

27

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Learning rate CB513
1 · 10−4
61.9
2 · 10−4
63.7
4 · 10−4
65.2

Table B.7. A peak learning rate of 4 · 10−4 performs best, 8 class
secondary structure accuracy. 100M parameter models are trained
for 10 epochs.

Positional embeddings Weights CB513
63.7
Harmonic
Harmonic
64.8
65.7
Learned
Learned
66.0

Untied
Tied
Untied
Tied

Table B.8. Comparison of tied embeddings and l.earned or har-
monic positional embeddings, 8 class secondary structure accuracy.
100M parameter models are trained for 17 epochs.

Tying embeddings Although all experiments above are
performed with tied input and output embeddings, we in-
vestigate whether learning these separately could improve
performance. Our results (Table B.8) indicate that sharing
the weights negatively impacts performance. Therefore, we
maintain our initial setup.

Positional embeddings We also investigate the impact of
learning positional embeddings compared to ﬁxed harmonic
embeddings (Vaswani et al., 2017a), ﬁnding that learned
positional embeddings perform better (Table B.8).

of a ﬁnal layer norm, we additionally add a ﬁnal layer norm
before the linear output projection, improving performance
(Table B.5).

Model
Post-activation
Pre-activation

CB513
60.8
61.1

Table B.4. Comparison of layer norm placement, 8 class secondary
structure accuracy. 100M parameter models are trained for 10
epochs and evaluated on CB513.

Final layer norm CB513
61.3
No
Yes
61.9

Table B.5. Including a ﬁnal layer norm before the language mod-
eling head performs best, 8-class secondary structure accuracy.
100M parameter models are trained for 12 epochs.

Token dropout Usually, masked language models are pre-
trained with corrupted inputs and ﬁne-tuned with complete
sequences. We hypothesize that this shift in data distribution
reduces performance on downstream tasks. Therefore, we
propose a token dropout scheme which replaces the mask
token embedding with a ﬁxed tensor of zeros. As 10 − 15%
of positions are masked, the zero tensors cause a change in
the mean statistics of the word embeddings. We therefore
adjust the distribution during ﬁne-tuning by multiplying
the word embeddings by a ﬁxed constant. Formally, if a
mask token is introduced during pre-training with probabil-
ity p = 0.15 · 0.8, then during ﬁne-tuning, we multiply the
embeddings by 1/p. We ﬁnd that this signiﬁcantly improves
performance (Table B.6).

Masking
Mask Tokens
Token Dropout

CB513
61.9
62.6

Table B.6. Token dropout performs better than mask tokens, 8-
class secondary structure accuracy. 100M parameter models are
trained for 12 epochs.

Learning rate Next, we investigate learning rate, ﬁnding
that a peak learning rate of 4 · 10−4 performs best (Table
B.7). Higher learning rates result in instability, while lower
learning rates result in lower performance.
In all cases,
learning rate is warmed up linearly for 16000 steps and then
decayed following an inverse square-root schedule (Raffel
et al., 2020).

28

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

(a) Recovery rates under group translation

(b) Recovery rate under species translation

Figure S1. Learned sequence representations can be translated between orthologous groups and species. Depicted is the recovery rate of
nearest-neighbor search under (a) orthologous group translation, and (b) species translation. In both settings, the trained Transformer
representation space has a higher recovery rate. Results shown for 36-layer dev Transformer pre-trained on UniParc. To deﬁne a linear
translation between protein a and protein b of the same species, we deﬁne the source and target sets as the average of protein a or protein
b across all 24 diverse species. If representation space linearly encodes orthology, then adding the difference in these averages to protein a
of some species will recover protein b in the same species. We use an analogous approach to translate a protein of a source species s to
its ortholog in the target species t. Here, we consider the average representation of the proteins in s and in t. If representation space is
organized linearly by species, then adding the difference in average representations to a protein in species s will recover the corresponding
protein in species t.

29

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Representation type

Overall

Identical amino acid pairs Distinct amino acid pairs

Transformer (trained)
Transformer (untrained)

0.841
0.656

0.870
0.588

0.792
0.468

Table S1. Area under the ROC curve (AUC) of per-residue representational cosine similarities in distinguishing between aligned and
unaligned pairs of residues within a Pfam family. Results displayed are averaged across 128 families.

30

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Representation

PF00005

PF00069

PF00072

Amino acid identity
12-layer (untrained)
12-layer (PF00005)
12-layer (PF00069)
12-layer (PF00072)
12-layer (UniParc)
36-layer (UniParc)

0.516
0.818
0.864
0.816
0.789
0.900
0.902

0.506
0.719
0.725
0.842
0.688
0.872
0.884

0.536
0.835
0.842
0.850
0.888
0.906
0.902

Table S2. Three-class secondary structure prediction accuracy by linear projection. Learning across many protein families produces
better representations than learning from single protein families. Transformer models are trained on three PFAM families: ATP-binding
domain of the ABC transporters (PF00005), Protein kinase domain (PF00069), and Response regulator receiver domain (PF00072). The
single-family models are contrasted with models trained on the full UniParc data. Comparisons are relative to the family (columnwise),
since each of the families differ in difﬁculty. Underline indicates models trained and evaluated on the same family. Representations
learned from single families perform well within the family, but do not generalize as well to sequences outside the family. Representations
trained on UniParc outperform the single-family representations in all cases.

31

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Family

Superfamily

Fold

(a) Linear projections pre-training vs no pre-training

Family

Superfamily

Fold

(b) Linear projections ESM1-b vs HMM Proﬁle

Figure S2. Secondary structure prediction 8-class accuracy distributions for linear projections.
pretraining; (b) comparison of ESM-1b Transformer representations with HMM sequence proﬁles. Density is indicated by color.

(a) Comparison with and without

32

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Family

Superfamily

Fold

(a) Linear projections pre-training vs no pre-training

Family

Superfamily

Fold

(b) Linear projections ESM1-b vs CCMpred

Figure S3. Contact prediction Top-L long-range precision distributions for linear projections. (a) Comparison with and without pretraining;
(b) comparison of ESM-1b Transformer representations with CCMpred predictions. Density is indicated by color.

33

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Figure S4. Eight-class secondary structure prediction accuracy as a function of pre-training ECE. A deep secondary structure predictor is
trained using features from Transformer models over the course of pre-training on UR50/S. The Netsurf training sequences and CB513
test set are used. Averages across three seeds of the downstream model per pre-training checkpoint are plotted, with line of best ﬁt for
each Transformer.

34

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Metric: top-
Test

CASP11

(a) RaptorX
(b) +direct
(c) +avg
(d) +cov
(a) RaptorX
(b) +direct
(c) +avg
(d) +cov

CASP12-AVG (a) RaptorX
(b) +direct
(c) +avg
(d) +cov
(a) RaptorX
(b) +direct
(c) +avg
(d) +cov

CASP12-ENS

CASP13-AVG (a) RaptorX
(b) +direct
(c) +avg
(d) +cov
(a) RaptorX
(b) +direct
(c) +avg
(d) +cov

CASP13-ENS

L/5,LR
84.3 ± .2
86.7 ± .3
87.7 ± .3
87.8 ± .3
77.5 ± .4
78.3 ± .1
80.4 ± .5
80.3 ± .3
72.7 ± .6
74.0 ± .8
74.4 ± 1.4
76.6 ± .7
77.1 ± .9
77.0 ± .6
76.7 ± 1.4
79.7 ± .8
68.0 ± .9
67.4 ± .8
68.1 ± 1.6
70.3 ± 1.3
72.1 ± .8
68.0 ± .7
70.8 ± 2.2
71.9 ± 1.9

L,LR
59.4 ± .2
61.7 ± .4
62.9 ± .4
63.3 ± .2
53.8 ± .3
55.0 ± .1
56.6 ± .4
56.8 ± .2
51.1 ± .2
51.5 ± .5
52.4 ± .5
53.0 ± .3
54.5 ± .4
53.5 ± .6
54.4 ± .7
55.3 ± .2
43.4 ± .4
43.7 ± .4
44.8 ± .8
45.2 ± .5
46.3 ± .5
45.0 ± .6
46.4 ± 1.1
47.2 ± .4

L/5,MR
74.4 ± .1
76.5 ± .3
77.4 ± .2
77.6 ± .2
75.0 ± .5
76.2 ± .4
76.5 ± .4
76.6 ± .4
68.3 ± .5
70.7 ± .7
71.7 ± .6
70.1 ± .3
70.6 ± .6
71.9 ± .9
74.1 ± .8
72.7 ± .5
71.3 ± .4
69.5 ± .9
73.0 ± .6
73.5 ± 1.4
73.6 ± .4
71.4 ± 1.2
75.4 ± .5
75.2 ± 1.5

L,MR
33.0 ± .0
33.8 ± .1
34.0 ± .1
34.0 ± .1
35.6 ± .2
35.9 ± .2
36.3 ± .2
36.5 ± .2
31.2 ± .3
32.4 ± .3
32.2 ± .3
31.9 ± .3
32.4 ± .4
33.1 ± .3
33.0 ± .3
32.7 ± .3
36.5 ± .3
35.5 ± .4
36.9 ± .1
37.0 ± .2
38.1 ± .3
36.6 ± .4
38.1 ± .2
38.3 ± .4

L/5,SR
71.6 ± .1
73.6 ± .2
73.7 ± .3
73.7 ± .2
72.1 ± .6
74.0 ± .5
73.8 ± .5
74.0 ± .4
66.5 ± .2
68.9 ± .9
70.1 ± .2
69.1 ± .5
68.6 ± .4
69.8 ± .7
71.5 ± .3
71.0 ± .6
68.8 ± 1.0
68.1 ± .4
71.2 ± .6
70.2 ± .8
71.0 ± 1.4
70.2 ± .2
73.6 ± .5
72.3 ± .8

L,SR
25.8 ± .0
26.1 ± .1
26.1 ± .1
26.1 ± .0
28.6 ± .2
28.8 ± .2
28.8 ± .1
29.0 ± .0
26.3 ± .1
27.2 ± .2
26.9 ± .2
26.6 ± .1
27.0 ± .1
27.6 ± .2
27.4 ± .2
27.2 ± .2
28.4 ± .0
28.3 ± .2
28.6 ± .2
28.6 ± .3
29.6 ± .1
29.1 ± .2
29.3 ± .2
29.3 ± .2

Table S3. Additional metrics. Feature combination for supervised contact prediction. AVG corresponds to the average of the metrics over
the different MSAs, while in ENS the probabilities are averaged (ensembled) over the different MSA predictions before computing the
metrics.

35

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Figure S5. After pre-training, the Transformer can be adapted to predict mutational effects on protein function. The 34-layer Transformer
model pre-trained on UR50/S is ﬁne-tuned on mutagenesis data. Spearman ρ on each protein when supervised with smaller fractions of
the data.

36

0.60.8AminoglycosidekinaseBRCA1 (Bard1binding)BRCA1 (E3ligase activity)BetaLactamaseE4B (U-boxdomain)Hsp90PSD95 (pdz3domain)Pab1 (RRMDomain)Protein GUbiquitin (E1activation)Yap65 (WWdomain)Training data1%10%30%50%80%bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Amount of training data
Protein
Aminoglycoside kinase
BRCA1 (Bard1 binding)
BRCA1 (E3 ligase activity)
Beta Lactamase
E4B (U-box domain)
Hsp90
PSD95 (pdz3 domain)
Pab1 (RRM Domain)
Protein G
Ubiquitin (E1 activation)
Yap65 (WW domain)

1% data

10% data

30% data

50% data

80% data

0.25 ± 0.07
0.33 ± 0.02
0.16 ± 0.01
0.39 ± 0.03
0.15 ± 0.03
0.29 ± 0.02
0.20 ± 0.02
0.42 ± 0.07
0.34 ± 0.15
0.41 ± 0.06

0.61 ± 0.02
0.32 ± 0.01
0.23 ± 0.03
0.69 ± 0.01
0.23 ± 0.03
0.54 ± 0.01
0.46 ± 0.05
0.60 ± 0.02
0.62 ± 0.03
0.58 ± 0.02
0.38 ± 0.06

0.77 ± 0.01
0.32 ± 0.03
0.28 ± 0.07
0.84 ± 0.01
0.35 ± 0.01
0.67 ± 0.01
0.68 ± 0.01
0.76 ± 0.01
0.85 ± 0.02
0.67 ± 0.02
0.58 ± 0.06

0.81 ± 0.01
0.33 ± 0.03
0.33 ± 0.05
0.88 ± 0.01
0.46 ± 0.03
0.70 ± 0.02
0.76 ± 0.01
0.80 ± 0.01
0.89 ± 0.02
0.72 ± 0.02
0.68 ± 0.05

0.84 ± 0.01
0.35 ± 0.03
0.37 ± 0.04
0.89 ± 0.01
0.53 ± 0.04
0.72 ± 0.05
0.81 ± 0.02
0.83 ± 0.02
0.93 ± 0.01
0.74 ± 0.02
0.78 ± 0.05

Table S4. Aggregate spearman ρ measured across models and datasets. Mean and standard deviations of spearman ρ performance for
the ﬁne-tuned Transformer-34 on intraprotein tasks. Performance was assessed on ﬁve random partitions of the validation set. Model
pre-trained on UR50/S.

37

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

dataset
model
Transformer
LSTM biLM (Large)
Gray, et al. 2018
Riesselman, et al. 2018

spearmanr
Envision

Envision (LOPO) DeepSequence

0.71 ± 0.20
0.65 ± 0.22
0.64 ± 0.21

0.51

0.45

0.70 ± 0.15
0.62 ± 0.19
0.48 ± 0.26

Table S5. Aggregate spearman ρ measure across models and datasets. 34-layer Transformer pre-trained on UR50/S. For intraprotein
models, the train/valid data was randomly partitioned ﬁve times. The mean ± standard deviation across the ﬁve runs is reported. No
standard deviations are reported for LOPO experiments, as the evaluation is performed across all proteins.

38

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Figure S6. Leave-one-out experiment on Envision dataset (Gray et al., 2018). Pre-training improves the ability of the Transformer to
generalize to the mutational ﬁtness landscape of held-out proteins. All mutagenesis data from the protein selected for evaluation are
held out, and the model is supervised with data from the remaining proteins. For each evaluation protein, a comparison is shown for the
34-layer Transformer pre-trained on UR50/S.

39

Ubiquitin (E1 activation)UbiquitinYap65 (WW domain)Beta LactamaseProtein GPSD95 (pdz3 domain)Pab1 (RRM Domain)Hsp90Aminoglycoside kinaseBRCA1 (Bard1 binding)E4B (U-box domain)BRCA1 (E3 ligase activity)0.00.20.40.60.81.0Spearman TransformerGray, et al. 2018bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Model
Transformer-34
UniRep (LSTM)
SeqVec (LSTM)
TAPE (Transformer)
LSTM (S)
LSTM (L)
Transformer-6
Transformer-12
Transformer-34
Transformer-34
ESM-1b

SSP

Contact

Pre-
Training Params CB513 CASP13 Test CASP11 CASP12 CASP13
13.3
(None)
14.3
17.9
16.0
15.3
16.4
19.8
20.7
19.1
30.1
35.9

669.2M
18.2M
93M
38M
28.4M
113.4M
42.6M
85.1M
669.2M
669.2M
652.4M

UR50/S
UR50/S
UR50/S
UR50/S
UR100
UR50/S
UR50/S

56.8
58.4
62.1
58.0
60.4
62.4
62.0
65.4
64.3
69.1
71.6

16.3
21.9
29.0
23.2
24.1
27.8
30.2
37.7
32.7
50.2
56.9

17.7
21.4
25.5
23.8
23.6
26.4
29.9
33.6
28.9
42.8
47.4

14.8
16.8
23.6
20.3
19.9
24.0
25.3
27.8
24.3
34.7
42.7

60.0
60.1
64.0
61.5
63.2
64.1
64.2
67.2
66.5
70.7
72.5

Table S6. Comparison to related methods. Top-L long-range contact precision. Predictions are directly from protein sequence, no
coevolutionary features or MSAs used. Test is RaptorX test set of Wang et al. (2017). Model weights for related work are obtained and
evaluated in our codebase with same downstream architecture, training, and test data.

40

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Ground Truth

ESM-1b (pre-trained)

Linear Projections
Top-L,LR: 21.25

ESM-1b (no pre-training)

Linear Projections
Top-L,LR: 13.75

(a) d1bcoa1 (mu transposase, C-terminal domain) (80 residues)

Ground Truth

ESM-1b (pre-trained)

Linear Projections
Top-L,LR: 1.83

ESM-1b (no pre-training)

Linear Projections
Top-L,LR: 2.75

(b) d1gyoa (Multiheme cytochromes) (109 residues)

Ground Truth

ESM-1b (pre-trained)

Linear Projections
Top-L,LR: 28.04

ESM-1b (no pretraining)

Linear Projections
Top-L,LR: 8.41

(c) d1t92a1(Pili subunits) (107 residues)

CCMpred

Output

Top-L,LR: 27.50

CCMpred

Output

Top-L,LR: 0.92

CCMpred

Output

Top-L,LR: 14.95

Figure S7. Comparison of ground truth contact map, projections from ESM-1b with and without pre-training, and CCMpred output.
Labels indicate SCOPe domain, fold name, and number of residues. Eight domains randomly sampled from fold-level test sets are shown.

41

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Ground Truth

ESM-1b (pre-trained)

ESM-1b (no pre-training)

Linear Projections
Linear Projections
Top-L,LR: 38.30
Top-L,LR: 7.09
(d) d3ddja1 (CBS-domain pair) (141 residues)

CCMpred
Output
Top-L,LR: 31.91

Ground Truth

ESM-1b (pre-trained)

Linear Projections
Top-L,LR: 49.22

ESM-1b (no pre-training)

Linear Projections
Top-L,LR: 6.25

CCMpred
Output
Top-L,LR: 64.06

(e) d3paja1 (alpha/beta-Hammerhead ) (128 residues)

Ground Truth

ESM-1b (pre-trained)

ESM-1b (no pre-training)

Linear Projections
Top-L,LR: 60.13
(f) d4jnca1 (alpha/beta-Hydrolases ) (311 residues)

Linear Projections
Top-L,LR: 3.22

Figure S7. (continued from above)

CCMpred
Output
Top-L,LR: 29.26

42

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Ground Truth

Ground Truth

ESM-1b (pre-trained)

ESM-1b (no pre-training)

Linear Projections
Top-L,LR: 66.23
(g) d5esra1 (alpha/beta-Hydrolases ) (302 residues)

Linear Projections
Top-L,LR: 6.29

ESM-1b (pre-trained)

ESM-1b (no pre-training)

Linear Projections
Top-L, LR: 46.43

Linear Projections
Top-L, LR: 4.46
(h) d3b64a (Tautomerase/MIF) (112 residues)

Figure S7. (continued from above)

CCMpred
Output
Top-L,LR: 29.14

CCMpred
Output
Top-L, LR: 27.68

43

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Supplemental References
Adhikari, B. DEEPCON: protein contact prediction us-
ing dilated convolutional neural networks with dropout.
Bioinformatics, 36(2):470–477, 07 2019.

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.

arXiv preprint arXiv:1607.06450, 2016.

Balakrishnan, S., Kamisetty, H., Carbonell, J. G., Lee, S.-I.,
and Langmead, C. J. Learning generative models for
protein fold families. Proteins: Structure, Function, and
Bioinformatics, 79(4):1061–1078, 2011.

Bateman, A., Heger, A., Sonnhammer, E. L. L., Mistry,
J., Clements, J., Tate, J., Hetherington, K., Holm, L.,
Punta, M., Coggill, P., Eberhardt, R. Y., Eddy, S. R., and
Finn, R. D. Pfam: the protein families database. Nucleic
Acids Research, 42(D1):D222–D230, 11 2013.
ISSN
0305-1048. doi: 10.1093/nar/gkt1223.

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. URL
https://openai.com/blog/sparse-transformers, 2019.

Cuff, J. A. and Barton, G. J. Evaluation and improvement of
multiple sequence methods for protein secondary struc-
ture prediction. Proteins: Structure, Function, and Bioin-
formatics, 34(4):508–519, 1999.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. CoRR, abs/1810.04805, 2018a.
URL http://arxiv.org/abs/1810.04805.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. CoRR, abs/1810.04805, 2018b.
URL http://arxiv.org/abs/1810.04805.

Ekeberg, M., Lövkvist, C., Lan, Y., Weigt, M., and Aurell,
E. Improved contact prediction in proteins: using pseu-
dolikelihoods to infer potts models. Physical Review E,
87(1):012707, 2013.

Fox, N. K., Brenner, S. E., and Chandonia, J.-M. SCOPe:
Structural Classiﬁcation of Proteins—extended, inte-
grating SCOP and ASTRAL data and classiﬁcation
of new structures. Nucleic Acids Research, 42(D1):
D304–D309, January 2014.
ISSN 0305-1048, 1362-
4962. doi: 10.1093/nar/gkt1240. URL https://
academic.oup.com/nar/article-lookup/
doi/10.1093/nar/gkt1240.

Gray, V. E., Hause, R. J., Luebeck, J., Shendure, J., and
Fowler, D. M. Quantitative missense variant effect pre-
diction using large-scale mutagenesis data. Cell systems,
6(1):116–124, 2018.

He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings
in deep residual networks. In European conference on
computer vision, pp. 630–645. Springer, 2016.

Hendrycks, D. and Gimpel, K. Gaussian error linear units

(gelus). arXiv preprint arXiv:1606.08415, 2016.

Hochreiter, S. and Schmidhuber, J. Long short-term memory.
Neural Computation, 9(8):1735–1780, 1997. doi: 10.
1162/neco.1997.9.8.1735.

Hochreiter, S., Bengio, Y., and Frasconi, P. Gradient ﬂow
the difﬁculty of learning long-term

in recurrent nets:
dependencies. 2001.

Huerta-Cepas, J., Forslund, S. K., Bork, P., Hernández-
Plaza, A., von Mering, C., Szklarczyk, D., Heller, D.,
Cook, H., Jensen, L., Mende, D. R., Letunic, I., and
Rattei, T. eggNOG 5.0: a hierarchical, functionally and
phylogenetically annotated orthology resource based on
5090 organisms and 2502 viruses. Nucleic Acids Re-
search, 47(D1):D309–D314, 11 2018. ISSN 0305-1048.
doi: 10.1093/nar/gky1085.

Johnson, J., Douze, M., and Jégou, H. Billion-scale similar-
ity search with gpus. CoRR, abs/1702.08734, 2017. URL
http://arxiv.org/abs/1702.08734.

Joosten, R. P., Te Beek, T. A., Krieger, E., Hekkelman, M. L.,
Hooft, R. W., Schneider, R., Sander, C., and Vriend, G.
A series of pdb related databases for everyday needs.
Nucleic acids research, 39(suppl_1):D411–D419, 2010.

Kabsch, W. and Sander, C. Dictionary of protein secondary
structure: pattern recognition of hydrogen-bonded and
geometrical features. Biopolymers, 22(12):2577–2637,
1983.

Khandelwal, U., He, H., Qi, P., and Jurafsky, D. Sharp
nearby, fuzzy far away: How neural language models use
context. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Volume 1:
Long Papers), pp. 284–294, 2018.

Klausen, M. S., Jespersen, M. C., Nielsen, H., Jensen, K. K.,
Jurtz, V. I., Sonderby, C. K., Sommer, M. O. A., Winther,
O., Nielsen, M., Petersen, B., and Marcatili, P. NetSurfP-
2.0: Improved prediction of protein structural features
by integrated deep learning. Proteins, 87(6):520–527, 06
2019.

Lapedes, A. S., Giraud, B. G., Liu, L., and Stormo,
G. D. Correlated mutations in models of protein se-
quences: phylogenetic and structural effects. Lecture
Notes-Monograph Series, pp. 236–256, 1999.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.

44

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

RoBERTa: A Robustly Optimized BERT Pretraining Ap-
proach. 7 2019. URL http://arxiv.org/abs/
1907.11692.

Loshchilov, I. and Hutter, F. Decoupled weight decay regu-

larization. arXiv preprint arXiv:1711.05101, 2017.

Ma, J., Wang, S., Wang, Z., and Xu, J. Mrfalign: protein ho-
mology detection through alignment of markov random
ﬁelds. In International Conference on Research in Com-
putational Molecular Biology, pp. 173–174. Springer,
2014.

Mirdita, M., von den Driesch, L., Galiez, C., Martin, M. J.,
Söding, J., and Steinegger, M. Uniclust databases of
clustered and deeply annotated protein sequences and
alignments. Nucleic acids research, 45(D1):D170–D176,
2017.

Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,
N., Grangier, D., and Auli, M. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of NAACL-
HLT 2019: Demonstrations, 2019.

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., and Zettlemoyer, L. Deep contextualized
word representations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2018, New Orleans, Louisiana,
USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 2227–
2237, 2018. URL https://aclanthology.info/
papers/N18-1202/n18-1202.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. 2019a.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. 2019b.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer, 2020.

Seemayer, S., Gruber, M., and Söding, J. Ccmpred—fast
and precise prediction of protein residue–residue contacts
from correlated mutations. Bioinformatics, 30(21):3128–
3130, 2014.

Senior, A., Jumper, J., and Hassabis, D. AlphaFold: Using
AI for scientiﬁc discovery, 12 2018. URL https://
deepmind.com/blog/alphafold/.

Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,
J., and Catanzaro, B. Megatron-lm: Training multi-
billion parameter language models using model paral-
lelism, 2020.

Söding, J. and Remmert, M. Protein sequence comparison
and fold recognition: progress and good-practice bench-
marking. Current opinion in structural biology, 21(3):
404–411, 2011.

Steinegger, M. and Söding, J. Mmseqs2 enables sensi-
tive protein sequence searching for the analysis of mas-
sive data sets. Nature biotechnology, 35(11):1026–1028,
2017.

Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu,
C. H., and Consortium, U. Uniref clusters: a comprehen-
sive and scalable alternative for improving sequence sim-
ilarity searches. Bioinformatics, 31(6):926–932, 2015.

The UniProt Consortium. The universal protein resource
(uniprot). Nucleic Acids Research, 36(suppl_1):D190–
D195, 11 2007.
ISSN 0305-1048. doi: 10.1093/nar/
gkm895.

Thomas, J., Ramakrishnan, N., and Bailey-Kellogg, C.
Graphical models of residue coupling in protein fami-
lies. IEEE/ACM Transactions on Computational Biology
and Bioinformatics, 5(2):183–197, 2008.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017a.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017b.

Remmert, M., Biegert, A., Hauser, A., and Söding, J. Hh-
blits: lightning-fast iterative protein sequence searching
by hmm-hmm alignment. Nature Methods, 9:173 EP, 12
2011. doi: 10.1038/nmeth.1818.

Wang, S., Sun, S., Li, Z., Zhang, R., and Xu, J. Accu-
rate de novo prediction of protein contact map by ultra-
deep learning model. PLoS computational biology, 13(1):
e1005324, 2017.

Riesselman, A. J., Ingraham, J. B., and Marks, D. S. Deep
generative models of genetic variation capture the effects
of mutations. Nature Methods, 15(10):816–822, 2018.
ISSN 1548-7105. doi: 10.1038/s41592-018-0138-4.

Weigt, M., White, R. A., Szurmant, H., Hoch, J. A., and
Hwa, T. Identiﬁcation of direct residue contacts in protein–
protein interaction by message passing. Proceedings of
the National Academy of Sciences, 106(1):67–72, 2009.

45

bioRxiv preprint 
The copyright holder for this preprint (which was
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available 

this version posted December 15, 2020. 

https://doi.org/10.1101/622803
; 

doi: 

under a

CC-BY-NC-ND 4.0 International license
.

Xu, J. Distance-based protein folding powered by deep

learning. arXiv preprint arXiv:1811.03481, 2018.

46

