Expert Opinion on Drug Discovery

ISSN: (Print) (Online) Journal homepage: https://www.tandfonline.com/loi/iedc20

Current directions in combining simulation-based
macromolecular modeling approaches with deep
learning

Vikram Khipple Mulligan

To cite this article: Vikram Khipple Mulligan (2021): Current directions in combining simulation-
based macromolecular modeling approaches with deep learning, Expert Opinion on Drug
Discovery, DOI: 10.1080/17460441.2021.1918097
To link to this article:  https://doi.org/10.1080/17460441.2021.1918097

Published online: 31 May 2021.

Submit your article to this journal 

Article views: 107

View related articles 

View Crossmark data

Full Terms & Conditions of access and use can be found at

https://www.tandfonline.com/action/journalInformation?journalCode=iedc20

EXPERT  OPINION  ON  DRUG  DISCOVERY
https://doi.org/10.1080/17460441.2021.1918097

REVIEW
Current directions  in  combining  simulation-based macromolecular  modeling 
approaches  with deep  learning
Vikram Khipple Mulligan

Center  for  Computational  Biology,  Flatiron  Institute,  New  York,  USA

ABSTRACT
Introduction: Structure-guided drug discovery relies on accurate computational methods for modeling 
macromolecules.  Simulations  provide  means  of  predicting  macromolecular  folds,  of  discovering  func-
tion  from  structure,  and  of  designing  macromolecules  to  serve  as  drugs.  Success  rates  are  limited  for 
any of these tasks, however. Recently, deep neural network-based methods have greatly enhanced the 
accuracy of predictions of protein structure from sequence, generating excitement about the potential 
impact  of deep learning.
Areas  covered:  This  review  introduces  biologists  to  deep  neural  network  architecture,  surveys  recent 
successes  of  deep  learning  in  structure  prediction,  and  discusses  emerging  deep  learning-based 
approaches for structure-function analysis and design. Particular focus is given to the interplay between 
simulation-based and  neural network-based approaches.
Expert opinion: As deep learning grows integral to macromolecular modeling, simulation- and neural 
network-based  approaches  must  grow  more  tightly  interconnected.  Modular  software  architecture 
must  emerge  allowing  both  types  of  tools  to  be  combined  with  maximal  versatility.  Open  sharing  of 
code  under  permissive  licenses  will  be  essential.  Although  experiments  will  remain  the  gold  standard 
for  reliable  information  to  guide  drug  discovery,  we  may  soon  see  successful  drug  development 
projects  based  on  high-accuracy  predictions  from  algorithms  that  combine  simulation  with  deep 
learning – the  ultimate validation of this combination’s power.

ARTICLE  HISTORY
Received  31  January  2021  
Accepted  13  April  2021  

KEYWORDS
Deep  learning;  neural 
networks;  macromolecular 
modeling;  protein  structure 
prediction;  drug  design

1. Introduction
Computational  macromolecular  modeling  methods  play  an 
essential  role  in  the  development  of  new  drugs.  They  allow 
analysis  of  function  and  properties  of  target  biomolecules, 
design  of  small-molecule,  peptide,  and  protein  therapeutics, 
and prediction of target-drug interactions to rank designs and 
to prioritize  synthesis and  experimental validation  [1,2].  Many 
current  macromolecular  modeling  approaches  are  based  on 
mechanistic  models  (including  but  not  limited  to  physics- 
based  models);  for  the  purposes  of  this  review,  these  will  be 
referred  to  as  ‘simulations’.  Complementing  these,  there  is 
growing  interest  in  machine  learning  (ML)  approaches,  in 
which  the  performance  of  an  algorithm  at  a  given  data- 
processing  task  improves  with  exposure  to  relevant  data 
through  alteration  of  an 
internal,  non-mechanistic,  data- 
driven  model  [3].  Although  many  ML  technologies  exist, 
recent  advancements  using  deep  neural  networks  to  process 
language  and  images  have  created  much  enthusiasm  for 
‘deep  learning’  approaches  based  on  these  networks  [4].  The 
application  of  deep  learning  to  macromolecular  modeling  is 
a  rapidly  changing  area  of  active  research,  but  one  that 
already  has  produced  some  spectacular  results, particularly  in 
protein  structure  prediction  [5–7].  Effectively  applying  deep 
learning  to  macromolecular  modeling  problems  and  using 
deep 
to  complement  more 

learning-based 

techniques 

traditional  simulation-based  techniques  requires  considerable 
domain  expertise  in  both  structural  biology  and  neural  net-
work  architecture.  This  review  is  intended  to  introduce  biolo-
gists  to  the  central  ideas  in  deep  learning  and  to  survey  the 
problems in macromolecular modeling amenable to solutions 
combining  deep  learning and  simulation.  Because  this  field  is 
being  shaped  by  rapidly  emerging  approaches,  and  because 
some of the commonly used tools for deep learning have only 
been  described  in  preprints,  this  review  covers  material  both 
from  preprints  describing  computational  methods  and  from 
peer-reviewed articles. However, no preprint making any claim 
about  a  potential  drug  or  about  human  health  has  been 
included  here.  When  material  is  not  yet  peer-reviewed,  it  will 
be  noted,  and  the  reader  is  encouraged  to  treat  claims  with 
due skepticism.

2. Deep neural network architecture, hardware, and 
software
Algorithms may be  viewed as  mappings of  inputs to outputs. 
These  can  range  from  very  simple  (e.g.  an  addition  algorithm 
takes  two  numbers  as  input  and  produces  a  single  number, 
the  sum,  as  output)  to  very  complicated  (e.g.  a  protein  struc-
ture  prediction  algorithm  may  take  as  input  an  amino  acid 
sequence  and  produce  as  output  a  list  of  three-dimensional 
coordinates of every atom in a protein). An accurate algorithm 

CONTACT  Vikram  Khipple  Mulligan 
New  York,  NY  10010,  United  States  of  America
©  2021  Informa  UK  Limited,  trading  as  Taylor  &  Francis  Group

vmulligan@flatironinstitute.org 

Center  for  Computational  Biology,  Flatiron  Institute,  8th  Floor  162  Fifth  Avenue, 

2

V. K. MULLIGAN

Article  highlights

● Macromolecular modeling methods produce useful predictions about 
target  protein  structure  and  function,  and  permit  modulatory  site 
identification, in silico docking, and rational molecular design for drug 
development.

● Recent  advances  in  machine  learning  technology  –  particularly  in 
deep  neural  networks  –  have  greatly  increased  the  predictive  power 
of  macromolecular  modeling  methods  in  recent  years.

● Predictions  of  folded  structures  from  sequence,  analysis  and  annota-
tion of protein structure, design of new protein folds and sequences, 
and  approximation  of  difficult-to-compute  energy  terms  used  in 
physical  simulation  all  benefit  from  deep  neural  networks.

● Ongoing challenges include finding suitable tensor representations of 
macromolecular  structures  that  can  serve  as  inputs  and  outputs  of 
deep  neural  networks,  integrating  neural  networks  and  simulation- 
based  approaches,  and  ensuring  that  methods  are  robust,  versatile, 
reusable, and accessible to the research community under permissive 
licenses.

● Despite  the  advantages,  significant  challenges  remain  in  using  deep 
learning  to  model  special  cases  not  considered  when  model  archi-
tecture  was  developed  (such  as  chemical  modifications  of  macromo-
lecules or macromolecules built from exotic chemical building-blocks) 
or phenomena for which few training examples exist; for these cases, 
physics-based  mechanistic  models  are  often  still  more  generalizable.

This box  summarizes key points  contained in the  article.

is  one  with  low  error  (i.e.  low  deviation  of  observed  outputs 
from  expectations)  over  the  domain  of  likely  or  anticipated 
inputs:  a  good  addition  algorithm  should  yield  the  expected 
sum  of  any  two  numbers,  while  a  good  protein  structure 
prediction  algorithm  should  yield  structures  closely  matching 
the true structure for any sequence with a unique native state. 
Where  traditional  programming  involves  writing  code  that 
executes  a  series  of  human-interpretable  steps  to  transform 
inputs  to  outputs,  deep  learning  approaches  place  an  algo-
rithm  with  generic  structure  (a  deep  neural  network)  between 
the inputs and outputs, then optimize the many parameters in 
this  algorithm  that  control  the  mapping  of  inputs  to  outputs 
to  minimize  the  error  between  observed  and  expected  out-
puts  for  some  training  set  consisting  of  pairs  of  inputs  and 
expected  outputs.  After  this  training  phase,  parameters  are 
fixed, and new inputs can be provided to the trained network 
to  produce  new  outputs  –  an  operational  phase  known  as 
feed-forward  evaluation  or 
(Note  that  this 
describes  a  subset  of  ML  known  as  supervised  ML,  in  which 
known input/output pairs can be used for training. There also 
exist unsupervised ML techniques, which find patterns in input 
data  without  known  output  training  examples,  examples  of 
which  will  be  seen.)  Strong  theoretical  arguments  exist  sug-
gesting  that  deep  neural  networks  possess  a  structure  that 
allows  them  to  be  very  effective  universal  function  approxi-
mators  [8,9],  making  them  popular  for  learning  very  compli-
cated underlying functions mapping inputs to outputs.

inference 

[4]. 

In a basic deep neural network (a ‘feed-forward network’ or 
‘multilayer perceptron’), a vector of inputs is sequentially oper-
ated  upon  to  produce  a  series  of  intermediate  vectors,  or 
‘hidden  layers’,  before  a  final  operation  transforms  the  last 
hidden  layer  to  produce  the  vector  of  outputs.  Each  vector 
operation  typically  involves  multiplication  by  a  matrix  of 
weights,  addition  of  a  vector  of  biases  and  element-wise 

application  of  a  nonlinear  function  (Figure  1).  The  matrix 
multiplication  ensures  that  each  element  in  each  layer  can 
influence  every  element  in  the  subsequent  layer,  and  the 
nonlinear  function  ensures  that  the  overall  transformation  of 
inputs to outputs is nonlinear, allowing approximation of non-
linear  functions.  The  depth  of  the  neural  network  and  the 
complexity  (number  of  parameters)  of  each  layer  both  deter-
mine the complexity of function that the network can approx-
imate.  However,  more  complicated  networks  require  larger 
training  datasets  to  avoid  overfitting,  or  perfectly  approximat-
ing the input dataset while poorly generalizing to new inputs. 
Both the size of the training dataset and the complexity of the 
model  also  impact  the  computational  expense  of  training. 
Many  variations  on  the  basic  feed-forward  network  shown  in 
Figure  1  exist,  performing  operations  on  multi-dimensional 
arrays  or  tensors  and  offering  improvements  in  training  and 
stability  (e.g.  residual  neural  networks  [10],  normalization 
schemes  [11])  or  strategies  for  tackling  specialized  tasks 
using  fewer  parameters  (e.g.  convolutional  neural  networks 
[12],  recurrent  neural  networks  [13]).  Variations  on  feed- 
forward  neural  networks 
the  applications 
to 
described here are summarized in  Table 1.

relevant 

One  of  the  major  challenges  of  applying  deep  neural  net-
works to macromolecular modeling problems is choosing sui-
table  vector  or  tensor  representations  for  input  and  output 
data. If one wished to train a neural network to predict protein 
structure  from  amino  acid  sequence,  for  example,  one  would 
need to be able to convert amino acid sequences to a suitable 
input  tensor  format  and  to  choose  a  suitable  output  tensor 
format  that  could  easily  be  converted  to  a  3D  structural 
model.  Some  examples  of  input  and  output  representations 
relevant  to  macromolecular  modeling  are  shown  in  Figure  1 
(D).  Although  some  types  of  information,  such  as  discrete 
protein  properties  or  amino  acid  sequences,  are  easily  repre-
sented  using  one-hot  encodings,  finding  useful  tensor  repre-
sentations of complicated information like protein structures is 
an  area  of  active  research,  and  different  representations  may 
be better suited to different situations. Two considerations are 
the ease with which the data may be converted to or from the 
tensor  representation  (which  affects  the  usefulness  of  the 
representation  as  an  input  or  output,  respectively)  and  the 
ease  with  which  a  particular  neural  network  architecture  may 
extract  information  from  a  particular  input  representation  or 
generate  a  particular  output  representation.  In  the  structure 
generation  example,  it  would  be  much  more  challenging  to 
generate  Cartesian  coordinates  of  a  predicted  structure 
directly  using  a  neural  network  than  it  would  be  to  generate 
a  contact  map  matrix,  though  conversion  of  the  former  to 
a  full  3D  structure  is  trivial  while  conversion  of  the  latter 
requires  a  subsequent  simulation  of  moderate  cost.  Often 
one may try many different input and output representations, 
as well as many different network architectures, attempting to 
train  each  before  choosing  the  one  yielding  the  best  perfor-
mance on a given task.

Training  a  neural  network  means  finding  values  for  all 
parameters  to  minimize  output  error  over  the  training  set.  In 
the example in Figure 1(A) through C, this would mean finding 
values  for  all  entries  in  each  layer’s  weight  matrix  and  bias 
vector.  The  neural  network  architecture  allows  not  only 

EXPERT  OPINION ON DRUG DISCOVERY

3

Figure 1.  Three representations of the same simple feed-forward deep neural network. This network transforms a 4-element vector of inputs into a 3-element vector 
of  outputs.  It  uses  two  hidden  layers  in  which  the  intermediate  representation  in  each  is  a  6-element  vector.  (A)  Matrix  representation.  An  input  vector  [x1  . . .  x4] 
(black) is subjected to three nested operations (red, green, and blue), each consisting of multiplication by a matrix of weights w, addition of a vector of biases c, and 
element-wise  application  of  a  nonlinear  activation  function  f,  to  produce  a  final  output  vector  [y1  . . .  y3]  (blue).  The  training  process  optimizes  the  values  of  each 
w and c so that known inputs are mapped to known outputs with minimal error. More generally, tensor operations operating on N-dimensional arrays are separated 
by  elementwise  nonlinear  operations.  A  sufficiently  deep  or  complex  neural  network  can  approximate  any  desired  mapping  of  inputs  to  outputs.  (B)  Common 
stylized representation in which input, hidden intermediate, and output vectors are represented as arrays of circles, with each circle representing an element in the 
vector. Red, green,  and blue sets of  arrows represent the  correspondingly colored operations  shown in panel  A. The flow of information is  left to right, with every 
element of a given layer influenced by every element in the previous layer. In the general case, the input, output, and hidden layers can be N-dimensional tensors. 
(C)  Simplified  stylized  representation  showing  inputs,  hidden  layers,  and  outputs  as  boxes,  with  arrows  representing  the  flow  of  information.  Colors  are  as  in  the 
previous panels. (D) Examples of conversion of input and output data to and from vectors, matrices, or tensors compatible with neural network architecture. Discrete 
categories (for example, protein properties) are easily converted to and from one-hot vector encodings. Amino acid sequences are often represented as 2D matrices 
with  sequence  position  along  one  dimension  and  one-hot  encoding  of  amino  acid  type  along  the  other.  Protein  structures  can  be  represented  in  many  ways 
depending on application, including matrices of Cartesian coordinates of atoms, one-hot matrices encoding contacts between spatially close sequence positions, or 
3D  tensors  of  voxelized  data.  Conversion  to  any  of  these  formats  to  prepare  inputs  is  generally  inexpensive,  but  conversion  from  these  formats  to  a  3D  structure 
ranges  from  trivial  (in  the  atomic  coordinate  case)  to  extremely  challenging  (in  the  voxelized  data  case).

forward  computation  but  backpropagation  to  find  derivatives 
of output error with respect to each parameter by way of the 
chain rule, making it simple to optimize the parameter values 
by  gradient  descent,  and  providing  a  means  of  training  the 
neural network against inputs with known outputs [13]. Since 
the neural network must be evaluated repeatedly during para-
meter optimization, with iteration over all data in the training 
set, both in the reverse direction for gradient computation and 
in  the  forward  direction  to  quantify  error,  training  is  necessa-
rily  a  computationally  expensive  process,  and  can  take  hours 
to weeks for more complex networks with large training data-
sets even on modern computing hardware. Once a network is 

trained,  feed-forward  evaluation  for  a  given  input  is  many 
orders  of  magnitude  less  expensive,  and  will  often  cost  milli-
seconds  to  seconds  for  a  single  input  on  a  modern  central 
processing  unit  (CPU).  Because  tensor  operations  are  easily 
and efficiently parallelized, enormous speedups for both train-
ing  and  inference  are  possible  by  using  graphics  processing 
units  (GPUs)  which  divide  vectors  of  computations  over  hun-
dreds  or  thousands  of  computing  cores  that  are  individually 
slower  than  a  CPU  core  but  which  collectively  can  complete 
work  vectors  more  quickly  [23].  Currently,  the  most  widely 
used  packages  for  building,  training,  and  using  deep  neural 
networks  are  Tensorflow  [24]  and  Pytorch  [25,26],  though 

4

V. K. MULLIGAN

Table 1.  A sampling of deep learning layers and approaches relevant to macromolecular modeling examples discussed in this review. Brief qualitative descriptions 
are  provided  here,  with  mathematical  details  available  in  the  references  listed.
Name
Autoencoders

Early descriptions of the encoder 

Autoencoders  are  useful  for 

Advantages  and  Limitations

An  unsupervised  learning 

Applications

Description

References

denoising  or  otherwise 
‘correcting’  new  inputs  to 
resemble  the  pool  of  training 
inputs  by  discarding  the 
information  that  least 
resembles  the  training  data. 
They  can  also  be  used 
generatively:  since  the 
bottleneck  layer  is  limited  in 
the  information  that  it  can 
store,  random  values  passed  to 
the  decoding  layers  can  be 
used  to  sample  rapidly  from 
the  distribution  of  valid 
outputs.

Convolutions  allow  translation- 

equivariant  recognition  of 
features:  in  an  image,  for 
example,  a  face  can  be 
recognized  regardless  of  its 
position  in  the  image.  They 
also  use  far  fewer  parameters 
per  layer  than  the  equivalent 
basic  feed-forward  layer. 
Sequential  convolutional  layers 
can  often  automatically  learn 
to  recognize  features  of 
increasing  complexity  relevant 
to  the  task  at  hand.  (For 
example,  an  image-processing 
convolutional  neural  net  may 
learn  to  recognize  edges  and 
corners of shapes in the first 2D 
convolutional  layer,  simple 
shapes  in  the  second  layer, 
composite  shapes  in  the  third, 
etc.).  Grid-based  convolutions 
are not  equivariant on rotation, 
column  or  row  swaps,  or  other 
permutations,  however.

GANs  solve  the  problem  of 
defining  a  good  objective 
function  for  success  when 
training  a  generative  model: 
success  means  fooling 
a  simultaneously-trained 
discriminative  model.  Failure 
modes  can  include  the 
generator  and/or  discriminator 
‘memorizing’  the  training  set, 
preventing  generation  of 
anything  new.

Like  grid  convolutions,  graph  and 

other  geometric  convolutions 
are  equivariant  on  translation. 
They  are  also  equivariant  on 
rotation  or  column  or  row- 
swapping  (order  of  node 
indexing).

architecture  in  which,  during 
training,  information  passes 
through  encoding  layers  to 
a  low-dimensional  bottleneck 
layer,  which  feeds  into 
decoding layers that reconstruct 
salient  features  of  the  input. 
This forces the network to learn 
to  produce  at  the  bottleneck 
point  a  minimal,  efficient 
representation  of  the 
distribution  of  inputs  on  which 
it  is  trained.  Trained  encoding 
and/or  decoding  layers  may 
then  be  applied  to  other  tasks. 
Many  variations  exist.

Where  conventional  feed-forward 

networks  layers  apply  to  all 
elements  in  an  input  tensor  at 
once,  convolutional  layers 
apply  iteratively  to  a  sliding 
window  along  one  or  more 
dimensions.  The  output  is 
a  grid  of  dimensionality 
matching  the  input 
representing  features  identified 
by  the  layer.  Variants  in  the 
approach  involve  setting 
a broader stride (the amount by 
which  successive  windows  are 
spaced)  or  dilation  (the 
addition  of  gaps  between  the 
cells  over  which  the 
convolution  is  performed)  in 
order to increase the size of the 
region  over  which  features  are 
identified  (the  receptive  field) 
without  adding  model 
parameters.

GANs  are  a  strategy  for  learning 
to  produce  new  instances  of 
desired  output  by  training  one 
network  (the  generator)  to 
perform  the  task  while 
simultaneously  training 
a  competing  network  (the 
discriminator)  to  separate  the 
output  of  the  generator  from 
real  training  examples.

Convolutional 

layers  
(N-dimensional 
grid)

Generative 

adversarial 
networks 
(GANs)

Graph 

Where conventional convolutional 

convolutional 
neural 
networks 
(GCNNs)

layers  iteratively  slide 
a  window  over  inputs  to  allow 
flow  of information to  adjacent 
cells  in  an  N-dimensional  grid, 
graph  convolutions  allow  flow 
of  information  to  adjacent 
nodes  in  a  graph.  Input  graphs 
are  represented  by  feature  and 
adjacency  tensors,  and  the 
output  is  a  new  graph  tensor 
containing  higher-level 
features that the layer has been 
trained  to  recognize.  Related 
geometric  convolutions  (e.g. 
over  nearby  point  cloud  points 
in  space)  are  also  possible.

problem  are  in  [15,16]. 
Chapter  14  of  [4]  provides  an 
excellent  overview.

Perhaps  the  most  famous 
application  is  to  image 
replacement  and  image 
synthesis  technologies,  such  as 
face-swapping  and  face 
generation  (reviewed  in  [14]). 
Many  other  applications  exist 
in  signal  processing  and 
denoising.  The  DeeplyTough 
network  reviewed  is  an 
encoder  for  protein  structure 
[105].

2D  convolutions  inspired  by 

image processing in the retina 
and  visual  cortex  were 
introduced  in  [12].  Chapter  9 
of  [4]  describes  convolutional 
networks  in  detail.

Convolutional  layers  are 

frequently used to process data 
that are well-expressed as a 1D 
array  (e.g.  time-resolved  data, 
language,  amino  acid 
sequences)  or  2D  array  (e.g. 
images,  contact  or  distance 
maps  from  protein  structure, 
multiple  sequence  alignments). 
3D, 4D, and higher-dimensional 
convolutional  layers  are  less 
commonly  used  due  to  the 
expense  of  iterating  across 
many  dimensions,  though  3D 
convolutions  have  been 
applied  to  binding  pocket 
comparison  [105]  (reviewed 
here).

Many  of  the  image  synthesis 

Introduced  in  a  preprint  in  2014 

[17].

approaches using autoencoders 
also  use  GANs  to  improve  the 
quality  of  the  generated 
output.  The  Huang  group  has 
applied  GANs  to  generative 
models  of  protein  backbones 
(reviewed  here)  [120,121].

GCNNs  were  introduced  in 

a  2016  preprint  [18].  Variants 
conditioned  on  edges  [19] 
and pooling layers that merge 
nodes  [20,21]  have  been 
subsequently  proposed.

Since  macromolecular  structures 
have  no  single  proper  origin, 
orientation,  or  indexing  of 
atoms  or  residues,  graph  and 
other  geometric  convolutional 
layers  are  a  convenient  means 
of  processing  protein 
structures.  A  recent  paper  has 
applied  graph  convolutions  to 
predict  function  from  structure 
[108],  and  a  recent  preprint 
uses  quasi-geodesic 
convolutions  to  compare 
protein  binding  pockets  [106].

(Continued )

other packages like  Theano [27],  Caffe [28], and  the Microsoft 
Cognitive  Toolkit  [29]  have  followings  as  well,  and  new  tools 
are rapidly  becoming available.

3. Applications  of  deep  learning  in  macromolecular 
modeling
3.1. Prediction  of  folded  macromolecular  structure

Large-scale  sequencing  efforts  such  as  the  Human  Genome 
Project  [30–32]  have  produced  vast  databases  of  gene 
sequences, but inferring biological function from this informa-
tion is challenging. Nucleic acid sequences are translated into 
amino  acid  sequences  to  produce  proteins,  and  these  repre-
sent the primary functional components of the cell. A protein’s 
sequence  drives  it  to  fold  into  a  specific  three-dimensional 
structure corresponding to a unique lowest-energy conforma-
tional state, and this structure gives rise to the protein’s func-
tion  [33–35].  The  folded  structure  of  a  protein  also  yields 
insight  into  means  by  which  its  activity  may  be  modulated 
and can be used directly for in silico drug docking or to design 
molecules  able  to  fit  into  binding  pockets,  making  this  infor-
mation of high value for drug discovery. Protein structure can 
be  determined  by  x-ray  crystallography,  nuclear  magnetic 
resonance  spectroscopy  (NMR),  or  cryo-electron  microscopy 
(cryo-EM)  (reviewed  in  [36–40]).  Each  of  these  techniques  is 
laborious and can be hindered by poor protein solubility, poor 
stability,  conformational  heterogeneity,  poor  propensity  to 
form  crystals  (in  the  case  of  x-ray  crystallography),  or  large 
size  (in  the  case  of  NMR).  This  has  meant  that  sequence 
databases  have  grown  far  faster  than  structural  databases:  as 
of March 2021, over 197 million protein sequences were avail-
able in the NCBI Reference Sequence Database [41], while only 
162,904  experimentally  determined  protein  structures  were 
found in the Protein Data Bank (PDB) [42].

There is therefore immense interest in reliable methods for 
predicting  protein  structures  from  amino  acid  sequence. 
However,  this  is  a  considerable  computational  challenge:  an 
amino  acid  chain  can  exist  in  an  astronomical  number  of 
conformations,  from  which  one  seeks  the  unique  lowest- 
energy  conformation.  Even  given  accurate  and  rapid  means 
of computing conformational energies (vide infra), the ‘energy 
landscape’  of  a  given  protein  –  the  energy  as  a  function  of 
conformation  –  is  incredibly  rugged,  with  countless  local 
minima,  which 
frustrates  simple  gradient-descent-based 
approaches  to  find  the  global  minimum  [35].  Complicating 
this,  the  notion  of  a  single,  static  folded  state  only  approx-
imates the reality: all proteins undergo dynamic motions, and 
many have regions of intrinsic disorder relevant to their func-
tions [43].  Molecular  dynamics simulations  (MD), in  which  the 
physical  motion  of  atoms  of  a  protein  and  the  surrounding 
solvent  are  simulated  as  a  function  of  time,  can  capture  the 
folding  trajectories  and  conformational  dynamics  of  very 
small,  fast-folding  proteins  [44].  For  stability,  a  simulation 
must  use  femtosecond-scale  time-steps  [45],  and  since  most 
proteins fold on the time-scale of milliseconds to seconds [35], 
an  infeasibly  long  trajectory  of  1012  to  1015  time-steps  would 
be required to capture a single folding event or to predict the 
conformational  ensemble  of  a  disordered  region.  Moreover, 

EXPERT  OPINION ON DRUG DISCOVERY

5

since  every  atom’s  interactions  with  its  neighbors  must  be 
computed  at  every  time-step,  this  approach  scales  poorly 
with  protein  and  solvent  box  size.  MD  is  therefore  an  intrac-
tably  computationally  costly  means  of  predicting  protein 
structure  for  the  vast  majority  of  proteins.  Despite  this,  MD 
can be useful for exploring the local conformational neighbor-
hood  of  a  predicted  structure  to  refine  it  (vide  infra).  To 
predict  folds  de  novo,  much  research  has  gone  into  develop-
ing  clever  search  algorithms  capable  of  exploring  conforma-
tional  space  without  simulating  the  small  steps  of  a  true 
folding  trajectory,  as  well  as  into  devising  energy  functions 
that  can  be  computed  rapidly  for  sampled  conformations  in 
an  iterative  search  for  the  global  minimum  while  still  achiev-
ing high accuracy [46,47].

Computational  structure  prediction  methods  are  assessed  by 
the  biennial  Critical  Assessment  of  protein  Structure  Prediction 
(CASP) competitions. In these competitions, recent experimentally 
determined  protein  structures  are  withheld  temporarily  from 
deposition in the PDB while their sequences are released publicly 
and  blinded  predictions  are  solicited.  The  structures  are  then 
released  publicly,  and  prediction  methods  are  ranked  [48].  Until 
recently, ab  initio  protein  structure prediction  (that is,  prediction 
from  sequence  alone)  has  been  dominated  by  simulation-based 
methods. The Rosetta software suite [49,50] and QUARK software 
[51,52] have consistently ranked among the top prediction meth-
ods  (Figure  2(A,  B)).  Both  use  iterative  Monte  Carlo  searches  of 
protein  conformational  space,  with  moves  accepted  or  rejected 
based on improvements to a scoring function that approximates 
conformational energy. Both limit the search space by using ‘frag-
ment  insertion’  moves,  in  which  pieces  of  proteins  of  known 
structure are used as templates to set the confirmation of portions 
of an unknown protein’s backbone [53,54]. Both also use scoring 
functions with some terms based on statistics from known protein 
structures [46,47,52]. Other simulation-based approaches, such as 
MUFOLD, combine fragment assembly with MD-based refinement 
[55].  Deep  learning  approaches  have  sometimes  been  used  in 
a supporting role to provide inputs that guide simulations, such 
as secondary structure predictions or fragment selections [56,57]. 
Meta-prediction  strategies,  which  examine  the  output  of  many 
structure  prediction  algorithms  and  attempt  to  select  the  best 
models  produced,  have  also  been  used  for  many  years.  Some 
use  rationally  designed  scoring  and  ranking  algorithms  (e.g.  3D- 
Jury) [58], while others use simple neural networks (e.g. Pcons/the 
Structure Prediction Meta Server) [59,60]. In addition to providing 
rankings, some meta-prediction strategies, such as MULTICOM, are 
able  to  recombine  the  best  features  of  models  generated  by 
different algorithms, resulting  in more accurate models on  aver-
age [61].

The  ab 

folded  structure 

initio  prediction  of 

information  about  a  protein  of 

from 
sequence  alone  is  a  somewhat  unusual  case.  Often,  one 
has  additional 
interest, 
such  as  homologues  of  known  structure,  sequences  of  evo-
lutionarily  related  proteins,  or  limited  structural  information 
from  low-resolution  experiments.  The  rise  of  methods  for 
predicting  protein  structure  guided  by  available  prior 
knowledge  has  paralleled  the  development  of  pure  ab  initio 
methods.  When  homologous  protein  structures  are  avail-
able,  programs  like  RaptorX,  I-TASSER,  or  Rosetta  produce 
high-accuracy  predictions  by  homology  modeling  [63–65]. 

6

V. K. MULLIGAN

Table  1.  (Continued). 

Name
Normalization

Description

Advantages  and  Limitations

Applications

References

Intermediate  tensors  can  be 

renormalized  (multiplied  by 
a  constant)  and/or  shifted 
(offset  by  a  constant)  based  on 
averages  over  the  training  set 
(batch  normalization)  or  over 
the  individual  input  (instance 
normalization).

Renormalization  of  intermediate 
layers  avoids  extreme  gradient 
problems,  resulting  in  better 
stability  and  faster  training.

Normalization  is  commonly  used 
in  very  deep  neural  networks 
to  facilitate  training.  Many 
examples  reviewed  here  use 
batch  or  instance 
normalization.

Impact  of  and  theoretical 

arguments  for  batch 
normalization  are  reviewed  in 
[11].

Recurrent  neural 

RNNs  apply  the  same  operation 

Since  RNNs  resemble  very  deep 

RNNs  (and  particularly  LSTMs)  are 

RNNs were introduced in 1986 in 

networks 
(RNNs)

Residual  neural 

networks 
(ResNets)

repeatedly,  usually  to 
a  sequence  of  inputs,  with  the 
output  of  one  application 
contributing  to  the  inputs  to 
the  next,  allowing  state  to  be 
carried  forward  through  the 
applications.  Variants,  such  as 
long  short-term  memory 
models (LSTMs), are better able 
to  retain  and  extract  semantic 
information  from  the  whole 
sequence  and  placement  of 
elements  in  it.

A neural network  in which one or 
more layers are bypassed using 
skip  connections.  A  skip 
connection  sums  the  outputs 
of  layer  n  and  n  +  k  (where 
k  ≥  1)  as  input  into  layer 
n + k + 1, providing a path for 
information  to  flow  around 
layers  n  +  1  through  n  +  k 
during  feed-forward  evaluation 
or  backpropagation  for 
training.

feed-forward  networks  in 
which  parameters  are  shared 
across  all  layers,  they  can 
approximate  complicated 
functions  with  relatively  few 
parameters.  Additionally,  they 
can  generally  be  applied  to 
input  sequences  of  any  length.

commonly  used  to  process 
language  or  other  sequential 
data  in  which  there  exists 
a  semantic  relationship 
between  entries  in  the 
sequence.  They  have  been 
applied  to  analyzing  protein 
sequences  (e.g.  for  functional 
annotation  [108],  as  reviewed 
here).

[13].  They  are  described  in 
detail  in  chapter  10  of  [4]. 
LSTMs  were  introduced  in 
1997  in  [22]

Almost  all  deep  neural  network 

Introduced  in  2016  in  [10].

architectures  for  structure 
prediction,  analysis,  or  design 
reviewed  here  have  enhanced 
training  with  skip  connections.

Theoretical  arguments  and 
practical  demonstrations 
establish  that  skip  connections 
improve  training  of  very  deep 
networks  (dozens  or  hundreds 
of  layers),  partly  by  better 
allowing  gradient  information 
to  propagate  during  training. 
Bypassed  layers  learn  residuals 
or  correction  factors  to  the 
function  approximated  by  the 
rest  of  the  network.

the  G-protein  coupled 

Homology  models  of  target  proteins  sometimes  possess 
sufficient  quality  for  in  silico  drug  screens,  and  this  has 
been  used  to  find  molecules  able  to  bind  to  targets  in 
families  in  which  available  sequences  far  exceed  available 
structures,  such  as 
receptors 
(reviewed  in  [66]).  When  more  limited  experimental  data 
are  available,  simulation-based  methods  can  be  used  in 
conjunction with a modified energy function with additional 
terms  to  reward  accordance  with  experimental  data  and  to 
guide  the  structural  search  to  productive  conformations. 
There  has  been  considerable  success  in  producing  accurate 
fold  predictions  given  partial  NMR  chemical  shifts  [67],  fea-
ture-poor  electron  density  from  low-resolution  X-ray  crystal-
lography  or  cryo-EM  [68,69],  or  information  about  residues 
in  proximity  from  labeling  experiments  [70,71].  When  the 
prior 
form  of  known  contacts 
between  amino  acid  residues  that  are  distant  in  linear 
sequence,  energy  functions  may  be  augmented  with  har-
monic  or  Gaussian  functions  of  the  separation  distance  of 
the  known  contacting  residues,  effectively  serving  as  ‘elas-
tics’  or 
‘pull’  the  contacting  residues 
together  during  structure  prediction  trajectories.  Despite 
the  successes,  however,  simulation-based  prediction  meth-
ods  have  not  had  the  reliability  or  precision  to  displace 
experimental  methods  yielding  high-resolution  structures 
as  the  preferred  method  for  generating  target  structures 

‘rubber  bands’  to 

information 

takes 

the 

for 

for  drug  design.  Although  folds  are  often  broadly  accurate, 
the  models  produced  often  lack  the  atomic-resolution  pre-
in  silico  drug  screening  or  design. 
cision  needed 
Nevertheless,  even  without  such  precision,  models  pro-
duced  in  this  way  can  be  useful  for  structural  analysis  of 
a  drug  target.  For  example,  models  of  the  P.  aeruginosa 
ExoU  cytotoxin  constructed  using  Rosetta  simulations  con-
strained  with  sparse  double  electron-electron  resonance 
(DEER)  data  were  of  sufficient  quality  to  identify  the  surface 
that  mediates  interactions  with  ubiquitin  necessary  for  the 
toxin’s  function  as  confirmed  by  subsequent  mutagenesis 
experiments  [72].  It  has  also  been  shown  that  de  novo 
computational  models  and  models 
informed  by  sparse 
NMR  data  are  frequently  of  sufficient  quality  to  allow  phas-
ing  of  x-ray  diffraction  data  (when  available)  to  produce 
high-precision  models  useful  for  in  silico  drug  screening  or 
design  [73,74].  MD  simulations  of  homology  models  can 
often  refine  structures  further,  in  the  best  cases  correcting 
inaccurate  model  features  to  very  closely  resemble  experi-
mentally  determined  structures  [75].

A  protein’s  amino  acid  sequence  alone  theoretically  con-
tains  all  the  information  needed  to  fold  that  protein,  but 
additional 
large  databases  of 
sequences  of  evolutionarily  related  proteins.  In  1994,  it  was 
proposed that pairs of contacting amino acids  could be  iden-
tified  using  multiple  sequence  alignments  (MSAs)  of  related 

information  exists 

in  the 

EXPERT  OPINION ON DRUG DISCOVERY

7

Figure 2.  Comparison of a selection of published simulation- and deep learning-based approaches featured in recent CASP competitions. Inputs and preprocessing 
steps are shown in red, neural  network layers in black and gray, and simulation-based steps  and outputs, in blue. Additional inputs to the simulation phases have 
dashed outlines. (A) QUARK and (B) Rosetta AbRelax structure prediction methods. These simulation-based methods take an amino acid sequence and a selection of 
overlapping fragments of proteins of known structure (drawn from the Protein Data Bank in a preprocessing step) as input. They both build a coarse-grained model 
of  the  polypeptide  backbone  and  explore  its  conformations  using  fragment  insertion  moves,  either  in  the  context  of  parallel  replica  exchange  Monte  Carlo 
simulations (QUARK) or  a single simulated annealing  trajectory  (Rosetta). QUARK  clusters the  many models  produced to provide a  small number of  models for all- 
atom refinement using its ModRefiner algorithm. Rosetta AbRelax takes the single lowest-energy model from the simulated annealing search and converts it to an 
all-atom model, performing alternating rounds of discrete side-chain rotamer optimization (by simulated annealing) and continuous-space gradient-descent energy 
minimization  to  refine  the  all-atom  model.  Simulation  steps  (blue)  in  both  approaches  are  typically  run  many  times  with  the  same  inputs  (red)  to  generate  many 
candidate  models.  The  lowest-energy  models  represent  the  most  likely  structures.  (C)  The  RaptorX-Contact  algorithm,  the  top  neural  network-based  contact  map 
generator  in  the  2016  CASP12  competition.  This  method  uses  a  1D  sequence  profile  and  2D  residue  pair  correlation  matrix  computed  from  a  multiple  sequence 
alignment  (MSA)  as  input.  The  sequence  profile  is  processed  by  a  1D  convolutional  residual  neural  network  (ResNet),  producing  a  2D  features  matrix  that  is 
concatenated  with  the  2D  correlations  and  processed  by  a  very  deep  2D  convolutional  ResNet.  The  output  is  a  contact  map  used  for  constrained  structure 
minimization  with  the  Crystallography  &  NMR  System  (CNS)  software  [62].  (D)  The  version  1  AlphaFold  algorithm,  the  top  performer  in  the  2018  CASP13 
competition’s regular targets category. AlphaFold also takes as inputs a precomputed set of features extracted from an MSA, but feeds this directly into an extremely 
deep 2D convolutional ResNet, with convolutional layers recognizing 128 to 256 features with varying levels of dilation. Output probability distributions of pairwise 
distances and backbone torsion angles are combined with the Rosetta energy function in Rosetta gradient-descent minimization to generate final structures. (E) The 
trRosetta  method,  featured  in  the  2020  CASP14  competition.  trRosetta  is  able  to  extract  more  information  from  unformatted  MSA  inputs,  requiring  less 
preprocessing.  In  addition  to  pairwise  distance  probability  distributions,  it  also  produces  predictions  of  torsion  and  angle  probability  distributions  between  pairs 
of residues, constraining their relative orientation in space. As in panel D, Rosetta-based gradient descent minimization using the Rosetta energy function is used to 
convert  trRosetta  distance  and  orientation  constraints  to  3D  structures.  Figure  drawn  with  reference  to  descriptions  in  [5–7,51,52,90].

proteins since mutations at a given position are often accom-
panied by compensatory mutations at contacting positions to 
preserve protein structure or function [76]. Contact maps (two- 

dimensional  plots  of  contact  probability  between  pairs  of 
amino  acids  as  functions  of  sequence  position)  derived  from 
mutational  covariation  analysis  were  used  to  produce  low- 

8

V. K. MULLIGAN

in 

improvements 

resolution  predictions  of  protein  structure  as  early  as  1997 
[77]. Around 2010, several groups developed improved meth-
ods  for  extracting  contact  map  predictions  from  MSAs,  using 
direct  covariation  analysis  or  enhanced  methods  aiming  to 
prune  spurious  covariation  from 
indirect  effects  [78–81]. 
These  methods  were  used  in  2014’s  CASP11  competition  to 
bolster  Rosetta  and  other  simulation-based  approaches  by 
appending  contact-map  derived  pair  distance  potentials  to 
the  energy  function  used  for  prediction  as  had  been  done 
previously  for  NMR-based  constraints  [82].  By  2016’s  CASP12 
competition,  the  use  of  MSA-derived  contact  maps  produced 
with the program GREMLIN in conjunction with Rosetta Monte 
Carlo  conformational  searches  and  short  MD  trajectories  for 
refinement  led  to  top  performance  in  the  regular  targets 
category  [83].  In  the  same  year,  RaptorX-Contact  (shown  in 
Figure  2(C)),  which  used  a  deep  neural  network-based 
approach for converting MSAs to contact predictions stripped 
of spurious contacts, provided the most accurate contact map 
predictions with fewer sequences in the MSA (though this did 
not  translate  into  greater  accuracy  in  the  final  structural  pre-
dictions at the time) [5]. Other groups were also able to show 
improvements  to  their  methods  by  incorporating  deep  learn-
ing.  For  example,  He  et  al.  introduced  NeBcon,  which  used 
several  neural  networks  and  direct  covariation  analysis 
approaches,  combining  these  using  Bayesian  classification,  to 
generate  contact  maps  from  sequence  [84].  Used  in  conjunc-
tion  with  the  existing  I-TASSER  and  QUARK  methods,  consid-
erable 
fold  prediction  accuracy  were 
observed [85]. Following CASP12, deep-learning-based predic-
tors  of  secondary  structure  and  β-turns  (MUFOLD-SS  and 
MUFOLD-BetaTurn)  were  also  shown  to  improve  structure 
prediction accuracy with the existing MUFOLD method [86,87].
Although  binary  predictions  of  whether  pairs  of  residues 
make contact are useful, 2018’s CASP13 competition revealed 
that  distance  predictions  can  also  be  gleaned  from  coevolu-
tionary data. This allowed the AlphaFold algorithm to outper-
form  all  other  methods.  AlphaFold’s  approach,  shown  in 
Figure  2(D),  took  as  input  the  sequence  of  a  protein  of 
unknown  structure  and  an  MSA  of  related  sequences  from 
the  Uniclust30  database  [88].  The  MSA  was  subjected  to  pre-
processing  using  PSI-BLAST  [89]  to  convert  it  to  a  position- 
specific scoring matrix (PSSM) and to extract an additional set 
of  features.  These  were  then  fed  into  a  deep  neural  network 
trained  on  backbone  torsion  angles  and  inter-residue  dis-
tances  from  a  curated  set  of  29,427  PDB  structures.  Outputs 
from  the  neural  network  –  binned  distograms  representing 
probability  as  a  function  of  separation  distance  for  pairs  of 
residues,  as  well  as  similar  predictions  for  backbone  torsion 
angles – were then used to constrain Rosetta structure predic-
tions, augmenting the Rosetta energy function with potentials 
derived from neural network predictions. Initial attempts used 
fragment-based  searches  of  conformation  space,  but  it  was 
found that slightly more accurate results could be obtained at 
lower  computational  cost  by  simply  using  gradient-descent 
minimization  from  random  starting  conformations  sampled 
from  the  distribution  of  predicted  backbone  torsions  [6,90]. 
Although  AlphaFold  was  the  top-performing  CASP13  method 
in  the  regular  targets  category,  the  runner-up,  RaptorX,  also 
showed  considerable  performance 
improvements  over 

previous  years  by  switching  to  deep  neural  network-based 
inter-residue  distance  predictions  instead  of  contact  predic-
tions 
[91].  Attempts  to  enhance  the  MULTICOM  meta- 
prediction  method  with  deep  neural  network-predicted  con-
tact  maps  revealed  that  here  too,  direct  minimization  using 
constraints  derived  from  the  contact  map  was  more  effective 
than  were  fragment-based  predictions  guided  by  contact 
maps,  leading  to  third-place  performance  [92].  Following  the 
competition,  Heo  et  al.  showed  that  MD  refinement  of 
AlphaFold  models  improved  model  quality  on  average,  in 
some  cases  to  the  point  at  which  predicted  models  could  be 
used for crystallographic phasing [93]. This demonstrates how 
simulation continues to be an important complement to deep 
learning in  structure prediction.

In  2019,  Yang  et  al.  improved  on  the  results  achieved  by 
AlphaFold  by  devising  a  simpler  and  more  elegant  deep 
(for  transform-restricted 
neural  network,  called  trRosetta 
Rosetta),  able  to  take  MSAs  directly  as  the  sole  input  and  to 
produce  binned  probability  distograms,  as  well  as  probability 
distributions of relative inter-reside orientations, directly [7]. As 
before,  this  was  used  to  constrain  Rosetta-based  gradient- 
descent  minimization  to produce  final predicted structures  as 
shown in Figure 2(E). Where AlphaFold relied on considerable 
preprocessing  to  extract  up  to  37  different  types  of  features 
likely  to  be  relevant  to  structure  from  the  MSAs,  by  training 
trRosetta  using  the  one-hot  encoding  of  the  MSA  as  the  sole 
input,  the  network  itself  was  able  to  identify  the  salient  fea-
tures in the data, reducing reliance on human preconceptions 
of  relevant  information.  Additionally,  where  the  AlphaFold 
neural  network  was  limited  to  64-residue  sequences,  necessi-
tating  a  sliding-window  approach  to  generate  distograms  for 
longer  sequences,  trRosetta’s  architecture  was  sequence 
length-independent.  Tested  on  CASP13  targets,  trRosetta  was 
able  to  outperform  AlphaFold  [7].  The  trRosetta  method  was 
featured 
it 
achieved second-best performance in the regular targets cate-
gory.  The  top  performer,  AlphaFold  version  2,  used  an 
approach that is as yet unpublished.

in  the  CASP14  competition 

in  2020,  where 

In  parallel  to  efforts  to  use  deep  learning  to  predict  well- 
defined  protein  structures,  the  prediction  of  intrinsic  disorder 
has been a focus of research since at least the late 1990s. Since 
there  exist  curated  databases  of  sequences  that  are  experi-
mentally  evidenced  to  be  disordered,  such  as  the  DisProt 
database  [94],  it  is  possible  to  train  deep  neural  networks  or 
other  ML  models  to  predict  degree  of  disorder  from  amino 
acid sequence,  and  many  dozens  of  such methods  now exist. 
Some  notable  recent  examples  include  the  RaptorX-Property 
method,  which  uses  deep  convolutional  neural  networks  to 
predict  properties 
from 
sequences  or  MSAs  [95],  the  SPOT-Disorder2  method,  which 
employs long short-term memory language models (LSTMs) to 
recognize  disordered  regions  from  sequences  or  MSAs  [96], 
and the SPOT-MoRF method, which uses the pre-trained inter-
nal  representations  of  the  SPOT-Disorder2  neural  network  (a 
type  of  machine  learning  known  as  transfer  learning)  but  is 
trained  to  recognize  the  subset  of  disordered  regions  that 
order  themselves  on  binding  to  another  protein  (molecular 
recognition features or MoRFs) [97]. The DEPICTER meta-server 
combines 10 of the available methods to produce a consensus 

intrinsic  disorder) 

(including 

prediction  [98].  For  a  more  exhaustive  review,  see  [99]  and 
[100]. These methods can aid drug discovery both by helping 
to avoid difficult-to-target disordered regions of some proteins 
and by identifying functional regions of disorder that perhaps 
could  be  modulated  by  next-generation  peptide  or  protein 
therapeutics.

The  progression  of  deep  learning-based  structure  prediction 
methods over the past 5 years illustrated in Figure 2(C) through 
E shows a trend from complexity to simplicity, both in the human- 
chosen architecture of the neural network itself and in the amount 
of  preprocessing  needed  for  the  inputs.  At  the  same  time,  the 
neural network component is being relied upon to extract more 
and  more  information  from  the  inputs,  both  identifying  salient 
features  of  the  input  data  and  producing  more  information-rich 
outputs constraining the structure. Simulation still plays an essen-
tial  role  in  generating  the  final  structure,  however.  One  notable 
exception  is  a  recently  published  neural  network  that  takes  an 
amino acid sequence as input and produces atomic coordinates as 
output  [101].  Because  no  simulation  is  required  to  convert  the 
output to a structure, the end-to-end differentiability of the net-
work  creates  opportunities  for  solving  the  inverse  problem  – 
designing sequences given a desired structure – directly by gra-
dient-descent  optimization.  Whether  this  will  prove  generally 
advantageous, in terms of versatility, accuracy, or computational 
cost, over hybrid methods combining neural network predictions 
and simulations remains to be seen. At present, although this end- 
to-end model produces extremely rapid predictions in the forward 
direction (seconds on a GPU), the predicted structures are some-
what less accurate than the state of the art from hybrid methods. 
At  the  time  of  this  writing,  trRosetta-based  constraints  and 
Rosetta-based energy minimization trajectories represent the pub-
licly available state of the art in protein structure prediction, offer-
ing the highest accuracy. (The trRosetta neural network is released 
freely under the MIT license, and Rosetta is made freely available 
for  not-for-profit  users,  and 
licensed  for  for-profit  use). 
Nevertheless, trRosetta’s performance was overshadowed by the 
enormous performance improvements exhibited by version 2 of 
AlphaFold. AlphaFold2 has not yet been published, and it is not 
clear whether it will be publicly accessible. Although there is great 
excitement about its success in CASP14, very little technical infor-
mation about its workings has been revealed. Until independent 
scientists can directly evaluate the AlphaFold2 method, the extent 
to which this will  impact structural biology  for drug discovery  is 
unknown. It is worth noting that protein folding prediction algo-
rithms are used not only to predict structures of natural proteins 
but  to  validate  computational  designs  and  to  select  designs  for 
expression and experimental characterization. Although it is well 
accepted  that  simulation-based  folding  algorithms  that  can 
explore  many  conformations  are  useful  for  identifying  and  dis-
carding designs that lack a unique low-energy folded state [102], it 
has not yet been similarly established that ML methods that are 
trained on sequences that fold are similarly useful for identifying 
sequences that do not fold.

is 

3.2. Analysis  and  annotation  of  macromolecular 
structures

Although  media 
reports  of  AlphaFold2  have  created 
a  perception  that  machine  learning  has  solved  the  problem 

EXPERT  OPINION ON DRUG DISCOVERY

9

the  prediction  of 

of  modeling  macromolecules, 
three- 
dimensional  structure  from  amino  acid  sequence  is  but  one 
problem  in  the  modeling  field.  It  represents  a  starting  point 
for  subsequent  analysis  to  extract  biologically  relevant  infor-
mation,  and  to  develop  strategies  for  altering  biological  sys-
tems  (for  example,  by  creating  drugs  targeting  disease- 
relevant  proteins).  Given  a  macromolecular  structure,  one 
may  wish  to  ask:  what  is  the  macromolecule’s  function  and 
mechanism  of  action?  To  what  other  biomolecules  does  it 
bind,  and  where  are  the  binding  sites?  Is  this  macromolecule 
an  enzyme,  and  if  so,  where  is  the  active  site?  Where  are  the 
regulatory  sites or  sites of posttranslational modification?  The 
gold  standard  for  answering  such  questions  is  laborious  low- 
throughput  experiments.  An  experienced  structural  biologist 
can  generate  useful  hypotheses  related  to  structure-function 
relationships based on examination of a structure,  and devise 
experiments  to  test  those  hypotheses.  Manual  inspection  of 
macromolecular  structures 
is  a  time-consuming  process, 
though.  As  databases  of  known  macromolecular  structures 
grow,  and  as  structure  prediction  methods  (enhanced  now 
with  deep  learning)  improve,  there  is  mounting  need  for 
automated  methods  that  can  rapidly  annotate  large  numbers 
of  macromolecular  structures  with  hypothetical  functions  or 
properties  to  allow  experimentalists  to  prioritize  those  low- 
throughput  experiments  that  are  likely  to  yield  the  greatest 
biological insight.

ML methods for annotating 1D amino acid sequences have 
been  used  for  many  years  but  are  limited  in  their  power 
(reviewed  in  [103]).  The  automated  analysis  of  3D  macromo-
lecular structure, particularly by deep learning, is a newer area 
of  study.  A  major  challenge  is  representing  the  structure  in 
a manner suitable for input into a neural network. One idea is 
to take inspiration from the 2D convolutional neural networks 
that have revolutionized image processing (reviewed in [104]), 
and  to  apply  a  3D  equivalent  to  macromolecular  structures. 
The  DeeplyTough  algorithm,  used  to  compare  binding  pock-
ets in macromolecules and to identify sites where similar small 
molecules  may  bind  different  macromolecules,  is  an  example 
of  this  approach  (Figure  3(A)).  Inputs  for  DeeplyTough  are 
prepared  by  laying  a  3D  voxel  grid  over  a  binding  pocket  of 
interest to construct a 4D tensor with three spatial dimensions 
and  a  fourth  dimension  representing  the  properties  of  the 
atoms  in  each  voxel.  The  network  applies  a  series  of  3D 
convolutional  layers  to  produce  an  abstract  1D  output  vector 
serving as input into downstream comparison operations. Two 
pockets  may  be  compared  by  computing  the  Euclidean  dis-
tance  between  the  output  vectors  [105].  Having  been  trained 
to  recognize  patterns  in  the  training  set  and  to  produce  an 
abstract but useful representation of the inputs rather than to 
produce outputs known a priori, DeeplyTough’s training repre-
sents an example of unsupervised learning. Although powerful, 
this  approach  has  limitations  which  demonstrate  the  impor-
tance  of  considering  symmetry  invariances  when  choosing 
a  network  architecture.  A  computational  method  exhibits 
a  symmetry  invariance  if  a  particular  transformation  of  its 
inputs  results  in  the  same  output.  2D  convolutions  and  pool-
ings are useful for processing images since together they have 
the  property  of  being  invariant  to  translational  transforma-
tions:  sliding  an  image  horizontally  or  vertically  has  no  effect 

10

V. K. MULLIGAN

on  the  pooled  output  of  the  network,  which  allows  a  feature 
of interest (e.g. a number, a face, a road sign) to be recognized 
whether it is centered or in a corner of an image. 3D convolu-
tions with pooling are similarly translationally invariant in any 
direction,  allowing  recognition  of  binding  pockets  indepen-
dent  of  the  coordinate  origin.  However,  neither  2D  nor  3D 
pooled convolutional networks are rotationally invariant [104], 
which  is  a  problem  since  macromolecules  may  be  presented 
with  any  orientation.  To  address  this,  DeeplyTough’s  training 
relied on using many copies of each pocket in the training set 
voxelized 
in  different  orientations,  with  additional  terms 
added  to  the  objective  function  that  was  minimized  during 
training  to  ensure  that  the  same  pocket  in  different  orienta-
tions  produced  minimally  different  output  vectors  [105].  This 
increased the complexity of training and potentially consumed 
some capacity of the model in learning to recognize different 
rotations of the same object.

Two  recent  works  have  used  the  alternative  approach  of 
relying  on  geometric  convolutions  that  are  rotationally  invar-
iant  on  pooling.  The  dMaSIF  architecture 
(Figure  3(B)), 
described in a preprint, is like DeeplyTough in that it produces 
a representation of a macromolecular surface useful for classi-
fication  or  comparison  tasks  –  for  example,  the  identification 

of binding sites or the detection of complementarity between 
two  surfaces  [106].  Rather  than  voxelizing  a  macromolecular 
structure,  however,  dMaSIF  takes  Cartesian  coordinates  of 
atoms and one-hot encodings of atom types directly as inputs, 
and  uses  fast  internal  tensor  operations  to  convert  these  to 
a point surface representation, which is then fed into a quasi- 
geodesic  convolutional  network  in  which  convolution  opera-
tions allow information to flow amongst surface points nearby 
in  space.  Outputs  are  a  vector  that  can  either  be  interpreted 
directly  (if  the  network  is  subjected  to  supervised  training  to 
predict properties of surfaces) or through subsequent compar-
ison operations (if the network is allowed to learn an abstract 
representation  in  an  unsupervised  manner  for  tasks  such  as 
surface  matching).  The  architecture  appears  to  be  versatile 
and  applicable  to  surface  analysis  tasks  beyond  the  demon-
strations  in  the  preprint.  Since  the  network  is  end-to-end 
differentiable  and  operates  on  Cartesian  atom  coordinates, 
there is future potential  for using it in conjunction with  more 
traditional  simulation-driven  software  like  Rosetta,  which  can 
perform gradient-descent minimization to optimize a structure 
given  derivatives  of  an  objective  function  with  respect  to 
atomic coordinates  [49,107].

Figure  3.  Architecture  of  recently  described  deep  learning  approaches  for  analyzing  protein  structures.  Colors  are  as  in  Figure  2.  An  asterisk  next  to  the  year 
indicates  a  preprint.  (A)  The  DeeplyTough  network,  which  uses  a  voxelized  representation  of  a  protein  binding  pocket  as  input  and  generates  an  abstract 
representation vector that can be compared to other pocket representation vectors by computing the Euclidean distance as a similarity metric. The network consists 
of  a  series  of  3D  convolutional  layers  with  varying  filter  size  and  window  stride.  (B)  The  dMaSIF  neural  network,  which  takes  atomic  coordinates  and  one-hot 
encoded  atom  types  directly  as  inputs,  rapidly  transforms  these  in-network  to  produce  a  list  of  surface  points  and  normals,  and  produces  an  output  vector. 
Depending  on  training,  the  output  vector  can  have  concrete  meaning  that  could  be  interpreted  directly  (e.g.  for  classification  tasks)  or  can  be  an  abstract 
representation useful as input into subsequent analysis steps (e.g. comparison operations). The network hidden layers use quasi-geodesic convolutions which allow 
information to flow amongst surface points nearby in space. (C) The DeepFRI network, which takes one-hot sequence encodings and amino acid proximity graphs as 
inputs and computes a one-hot encoding of the gene ontology (GO) terms describing the protein’s function as output. Hidden layers use a long short-term memory 
language model (LSTM) to extract amino acid features from sequence, as well as graph convolutions, which allow node information to flow along edges (i.e. amino 
acid  information  to  flow  to  neighboring  amino  acids  in  space).  Figure  drawn  with  reference  to  descriptions  in  [105,106,108].

The second work, a paper by Gligorijević et al., presents the 
DeepFRI architecture  (Figure  3(C)),  which aims  to  predict  pro-
tein function, represented as an output vector encoding gene 
ontology  (GO)  terms,  from  protein  structure  [108].  DeepFRI 
uses  both  the  amino  acid  sequence  and  contact  map  of 
a  structure  as  input.  A  one-hot  encoding  of  the  sequence  is 
passed through an LSTM to extract residue-level features. The 
contact  map  is  converted  to  a  graph  representation  in  which 
residues  are  nodes  and  inter-residue  contacts  are  edges,  and 
this  is  provided  along  with  the  residue-level  features  to 
a  graph  convolutional  neural  network  (GCNN).  The  GCNN 
allows  information  to  flow  from  node  to  node  along  graph 
edges, permitting recognition of features composed of amino 
acids  distant  in  sequence  but  close  in  space.  By  including 
structural  information,  the  authors  were  able  to  improve  per-
formance  over  sequence-based  methods  alone.  Moreover,  by 
backpropagation  through  the  network,  it  was  possible  to 
identify  each  residue’s  contribution  to  a  particular  functional 
annotation – information which, when mapped to the original 
structure,  provides  predictions  of  functional  sites  that  can 
inform  the  design  of  therapeutics  intended  to  block  or  alter 
function [108].

As  shown  in  Figure  3,  these  early  approaches  to  encoding 
structural  information  for  deep  learning-based  analysis  are 
diverse,  but  commonly  rely  on  convolutional  operations  (3D, 
through-space geodesic, or through-edge graph convolutions) 
to  allow  extraction  of  information  from  spatial  geometry 
rather  than  from  sequence  alone.  As  time  progresses,  we  will 
hopefully see the emergence of standard representations per-
mitting  the  implementation  of  reusable  application  program-
ming  interfaces  (APIs)  for  simulation-based  software  that  can 
provide inputs into diverse analysis networks.

3.3. Macromolecular  design

The  ultimate  test  of  our  understanding  of  the  theory  of  pro-
tein  folding  has  been  the  creation  of  new  amino  acid 
sequences  able  to  fold  into  new  structures  and  to  perform 
new  functions. The  practical  application  is the rational design 
of  peptide  or  protein  therapeutics.  Typical  de  novo  design 
(where  no  starting  template  is  provided)  relies  on  Monte 
Carlo searches of backbone conformation space to find candi-
date  ‘designable’  conformations  compatible  with  some  func-
tion,  followed  by  Monte  Carlo  searches  of  sequence  space  to 
find  amino  acid  sequences  that  maximize  stability  in  the 
sampled  backbone  conformation  [109–113].  Both  phases  rely 
on  objective functions reflecting both  the desired  function  or 
properties  of  the  molecule  being  designed  and  the  known 
physics  of  protein  folding.  Although  these  simulation-based 
approaches  have  produced  many  experimentally  validated 
examples  of  designed  proteins  and  protein-like  molecules 
[114–119],  deep  learning-based  design  is  still  in  its  infancy. 
Since traditional design has high failure rates even with rigor-
ous computational validation, requiring the screening of large 
numbers  of  candidate  designs  to  find  one  success,  published 
descriptions  of  deep 
lacking 
experimental  validation  should  be  treated  with  due  skepti-
cism.  With  this  in  mind,  this  section  reviews  some  of  the 
recent developments in this  area.

learning-based  approaches 

EXPERT  OPINION ON DRUG  DISCOVERY

11

to 

from  PDB  structures  to  construct  a 

The Huang group has produced some of the most interest-
ing approaches to the first half of the de novo design problem, 
the  generation  of  plausibly  ‘designable’  protein  backbones 
conformations. They trained a generative network to produce 
pairwise  distance  matrices  for  new  protein  backbones  and  to 
convert  these  to  backbone  3D  coordinates.  They  also  trained 
another  network  to  discriminate  between  protein  conforma-
tions  produced  by  the  generator  and  conformations  from 
experimentally solved PDB structures. By pitting these against 
one  another  during  training  (a  strategy  known  as  generative 
adversarial  networks  or  GANs),  they  were  able  to  produce 
a  generative  network  that  creates  backbones  amenable  to 
Monte  Carlo-based  Rosetta  sequence  design,  and  to  show 
that  subsets  of  the  designed  sequences  were  predicted, 
using  simulation-based  Rosetta  methods,  to  fold  into  struc-
tures  close 
the  generated  backbone  conformations 
[120,121].  A  recent  preprint  from  this  group  also  describes 
another  approach:  training  variational  autoencoders  on  dis-
tance  maps 
low- 
dimensional  representation  of  the  plausible  immunoglobulin 
folds,  with  subsequent  layers  able  to  convert  this  representa-
tion  back  to  a  distance  map  and  then  to  an  all-atom  repre-
sentation.  By  sampling  new  points  in  this  space,  they  were 
rapidly  able  to  generate  new  immunoglobulin-like  backbone 
conformations,  for  which  sequences  were  designed  using 
Rosetta  Monte  Carlo  simulations  [122].  The  ultimate  proof  of 
their  methods  will  rely  on  experimental  validation  of  designs 
such  as  these,  but  as  a  concept,  this  is  very  promising,  with 
wide applicability to the development of protein therapeutics.
What  about  the  problem  of  designing  an  amino  acid 
sequence  given  a  candidate  backbone  conformation?  An 
obvious  question  in  the  design  field  is  whether  the  deep 
neural  networks  that  are  used  for  structure  prediction  could 
be  harnessed  for  design.  Design  and  prediction  are  nearly 
inverse  problems:  a  prediction  algorithm  takes  an  amino  acid 
sequence as input and produces a three-dimensional structure 
as  output, while  a  sequence  design algorithm  takes  a  desired 
protein  backbone  conformation  as  input  and  produces  an 
amino acid sequence intended to uniquely stabilize that struc-
ture  as  output.  Although  there  is  little  in  the  peer-reviewed 
literature  as  yet  about  trying  to  reverse  existing  structure 
prediction  neural  networks  in  order  to  design  sequences, 
two  recent  preprints  by  the  Ovchinnikov  and  Baker  groups 
report early  attempts  to do  just  this.  In  the  first, Anishchenko 
et  al.  provided  a  random  input  sequence  to  the  trRosetta 
neural  network  to  produce  nonsensical  residue  distance  and 
orientation predictions, then performed  a Monte Carlo  search 
of  sequence  space,  iteratively  re-running  the  network  with 
progressive  introduction  of  point  mutations  to  the  starting 
sequence  and  accepting  or  rejecting  moves  based  on 
improvements  to  the  plausibility  of  the  generated  pairwise 
distance  and  orientation  predictions  [123].  Plausibility  was 
assessed by KL divergence (an informational entropy measure) 
[124] from the distribution of distance and orientation predic-
tions produced by random sequences, which are not expected 
to  fold  into  any  meaningful  structures.  In  this  way,  they  were 
able to produce new sequences predicted by trRosetta to fold 
into  plausible  and  diverse  folds.  Importantly,  these  authors 
expressed  and  characterized  several  of  these  proteins  and 

12

V. K. MULLIGAN

found that nearly one in five (27 of 129) had circular dichroism 
spectra  consistent  with  the  predicted  structures  [123].  Full 
high-resolution  structural  characterization  has  not  yet  been 
described,  however.  Where  this  study  explored  the  diversity 
of  structures  that  can  be  produced,  the  second  preprint  by 
Norn  et  al.  examined  the  more  practical  challenge  of  stabiliz-
ing  a  desired  backbone  conformation  using  back-propagation 
through  the  trRosetta  network  and  gradient  descent  to  con-
vert a random input sequence into a sequence that produces 
inter-residue  distance  and  orientation  predictions  maximally 
matching the desired conformation. Based on Rosetta ab initio 
simulations,  the  authors  report  that  folding  free  energy  land-
scapes  were  better  optimized  to  uniquely  favor  the  designed 
conformation  using  this  approach  than  with  more  conven-
tional  Monte  Carlo  methods  that  seek  to  find  an  amino  acid 
sequence  minimizing  energy  in  the  designed  conformation 
alone.  This  suggests  that  trRosetta  is  able  to  consider  the 
whole  conformational  ensemble 
implicitly  during  design 
[125].  These  studies  suggest  that  promising  new  deep  learn-
ing-based  approaches  for  designing  functional  proteins,  per-
haps  with  higher  success  rates  than  have  been  achieved  by 
pure  simulation-based  approaches,  are  on  the  horizon. 
However,  it  is  important  to  note  that  this  is  an  emerging 
direction, and that neither study has yet produced experimen-
tal validation through high-resolution structural techniques, or 
undergone peer-review.

Improving  energy  function  calculation  speed  and 

3.4.
accuracy

Although  many  of  the  applications  of  deep  learning  described 
above  trend  toward  replacing  parts  of  prediction,  analysis,  or 
design pipelines that have traditionally been performed by simu-
lation, ongoing deep learning research also aims to improve the 
speed or accuracy of traditional simulations. Both MD software 
packages  that  simulate  the  physical  motions  of  atoms  and 
macromolecular  modeling  software  that  allow  large-scale  con-
formational sampling, docking, or design rely on having energy 
functions  that  are  both  fast  to  evaluate  and  accurate  for  large 
molecular systems.  Traditionally,  these  energy  functions,  which 
include the CHARMM and AMBER energy functions for molecular 
dynamics, the QUARK energy function, and the Rosetta ref2015 
energy  function,  have  been  constructed  rationally  as  sums  of 
terms, each representing a type of physical interaction between 
atoms  (e.g.  steric  repulsion  or  Van  der  Waals  interactions,  elec-
trostatic  interactions,  hydrogen  bonds,  etc.)  [46,47,52,126,127]. 
Each  term’s  parameter  values  have  been  tuned  using  some 
combination of direct entry of known physical constants, statis-
tical analysis of properties of known structures (such as torsion 
angle distributions), training on datasets of known protein struc-
tures  (to  reproduce  observables  like  rotamer  distributions  or 
local  energy  minima),  or  training  large-scale  simulations  (to 
reproduce  bulk  material  properties  that  have  been  measured 
experimentally,  such  as  freezing  and  boiling  points,  viscosity, 
density, etc.).  To  aid the speed  of calculation, energy  terms  are 
often  decomposable  into  sums  of  pairwise  interactions,  either 
between  pairs  or  amino  acid  residues,  individual  chemical 
groups,  or  individual  atoms.  Such  energy  functions  necessarily 
sacrifice  some  accuracy  in  favor  of  speed.  In  some  cases,  the 

inclusion  of  non-pairwise  or  multi-body  terms,  constructed 
rationally  or  trained  on  known  structures,  can  aid  predictive 
accuracy  [128,129]  or  design  [112,119]  at  the  cost  of  some 
speed. Nevertheless, calculations invariably lack the high  preci-
sion of quantum mechanical (QM) energy calculations based on 
finding solutions to the Schrödinger equation (which can only be 
done  approximately,  at  enormous  computational  cost,  and  for 
very  small  molecular  systems).  In  the  case  of  implicit-solvent 
macromolecular modeling methods, the calculation of the influ-
ence of solvation effects is also imprecise. Given that the hydro-
phobic effect is a primary factor driving proteins to fold [130], this 
can limit the accuracy of predictions – yet precisely capturing this 
entropic effect would require long and costly MD simulations in 
which solvent molecules were modeled explicitly, and the com-
putational  expense  would  prevent  large-scale  macromolecular 
conformational sampling trajectories.

Since  deep  neural  networks  are  powerful  universal  function 
approximators, and since the evaluation of trained networks can 
be efficiently parallelized, deep learning offers a means of achiev-
ing  the  precision  and  accuracy  of  high-cost  computations  (QM 
calculations, long explicit-solvent MD trajectories) at much lower 
computational cost. In 2019, Schütt et al. introduced a deep neural 
network,  SchNOrb,  able  to  predict  the  electronic  wavefunction 
and  QM  ground  state  energy  from  molecular  geometry,  trained 
against  high-cost  QM  calculations  performed  on  various  small 
molecules  [131].  In  2020,  Hermann  et  al.  showed  that  their 
PauliNet,  a  deep  neural  net  trained  in  an  unsupervised  manner 
to  find  approximate  solutions  to  the  Schroödinger  equation  for 
arbitrary electronic systems, could scale with polynomial complex-
ity  to  allow  accurate  energy  calculations  for  larger  systems  than 
could  be  handled  by  more  traditional  QM  methods,  which  typi-
cally scale exponentially [132]. (A preprint by Pfau et al. describes 
a  similar  approach  [133],  albeit  one  not  yet  validated  by  peer 
review).  None  of  these  approaches  has  yet  been  demonstrated 
to work with a molecule with more than a few dozen electrons, 
however.  Where  fast  approximations  of  QM  energy  calculations 
could improve the accuracy of enthalpic terms in macromolecular 
energy  functions,  better  approximations  of  solvation  energies 
could  improve  the  accuracy  of  the  primary  entropic  term,  the 
solvent  term  in  implicit  solvent-based  methods.  Very  recently, 
several  groups  have  published  deep  learning-based  approaches 
to  predict  solvation-free  energies  of  small  molecules  [134–136]. 
Some  of  these  combine the  analysis  of  small-molecule structure 
with information from short MD simulations of the small molecule 
in explicit solvent to estimate solvation-free energies, while others 
are  able  to  achieve  impressive  predictive  power  by  structural 
analysis  alone  with  GCNNs  operating  on  graph  representations 
of  molecular  structures  [136].  These  methods  may  be  useful  to 
precompute  solvation  penalties  or  bonuses  for  macromolecular 
building-blocks  to  facilitate  rapid  energy  calculations  of  greater 
accuracy.  It  remains  to  be  seen  whether  deep  learning-based 
approximations  of  QM  energies  or  solvation  energies  will  be 
able to scale to operate on macromolecular  structures for direct 
use in structure prediction or design simulations.

4. Conclusions
Although  traditional  drug  discovery  has  relied  primarily  on 
large  screens  of  small-molecule  compounds,  such  screens 

inevitably  cover  a  small  fraction  of  chemical  space,  are  limited 
in  their  ability  to  select  compounds  that  achieve  potency  via 
a  desired  mechanism  of  action,  and  produce  many  false  posi-
tives and false negatives. Computational macromolecular mod-
eling  tools  therefore  play  a  growing  role  in  the  drug  discovery 
pipeline.  Structure-based  tools  are  used  for  target  analysis,  to 
identify  functional  sites  that  can  be  targeted  with  drugs,  for  in 
silico small-molecule screening, allowing drug candidate lists to 
be  shortened  at  lower  computational  cost  and  permitting 
resource-heavy  experiments  to  be  reserved  for  the  best  candi-
dates,  and  for  rational  design  of  small  molecule,  peptide,  and 
protein  therapeutics  with  potentially  higher  efficacy  and  speci-
ficity  than  can  be  achieved  through  undirected  library  screens. 
In many cases, structure prediction tools can generate accurate 
predictions  about  active  site  or  protein–protein  interaction 
regions,  and  in  some  cases,  when  close  homology  models  or 
partial  experimental  information  is  available,  predicted  models 
can  be  used  for  in  silico  drug  screening  or  design.  The  finite 
accuracy  of  predictive  methods  limits  the  situations  in  which 
they can be applied, however.

Traditionally,  many  of  the  macromolecular  modeling  tools 
available were simulation based. The application of deep learning 
to  macromolecular  modeling  is  a  young  and  rapidly  changing 
field,  yet  one  that  promises  to  increase  the  accuracy  and  speed 
of  predictions  by  harnessing  the  large  amount  of  information 
hidden in sequence and structure databases. These enhancements 
would  permit  broader  application  of  macromolecular  modeling 
tools  to  drug  discovery,  allowing  more  efficient  development  of 
new  molecular  entities  with  less  investment  of  resources  into 
laboratory screens. Challenges remain in representing something 
as complicated as a macromolecular structure in a manner suitable 
for input into or output from a deep neural network, but already, 
some patterns are emerging that may lead to common represen-
tations.  In  particular,  representations  and  network  architectures 
that have symmetry invariances suited to macromolecular struc-
tures, such as translational, rotational, and atom/residue indexing 
invariances, are emerging. As design approaches mature, symme-
try considerations will need to be applied to both input and output 
representations to allow, for example, networks that produce out-
put structures from input sequences to be reversed so that they are 
able to output sequences given a desired input structure. Despite 
the  challenges,  the  examples  reviewed  here  of  emerging  deep 
learning  methods  for  predicting  macromolecular  structure, 
extracting  functional  information  from  structure,  or  designing 
new macromolecules show enormous potential for transformative 
change,  allowing  the  patterns  that  were  previously  hidden  in 
available  sequence  or  structural  data  to  be  leveraged  as  never 
before.  In  the  coming  years,  this  will  hopefully  translate  into 
powerful  new  structural  tools  for  target  identification,  binding 
site  identification,  and  therapeutic  design.  The  ultimate  test  of 
these methods will  be whether they  can produce predictions  of 
sufficient accuracy to guide subsequent bioengineering work. It is 
plausible that in the next few years we will see the first example of 
a successful structure-guided drug development project informed 
not  by  an  experimentally  solved  protein  structure,  but  by 
a predicted one.

EXPERT  OPINION ON DRUG  DISCOVERY

13

5. Expert  opinion
The recent growth in deep learning methods has heralded the 
emergence of a  powerful new  tool  for macromolecular model-
ing.  As  with  any  tool,  it  is  important  to  recognize  its  proper 
relationship to existing tools, the new tool’s strengths and limita-
tions, and what is needed to transform today’s early prototypes 
into versatile, usable components of tomorrow’s drug discovery 
pipelines.

Many  of  the  current uses  of  deep  learning  rely  on  existing 
sequence or structural modeling tools for constructing tensors 
of  inputs  to  deep  neural  networks  or  for  interpreting  output 
tensors. Deep structure prediction networks, for example, typi-
cally  produce  structural  constraints  that  are  inputs  for  con-
strained  conformational  optimization  simulations,  using 
Monte  Carlo  or  gradient  descent  methods  that  combine  phy-
sics-based force fields with the neural network-generated con-
straints.  In  general,  deep  learning  is  best  considered  to  be 
a  complement  to,  rather  than  a  replacement  for,  simulation 
in  macromolecular  modeling.  It  has  some  severe  limitations. 
First, deep neural networks typically have tens of thousands to 
millions of parameters that must be optimized during training. 
This  creates  a  real  danger  of  overfitting:  training  on  limited 
training  data  can  produce  a  model  perfectly  tuned  to  repro-
duce  the  training  set,  but  which  poorly  generalizes  to  new 
inputs. This limits the application of deep learning to cases in 
which  very  large  numbers  of  training  examples  exist,  and 
requires  careful  testing  of  generalization  error  (using  valida-
tion  examples  not  used  in  training)  and  manual  tuning  of 
model  hyperparameters  controlling  the  training  to  minimize 
overfitting.  Unlike  mechanistic  models,  deep  neural  networks 
are also opaque and poorly human-interpretable, offering little 
insight  into  the  reasons  for  their  pathologies  when  they  do 
produce spurious outputs, and little ability to anticipate under 
what circumstances the model will produce erroneous output.
Extrapolation  to  cases  outside  the  training  realm  can  also  be 
more  difficult  with  deep  neural  networks  than  with  mechanistic 
models, particularly when the assumptions at the time of network 
construction  prevent  new  inputs  or  new  desired  outputs  from 
even being entered or retrieved. Figure 4 illustrates the limitations 
of current deep learning-based structure prediction methods with 
four  examples  of  macromolecules  that  can  be  modeled  using 
simulation-based methods (e.g. with MD or Monte Carlo methods), 
but  which  cannot  be  modeled  with  the  deep  learning-based 
structure  prediction  methods  reviewed  here.  The  input  tensors 
for  neural  networks  like  AlphaFold  and  trRosetta  provide  no 
means of indicating that a protein contains a cofactor like heme 
as  part  of  its  fold,  excluding  many  common  proteins  like  cyto-
chrome  c  (Figure  4(A))  or  myoglobin  (the  first  protein  whose 
structure  was  experimentally  solved)  [137,138].  Extremely  com-
mon  posttranslational  modifications,  such  as  phosphorylation, 
which  can  drive  major  conformational  changes  (as  in  cysteine 
string protein, Figure 4(B)) [139], are also not expressible in current 
network  inputs.  Since  structure  prediction  networks  are  trained 
only  on  examples  of  well-folded  proteins  and  on  homologous 
sequences, it is likely that they overestimate the degree of order 
of many sequences, making predicting the conformational ensem-
ble  of  disordered  polypeptides  such  as  Aβ42  (Figure  4(C)) 

14

V. K. MULLIGAN

challenging,  particularly  when  such  disorder  is  environment-  or 
context-dependent [140]. Although intrinsic disorder can be pre-
dicted from sequence with ML methods (vide supra), modeling the 
conformational ensemble of an intrinsically disordered protein or 
region relies on experiments and simulation at present. And since 
most  sequence  encoding  methods  rely  on  one-hot  encodings, 
sequences  of  synthetic  molecules 
like  the  computationally 
designed  S4-1  (Figure  4(D)),  built  from  mixtures  of  L-  and 
D-amino  acids  and  incorporating  structural  metal  ions  [113]  or 
synthetic cross-links [118], cannot be expressed in input tensors. 
Adapting existing neural networks for these cases would be diffi-
cult, and while entirely new neural networks may be constructed 
that allow for these features, the limited training data for molecules 
like these could hinder expansion into these realms. In contrast, if 
a mechanistic model is built on sufficiently general principles (such 
as unbiased physical simulation), extrapolation to new classes of 
molecules is more feasible.

These  problems  may  limit  the  realms  in  which  deep  learn-
ing  can  be  applied. Nevertheless,  the  early  demonstrations  of 
deep  learning  for  macromolecular  modeling  reviewed  here 
show  the  technology’s  enormous  potential.  To  maximize  the 
impact  for  real  biological  systems  of  interest  (particularly  for 
engineering new therapeutics) the next few years will need to 

see major effort invested not only in constructing and training 
new  deep  neural  networks  but  also  in  encapsulating  neural 
networks in versatile, reusable modules that can be combined 
with  existing  simulation-based  software  modules  to  develop 
new,  task-specific  modeling  protocols.  The  historical  develop-
ment  and  current  organization  of  powerful  simulation-based 
software  tools,  such  as  Rosetta,  can  serve  as  a  template  for 
those  developing  deep  learning-based  methods  [50].  Initially 
developed  as  a  FORTRAN  library  for  protein  structure  predic-
tion  with  a  linear  and  prescriptive  user  interface,  Rosetta  was 
subsequently  reimplemented  in  modern  object-oriented  C++ 
with a modular architecture allowing pieces of code for parti-
cular modeling tasks (Monte Carlo-based backbone conforma-
tional  searches,  Monte  Carlo-based  side-chain  optimization, 
gradient-descent energy minimization, etc.) to be recombined 
and  used  in  diverse  ways.  This  allowed  the  development  of 
diverse higher-level protocols for tasks such as de novo protein 
design  [109,110],  interface  design  [114,115],  enzyme  design 
[141,142],  nanomaterial  design  [116,117,143],  RNA  structure 
prediction  [144],  and  synthetic  peptide  design  and  structure 
prediction  [111–113,118,119].  Central  to  this  effort  has  been 
the  creation  of  an  object-oriented  framework  to  allow  devel-
opers  to  interact  with  familiar  concepts  (structures  or  ‘poses’, 

Figure  4.  Examples  of  molecules  that  could  be  modeled  with  general  simulation-based  approaches,  but  which  challenge  current  deep  learning-based  approaches 
with  specific  assumptions  built  into  their  architecture  or  training  sets.  (A)  X-ray  crystal  structure  of  cytochrome  c6  (PDB  ID  6TSY)  [137].  Many  proteins  incorporate 
cofactors like heme (gray) into their structures, but the presence of these cannot be expressed in current input tensors. (B) NMR structures showing phosphorylation- 
driven  conformational  switching  of  cysteine  string  protein  (PDB  IDs  2N04  and  2N05)  [139].  Common  structure-influencing  posttranslational  modifications  like 
phosphorylation  cannot  be  expressed  in  current  input  tensors.  (C)  NMR  structural  ensemble  for  the  Aβ42  polypeptide,  implicated  in  Alzheimer’s  disease  (PDB  ID 
1Z0Q)  [140].  Many  proteins  have  regions  of  inherent  flexibility  or  disorder,  yet  training  datasets  consisting  of  well-structured  proteins  poorly  reflect  this,  yielding 
deep  learning  models  likely  to  overestimate  the  order  of  arbitrary  sequences.  (D)  X-ray  crystal  structure  of  synthetic  mini-protein  S4-1  (PDB  ID  6UFA)  [113].  Input 
tensors typically encode sequences as one-hot encodings of the 20 natural L-amino acids (cyan residues), and cannot encode D-amino acids (orange residues), other 
non-canonical  amino  acids  (e.g.  2-aminoisobutyric  acid,  gray),  or  structural  metal  ions.  Additionally,  neural  networks  trained  exclusively  on  canonical  structures 
poorly  capture  non-canonical  structural  elements  like  left-handed  helices.  In  all  cases,  the  generality  of  physics-based  methods  such  as  MD  simulations  provides 
a  means  of  modeling  these  molecules.

amino acids or ‘residues’, dihedral angles identified with stan-
dard  biochemical  nomenclature,  etc.).  A  particular  challenge 
will  be  bridging  the  gap  between  the  functional  form  of  the 
inputs  and  outputs  to  neural  networks,  which  consist  of  ten-
sors with meaning (but no annotations) assigned to each entry 
in the tensor, and the object-oriented development paradigms 
that  minimize  developer  error  and  make  code  development 
more  accessible.  Full  integration  of  simulation-  and  deep 
learning-based  methods  will  ultimately  enable  exploration  of 
currently unexplored areas, such as breaking away from using 
pre-trained networks and learning from experience (reinforce-
ment learning) during  a simulation.

Finally, to maximize the impact of deep learning in macromo-
lecular modeling, methods must be made broadly available under 
permissive  licenses  to  allow  experimentation  with  incorporation 
into  existing  computational  pipelines,  and  must  be  subject  to 
rigorous  peer  review.  For  the  sake  of  safety,  the  drug  discovery 
field has learned to demand rigorous peer review to vet (poten-
tially dangerous) claims relevant to human health. Unfortunately, 
the same rigor is not routinely applied in the deep learning field: 
some of  the seminal  works of the deep learning  world are pub-
lished only as preprints, conference papers, or whitepapers, creat-
ing  the  risk  of  building  tomorrow’s  medicine  on  a  shaky 
foundation.  Widely  publicized  claims  about  the  performance  of 
deep learning methods need to be scrutinized and reproduced by 
independent scientists, with full peer review of any biologically or 
medically  relevant  claim  coming  out  of  such  methods,  prior  to 
broad dissemination, if the deep learning field is to play a larger 
role in drug discovery. There has also been considerable hetero-
geneity in the openness with which the developed methods are 
shared thus far. Some, like trRosetta, are released publicly under 
permissive licenses, with all scripts needed to prepare inputs and 
use outputs [145]. Others, like version 1 of AlphaFold, are released 
publicly for noncommercial use but with only sample inputs and 
without  the  tools  needed  to  prepare  new  inputs  [146].  Some 
publicly released tools, like DeeplyTough, have vague or nonstan-
dard  licenses  that  make  it  difficult  to  know  whether  researchers 
can incorporate the tool into their pipelines [147]. Many of these 
tools are also fragile and not subject to the same rigors of code 
maintenance as their simulation-based counterparts: both version 
1 of AlphaFold and trRosetta depend on version 1 of TensorFlow, 
for example, and are incompatible with version 2. As deep learning 
gains prominence in biomolecular modeling, the rigor of biological 
peer  review  must  be  brought  to  bear  to  scrutinize  not  only  the 
claims made themselves, but the reproducibility of the claims and 
availability and maintainability of the tools needed to reproduce 
the claims.

Abbreviations
Application  programming  interface,  API;  central  processing  unit,  CPU; 
Critical  Assessment  of  Protein  Structure  Prediction  Competition,  CASP; 
cryo-electron  microscopy,  cryo-EM;  generative  adversarial  network,  GAN; 
gene  ontology,  GO;  graph  convolutional  neural  network,  GCNN;  graphics 
processing  unit,  GPU;  long  short-term  memory  model,  LSTM;  machine 
learning,  ML;  molecular  dynamics  simulations,  MD;  multiple  sequence 
alignment,  MSA;  nuclear  magnetic  resonance  spectroscopy,  NMR;  Protein 
Data Bank, PDB; quantum mechanics, QM; recurrent neural network, RNN.

EXPERT  OPINION ON DRUG  DISCOVERY

15

Acknowledgments

The author thanks Vladimir Gligorijević, Sergey Ovchinnikov, Jack Maguire, 
and Richard Bonneau  for enlightening conversation.

Funding

V Mulligan  is funded by the Simons  Foundation.

Declaration  of  interest
V Mulligan is a co-founder and shareholder of Menten AI, a biotechnology 
company.  He  has  no  other  relevant  affiliations  or  financial  involvement 
with  any  organization  or  entity  with  a  financial  interest  in  or  financial 
conflict  with  the  subject  matter  or  materials  discussed  in  the  manuscript 
apart from those disclosed.

Reviewer  disclosures
Peer  reviewers  on  this  manuscript  have  no  relevant  financial  or  other 
relationships to disclose.

ORCID
Vikram Khipple Mulligan 

References

http://orcid.org/0000-0001-6038-8922

Papers of special note have been highlighted as either of interest (•) or of 
considerable interest (••) to readers.

1.  Setiawan  D,  Brender  J,  Zhang  Y.  Recent  advances  in  automated 
protein design and its future challenges. Expert Opin Drug Discov. 
2018;13(7):587–604.

2.  Mulligan VK. The emerging role of computational design in peptide 
macrocycle  drug  discovery.  Expert  Opin  Drug  Discov.  2020;15 
(7):833–852.

3.  Mitchell  TM. Machine  learning. New York: McGraw-Hill;  1997.
4.  Goodfellow  I,  Bengio  Y,  Courville  A.  Deep  learning.  Cambridge, 

Massachusetts: The MIT Press; 2016. 

••  This textbook is an excellent resource for biologists looking for 
a comprehensive guide to deep learning. Although it contains 
no shortage of mathematical details, it is written in a clear and 
accessible  way, with  plenty  of  concrete  examples.

5.  Wang  S,  Sun  S,  Xu  J.  Analysis  of  deep  learning  methods  for  blind 
protein contact prediction in CASP12. Proteins: Structure, Function, 
and  Bioinformatics. 2018;86:67–77. 

•  This  paper  introduced  one  of  the  first  methods  for  improving 
protein  structure  prediction  using  deep  learning,  and  antici-
pated  an  approach  that  would  prove  to  be  one  of  the  most 
successful  in  the  field.

6.  Senior  AW,  Evans  R,  Jumper  J,  et  al.  Protein  structure  prediction 
using  multiple  deep  neural  networks 
in  the  13th  Critical 
Assessment  of  Protein  Structure  Prediction  (CASP13).  Proteins 
Struct  Funct Bioinforma.  2019;87(12): 1141–1148. 

•  The  performance  of  the  version  1  AlphaFold  method  is 

described  here.

7.  Yang  J,  Anishchenko  I,  Park  H,  et  al.  Improved  protein  structure 
prediction using predicted interresidue orientations. Proc Natl Acad 
Sci.  2020;117(3): 1496–1503. 

••  This paper details the trRosetta method. At the time of writing 
(pending  the  publication  of  AlphaFold2)  this  represents  the 
top  performing  published  method  for  protein  structure 
prediction.

8.  Cybenko  G.  Approximation  by  superpositions  of  a  sigmoidal 

function. Math Control Signals Syst.  1989;2(4):303–314.

16

V. K. MULLIGAN

9. Hornik  K,  Stinchcombe  M,  White  H.  Multilayer  feedforward  net-
works  are  universal  approximators.  Neural  Netw.  1989;2 
(5):359–366.

10.  He  K,  Zhang  X,  Ren  S,  et  al.  Deep  residual  learning  for  image 
recognition.  Proc 
IEEE  Conf  Comput  Vis  Pattern  Recognit 
[Internet].  Las  Vegas,  Nevada,  USA.  2016  [cited  2021  Jan  29].  p. 
770–778.  Available  from:  https://openaccess.thecvf.com/content_ 
cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper. 
html

11.  Bjorck  J,  Gomes  C,  Selman  B,  et  al.  Understanding  batch 
normalization.  ArXiv180602375  Cs  Stat  [Internet].  2018  [cited 
2021 Jan 30];  Available from:  http://arxiv.org/abs/1806.02375

12.  Fukushima  K,  Miyake  S.  Neocognitron:  a  self-organizing  neural 
network  model  for  a  mechanism  of  visual  pattern  recognition.  in: 
amari  s,  Arbib  MA,  editors.  Compet  coop  neural  nets.  Berlin, 
Heidelberg: Springer; 1982. p. 267–285. 

•  Convolutional  neural  networks,  now  used  widely  in  image 

processing,  were  introduced  in  this  seminal  paper.

13.  Rumelhart DE, Hinton GE, Williams RJ. Learning representations by 

back-propagating  errors. Nature. 1986;323(6088):533–536. 

••  Both  the  back-propagation  method  and  the  recurrent  neural 

network  were  introduced  in  this  seminal  paper.

14.  Tolosana  R,  Vera-Rodriguez  R,  Fierrez  J,  et  al.  DeepFakes  and 
beyond:  a  survey  of  face  manipulation  and  fake  detection. 
ArXiv200100179  Cs  [Internet].  2020  [cited  2021  Jan  30];  Available 
from: http://arxiv.org/abs/2001.00179

15.  Ackley  DH,  Hinton  GE,  Sejnowski  TJ.  A  learning  algorithm  for 

Boltzmann machines. Cogn Sci.  1985;9(1):147–169.

16.  Rumelhart  DE,  McClelland  JL  Learning  internal  representations  by 
error  propagation.  Parallel Distrib  Process  Explor  Microstruct  Cogn 
Found  [Internet].  MIT  Press;  1987  [cited  2021  Jan  29].  p.  318–362. 
Available from:  https://ieeexplore.ieee.org/document/6302929

17. Goodfellow  IJ,  Pouget-Abadie  J,  Mirza  M,  et  al.  Generative  adver-
sarial networks. ArXiv14062661 Cs Stat [Internet]. 2014 [cited 2021 
Jan 30];  Available from: http://arxiv.org/abs/1406.2661

18. Kipf  TN,  Welling  M.  Semi-supervised  classification  with  graph  con-
volutional networks. ArXiv160902907 Cs Stat [Internet]. 2017 [cited 
2021 Jan 30];  Available from:  http://arxiv.org/abs/1609.02907

19.  Simonovsky  M,  Komodakis  N.  Dynamic  edge-conditioned  filters  in 
convolutional  neural  networks  on  graphs.  ArXiv170402901  Cs 
[Internet].  2017  [cited  2020  Aug  28];  Available  from:  http://arxiv. 
org/abs/1704.02901

20.  Bianchi  FM,  Grattarola  D,  Livi  L,  et  al.  Hierarchical  representation 
learning  in  graph  neural  networks  with  node  decimation  pooling. 
ArXiv191011436  Cs  Math  Stat  [Internet].  2019  [cited  2020  Jun  15]; 
Available from:  http://arxiv.org/abs/1910.11436

21.  Ying  R,  You  J,  Morris  C,  et  al.  Hierarchical  graph  representation 
learning  with  differentiable  pooling.  ArXiv180608804  Cs  Stat 
[Internet].  2019  [cited  2021  Jan  30];  Available  from:  http://arxiv. 
org/abs/1806.08804

22.  Hochreiter  S,  Schmidhuber  J.  Long  short-term  memory.  Neural 

Comput.  1997;9:1735–1780.

23. Raina R, Madhavan A, Ng AY Large-scale deep unsupervised learn-
ing using graphics processors. Proc 26th Annu Int Conf Mach Learn 
[Internet].  New  York,  NY,  USA:  Association 
for  Computing 
Machinery;  2009  [cited  2021  Jan  30].  p.  873–880.  doi:10.1145/ 
1553374.1553486.

24.  Abadi  M,  Barham  P,  Chen  J,  et  al.  TensorFlow:  a  system  for 
large-scale  machine  learning.  12th  USENIX  Symp  Oper  Syst  Des 
Implement  OSDI  16  [Internet].  Savannah,  GA.  2016  [cited  2021 
Jan  30].  p.  265–283.  Available  from:  https://www.usenix.org/sys 
tem/files/conference/osdi16/osdi16-abadi.pdf

25.  Paszke  A,  Gross  S,  Chintala  S,  et  al.  Automatic  differentiation  in 
PyTorch.  2017  [cited  2021  Jan  30];  Available  from:  https://openre 
view.net/forum?id=BJJsrmfCZ

26.  Paszke  A,  Gross  S,  Massa  F,  et  al.  PyTorch:  an  imperative  style, 
high-performance  deep  learning  library.  NeurIPS.  2019.  Available 
from:  https://proceedings.neurips.cc/paper/2019/file/bdbca288 
fee7f92f2bfa9f7012727740-Paper.pdf

27.  The  Theano  Development  Team,  Al-Rfou  R,  Alain  G,  et  al.  Theano: 
a  Python 
fast  computation  of  mathematical 
expressions.  ArXiv160502688  Cs  [Internet].  2016  [cited  2021  Jan 
30];  Available from: http://arxiv.org/abs/1605.02688

framework 

for 

28. Jia  Y,  Shelhamer  E,  Donahue  J,  et  al.  Caffe:  convolutional  archi-
tecture  for  fast  feature  embedding.  ArXiv14085093  Cs  [Internet]. 
2014  [cited  2021  Jan  30];  Available  from:  http://arxiv.org/abs/ 
1408.5093

29.  Seide  F,  Agarwal A.  CNTK:  microsoft’s  Open-Source Deep-Learning 
Toolkit.  Proc  22nd  ACM  SIGKDD  Int  Conf  Knowl  Discov  Data  Min 
[Internet].  New  York,  NY,  USA:  Association 
for  Computing 
Machinery;  2016  [cited  2021  Jan  30].  p.  2135.  doi:10.1145/ 
2939672.2945397

30.  Lander ES, Linton LM, Birren B, et al. Initial sequencing and analysis 

of the  human genome.  Nature. 2001;409:860–921.

31.  Venter JC, Adams MD, Myers EW, et al. The Sequence of the Human 

Genome. Science. 2001;291(5507):1304–1351.

32.  International  Human  Genome  Sequencing  Consortium.  Finishing 
the  euchromatic  sequence  of  the  human  genome.  Nature. 
2004;431(7011):931–945.

33.  Anfinsen  CB,  Haber  E,  Sela  M,  et  al.  The  kinetics  of  formation  of 
native  ribonuclease  during  oxidation  of  the  reduced  polypeptide 
chain.  Proc Natl  Acad Sci  U S A. 1961;47(9): 1309–1314. 

•  This  is  one  of  the  classic  publications  that  helped  to  establish 

some  of  the  fundamental  principles  of  protein  folding.

34.  Anfinsen  CB.  Principles  that  govern  the  folding  of  protein  chains. 

Science.  1973;181(4096):223–230. 

••  Anfinsen  published  this  after  receiving  the  Nobel  Prize  in 
Chemistry  in  1972.  This  work  summarizes  many  years  of 
work  studying  the  refolding  of  ribonuclease,  which  ultimately 
led  to  the  thermodynamic  hypothesis  of  protein  folding  (that 
the  native  state  of  a  protein  is  the  most  thermodynamically 
stable  under  cellular  conditions).

35.  Dill KA, Chan HS. From Levinthal to pathways to funnels. Nat Struct 

Biol. 1997;4(1):10.

36.  Maveyraud  L,  Mourey  L.  Protein  X-ray  crystallography  and  drug 

discovery. Mol Basel Switz. 2020;25

37.  Markley  JL.  View  from  nuclear  magnetic  resonance  spectroscopy. 

Adv  Exp Med Biol. 2018;1105:19–22.

38.  Baldwin  AJ,  Kay  LE.  NMR  spectroscopy  brings  invisible  protein 

states  into focus. Nat Chem Biol. 2009;5(11):808–814.

39.  Benjin  X,  Ling  L.  Developments,  applications,  and  prospects  of 
cryo-electron  microscopy.  Protein  Sci  Publ  Protein  Soc.  2020;29 
(4):872–882.

40.  García-Nafría J, Tate CG. Cryo-electron microscopy: moving beyond 
X-ray  crystal  structures  for  drug  receptors  and  drug  development. 
Annu Rev Pharmacol Toxicol. 2020;60(1):51–71.

41.  O’Leary  NA,  Wright  MW,  Brister  JR,  et  al.  Reference  sequence 
(RefSeq)  database  at  NCBI:  current  status,  taxonomic  expansion, 
and  functional  annotation.  Nucleic  Acids  Res.  2016;44(D1):D733– 
745.

42.  Burley  SK, Berman HM, Bhikadiya  C,  et al. RCSB protein data bank: 
biological macromolecular structures enabling research and educa-
tion  in  fundamental  biology,  biomedicine,  biotechnology  and 
energy. Nucleic  Acids Res.  2019;47(D1):D464–D474.

43. Mittag  T, Kay  LE, Forman-Kay  JD. Protein dynamics  and conforma-
tional  disorder  in  molecular  recognition.  J  Mol  Recognit.  2010;23 
(2):105–116.

44.  Lindorff-Larsen K, Piana S, Dror RO, et al. How fast-folding proteins 

fold. Science.  2011;334(6055):517–520.

45.  Hollingsworth  SA,  Dror  RO.  Molecular  dynamics  simulation  for  all. 

Neuron. 2018;99(6):1129–1143.

46.  Alford  RF,  Leaver-Fay  A,  Jeliazkov  JR,  et  al.  The  Rosetta  all-atom 
energy function for macromolecular modeling and design. J Chem 
Theory Comput. 2017;13(6):3031–3048.

47.  Park  H,  Bradley  P,  Greisen  P,  et  al.  Simultaneous  optimization  of 
biomolecular  energy  functions  on  features  from  small  molecules 
and  macromolecules. 
J  Chem  Theory  Comput.  2016;12 
(12):6201–6212.

48.  Kryshtafovych  A,  Schwede  T,  Topf  M,  et  al.  Critical  assessment  of 
methods  of  protein  structure  prediction  (CASP)—Round  XIII. 
Proteins Struct  Funct Bioinforma. 2019;87(12):1011–1020.

49.  Leaver-Fay A, Tyka M, Lewis SM, et al. Rosetta3. Methods enzymol. 

66.  Grant  MA.  Protein  structure  prediction  in  structure-based  ligand 
design  and  virtual  screening.  Comb  Chem  High  Throughput 
Screen.  2009;12(10):940–960.

67.  Nerli 

S, 

Sgourakis  NG.  CS-ROSETTA.  Methods 

Enzymol. 

EXPERT  OPINION ON DRUG  DISCOVERY

17

San Diego, CA: Elsevier. 2011:545–574. 

•  Of  the  works  published  on  the  Rosetta  software  suite,  this 
provides  the  most  detailed  overview  of  the  core  architecture 
of  the  current  version  of  the  software  (Rosetta  3).  Rosetta  is 
one of the top simulation-based packages for macromolecular 
modelling,  and  has  remained  a  key  part  of  the  pipeline  of 
many  deep  learning-based  approaches  including  AlphaFold 
version  1  and  trRosetta.

50.  Koehler  Leman  J,  Weitzner  BD,  Renfrew  PD,  et  al.  Better  together: 
elements  of  successful  scientific  software  development 
in 
a  distributed  collaborative  community.  PLoS  Comput  Biol. 
2020;16:e1007507. 

••  As  deep  learning  approaches  grow  more  central  to  macromo-
lecular  modelling,  it  will  be  necessary  to  engineer  maintain-
able,  modular,  well-tested  methods  that  can  be  robustly 
incorporated  into  biological  discovery  and  bioengineering 
pipelines.  This  review  describes  the  growing  pains  and  best 
practices  discovered  in  the  Rosetta  community  for  develop-
ment,  maintenance,  and  testing  of  the  Rosetta  software.  The 
lessons  learnt  here  are  likely  broadly  applicable.

51.  Xu  D,  Zhang  Y.  Improving  the  physical  realism  and  structural 
accuracy  of  protein  models  by  a  two-step  atomic-level  energy 
minimization. Biophys  J. 2011;101(10):2525–2534.

52. Xu  D,  Zhang  Y.  Ab  initio  protein  structure  assembly  using  contin-
uous  structure  fragments  and  optimized  knowledge-based  force 
field.  Proteins. 2012;80:1715–1735.

53.  Handl  J,  Knowles  J,  Vernon  R,  et  al.  The  dual  role  of  fragments  in 
fragment-assembly methods for de novo protein structure prediction. 
Proteins: Structure, Function, and Bioinformatics. 2012;80(2):490–504.
54.  Xu D, Zhang Y. Toward optimal fragment generations for ab initio 
protein  structure  assembly.  Proteins:  Structure,  Function,  and 
Bioinformatics. 2013;81(2):229–239.

55.  Zhang J, Wang Q, Barz B, et al. MUFOLD: a new solution for protein 
3D  structure  prediction.  Proteins:  Structure,  Function,  and 
Bioinformatics. 2010;78(5):1137–1152.

56.  Spencer M, Eickholt J, Cheng J. A deep learning network approach 
to ab initio protein secondary structure prediction. IEEE/ACM Trans 
Comput Biol Bioinform.  2015;12(1):103–112.

57.  Wang T, Qiao Y, Ding W, et al. Improved fragment sampling for ab 
initio protein structure prediction using deep neural networks. Nat 
Mach  Intell. 2019;1(8):347–355.

58.  Ginalski  K,  Elofsson  A,  Fischer  D,  et  al.  3D-Jury:  a  simple  approach  to 
improve  protein  structure  predictions.  Bioinformatics.  2003;19 
(8):1015–1018.

59.  Lundström  J,  Rychlewski  L,  Bujnicki  J,  et  al.  Pcons:  a  neural- 
network–based  consensus  predictor  that  improves  fold  recogni-
tion. Protein Sci  Publ Protein Soc. 2001;10(11):2354–2362.

60.  Bujnicki  JM,  Elofsson  A,  Fischer  D,  et  al.  Structure  prediction  meta 

server. Bioinformatics.  2001;17(8):750–751.

61.  Wang Z, Eickholt J, Cheng J. MULTICOM: a multi-level combination 
approach  to  protein  structure  prediction  and  its  assessments  in 
CASP8. Bioinformatics. 2010;26(7):882–888.

62.  Brünger AT, Adams PD, Clore GM, et al. Crystallography & NMR 
system:  a  new  software  suite  for  macromolecular  structure 
determination. 
Crystallogr. 
1998;54:905–921.

Crystallogr  D 

Acta 

Biol 

63.  Misura KMS, Chivian D, Rohl CA, et al. Physically realistic homology 
models  built  with  Rosetta  can  be  more  accurate  than  their 
templates. Proc Natl  Acad Sci.  2006;103(14):5361–5366.

64.  Yang  J,  Yan  R,  Roy  A,  et  al.  The  I-TASSER  Suite:  protein  structure 

and function  prediction. Nat Methods. 2015;12(1):7–8.

65. Källberg  M,  Wang  H,  Wang  S,  et  al.  Template-based  protein  struc-
ture  modeling  using  the  RaptorX  web  server.  Nat  Protoc.  2012;7 
(8):1511–1522.

2019;614:321–362.

68.  DiMaio  F,  Song  Y,  Li  X,  et  al.  Atomic-accuracy  models  from  4.5-Å 
cryo-electron  microscopy  data  with  density-guided  iterative  local 
refinement. Nat  Methods. 2015;12:361–365.

69.  Frenz B, Walls AC, Egelman EH, et al. RosettaES: a sampling strategy 
enabling  automated  interpretation  of  difficult  cryo-EM  maps.  Nat 
Methods. 2017;14(8):797–800.

70. Hirst  SJ,  Alexander  N,  Mchaourab  HS,  et  al.  RosettaEPR:  an  inte-
grated  tool  for  protein  structure  determination  from  sparse  EPR 
data. J Struct  Biol. 2011;173(3):506–514.

71. Fischer  AW,  Alexander  NS,  Woetzel  N,  et  al.  BCL::MP-fold:  mem-
brane  protein  structure  prediction  guided  by  EPR  restraints. 
Proteins Struct  Funct Bioinforma.  2015;83:1947–1962.

72.  Tessmer  MH,  Anderson  DM,  Pickrum  AM,  et  al.  Identification  of  a 
ubiquitin-binding interface using Rosetta and DEER. Proc Natl Acad 
Sci.  2018;115(3):525–530.

73.  Qian  B,  Raman  S,  Das  R,  et  al.  High  resolution  protein  structure 
prediction  and  the  crystallographic  phase  problem.  Nature. 
2007;450(7167):259–264.

74.  DiMaio  F.  Rosetta  structure  prediction  as  a  tool  for  solving  difficult 
molecular replacement problems. Wlodawer A, Dauter Z, Jaskolski M, 
editors  Protein  crystallogr.  [Internet]  New  York:  Springer  New  York; 
2017.  [cited  2017  Oct  26].  p.  455–466.  Available  from.  http://link. 
springer.com/10.1007/978-1-4939-7000-1_19

75.  Heo L, Arbour CF, Feig M. Driven to near-experimental accuracy by 
refinement  via  molecular  dynamics  simulations.  Proteins  Struct 
Funct Bioinforma.  2019;87(12):1263–1275.

76.  Göbel  U,  Sander  C,  Schneider  R,  et  al.  Correlated  mutations  and 
residue  contacts  in  proteins.  Proteins  Struct  Funct  Bioinforma. 
1994;18(4): 309–317. 

•  This was one of the first attempts to infer inter-residue contact 

information  from  multiple  sequence  alignments.

77.  Ortiz  AR,  Hu  WP,  Kolinski  A,  et  al.  Method  for  low  resolution 
structure.  Pac  Symp 

small  protein 

tertiary 

prediction  of 
Biocomput. 1997:316–327. 

•  Although  robust  structure  prediction  using  contact,  distance, 
or orientation information from multiple sequence alignments 
did  not  come  of  age  until  recently,  early  attempts  to  use  this 
information date back to the 1990s. This paper represents one 
of  the  first.

78.  Weigt  M,  White  RA,  Szurmant  H,  et  al.  Identification  of  direct 
residue  contacts  in  protein–protein  interaction  by  message  pas-
sing. Proc Natl  Acad Sci U S A. 2009;106(1):67–72.

79.  Morcos  F,  Pagnani  A,  Lunt  B,  et  al.  Direct-coupling  analysis  of 
residue  coevolution  captures  native  contacts  across  many  protein 
families.  Proc Natl  Acad Sci.  2011;108(49):E1293–E1301.

80.  Jones DT, Buchan DWA, Cozzetto D, et al. PSICOV: precise structural 
contact  prediction  using  sparse  inverse  covariance  estimation  on 
large  multiple  sequence  alignments.  Bioinforma  Oxf  Engl. 
2012;28:184–190.

81.  Kamisetty  H,  Ovchinnikov  S,  Baker  D.  Assessing  the  utility  of 
residue-residue  contact  predictions 
in  a 
structure-rich  era.  Proc  Natl  Acad  Sci. 

and 

coevolution-based 
sequence- 
2013;110:15674–15679.

82.  Ovchinnikov  S,  Kim  DE,  Wang  -RY-R,  et  al.  Improved  de  novo 
structure prediction in CASP11 by incorporating coevolution infor-
mation 
and 
Bioinformatics.  2016;84(Suppl 1):67–75.

into  Rosetta.  Proteins:  Structure, 

Function, 

83.  Ovchinnikov  S,  Park  H,  Kim  DE,  et  al.  Protein  structure  prediction 
in  CASP12.  Proteins:  Structure,  Function,  and 

using  Rosetta 
Bioinformatics.  2018;86:113–121. 

••  Structure  prediction  using  contact  predictions  showed  its  uti-
lity  in  the  CASP12  competition,  where  Rosetta-based  predic-
tion  informed  by  GREMLIN-generated  contact  maps  from 
multiple sequence alignments outperformed all other methods 

18

V. K. MULLIGAN

in  the  regular  targets  category.  This  paper  summarizes  the 
advancements  made  in  the  2016  competition.

84.  He  B,  Mortuza  SM,  Wang  Y,  et  al.  NeBcon:  protein  contact  map 
prediction using neural network training coupled with naïve Bayes 
classifiers. Bioinformatics. 2017;33(15):2296–2306.

85. Zhang C, Mortuza SM, He B, et al. Template-based and free model-
ing of I-TASSER and QUARK pipelines using predicted contact maps 
in CASP12. Proteins Struct  Funct Bioinforma.  2018;86:136–151.

86.  Fang  C,  Shang  Y,  Xu  D.  MUFOLD-SS:  new  deep  inception-inside- 
inception  networks  for  protein  secondary  structure  prediction. 
Proteins:  Structure,  Function,  and  Bioinformatics.  2018;86 
(5):592–598.

87.  Fang C, Shang Y, Xu D. A deep dense inception network for protein 
and 

Structure, 

Function, 

beta-turn  prediction. 
Proteins: 
Bioinformatics. 2020;88(1):143–151.

88.  Mirdita M, Von Den Driesch L, Galiez C, et al. Uniclust databases of 
sequences  and 

clustered  and  deeply  annotated  protein 
alignments.  Nucleic Acids  Res. 2017;45(D1):D170–D176.

89.  Altschul  SF,  Madden  TL,  Schäffer  AA,  et  al.  Gapped  BLAST  and 
PSI-BLAST:  a  new  generation  of  protein  database  search 
programs. Nucleic  Acids  Res. 1997;25(17):3389–3402.

90.  Senior  AW,  Evans  R,  Jumper  J,  et  al.  Improved  protein  structure 
prediction  using  potentials  from  deep  learning.  Nature.  2020;577 
(7792): 706–710. 

••  The  version  1  AlphaFold  method  is  described  in  this  paper. 
AlphaFold was the top performer in the CASP13 competition in 
2018,  using  distance  distribution  predictions  produced  by 
a  deep  neural  network  to  guide  Rosetta  energy  minimization.
91. Xu  J,  Wang  S.  Analysis  of  distance-based  protein  structure  predic-
tion by deep learning in CASP13. Proteins: Structure, Function, and 
Bioinformatics. 2019;87(12):1069–1081.

92.  Hou J, Wu T, Cao R, et al. Protein tertiary structure modeling driven 
by  deep  learning  and  contact  distance  prediction  in  CASP13. 
Proteins Struct  Funct Bioinforma. 2019;87(12):1165–1178.

93.  Heo  L,  Feig  M.  High-accuracy  protein  structures  by  combining 
machine-learning  with  physics-based  refinement.  Proteins  Struct 
Funct Bioinforma.  2020;88(5):637–642.

94.  Hatos  A,  Hajdu-Soltész  B,  Monzon  AM,  et  al.  DisProt:  intrinsic 
protein  disorder  annotation  in  2020.  Nucleic  Acids  Res.  2020;48 
(D1):D269–D276.

95.  Wang  S,  Li  W,  Liu  S,  et  al.  RaptorX-property:  a  web  server  for 
protein  structure  property  prediction.  Nucleic  Acids  Res.  2016;44 
(W1):W430–435.

96.  Hanson  J,  Paliwal  KK,  Litfin  T,  et  al.  SPOT-Disorder2:  improved 
protein  intrinsic  disorder  prediction  by  ensembled  deep  learning. 
Genomics Proteomics Bioinformatics. 2019;17:645–656.

97.  Hanson J, Litfin T, Paliwal K, et al. Identifying molecular recognition 
features  in  intrinsically  disordered  regions  of  proteins  by  transfer 
learning. Bioinforma  Oxf Engl. 2020;36:1107–1113.

98.  Barik A, Katuwawala A, Hanson J, et al. DEPICTER: intrinsic disorder 
and  disorder  function  prediction  server.  J  Mol  Biol.  2020;432 
(11):3379–3387.

99.  He B,  Wang K, Liu Y, et al.  Predicting intrinsic disorder in proteins: 

an overview. Cell Res. 2009;19(8):929–949.

100.  Liu  Y,  Wang  X,  Liu  B.  A  comprehensive  review  and  comparison  of 
existing computational methods for intrinsically disordered protein 
and region  prediction. Brief Bioinform.  2019;20(1):330–346.

101.  AlQuraishi  M.  End-to-end  differentiable 

learning  of  protein 

structure. Cell Syst.  2019;8(4):292–301.e3.

102.  Rocklin  GJ,  Chidyausiku  TM,  Goreshnik  I,  et  al.  Global  analysis  of 
protein  folding  using  massively  parallel  design,  synthesis,  and 
testing. Science. 2017;357(6347):168–175.

103.  Gao  W,  Mahajan  SP,  Sulam  J,  et  al.  Deep  learning  in  protein 

structural modeling  and design. Patterns.  2020;1(9): 100142. 

•  This  review  covers  various  deep  learning  approaches  relevant 
to  modelling  proteins,  with  greater  coverage  of  sequence- 
based  methods.

104.  Rawat  W,  Wang  Z.  Deep  convolutional  neural  networks  for  image 
classification:  a  comprehensive  review.  Neural  Comput.  2017;29 
(9):2352–2449.

105.  Simonovsky  M,  Meyers  J.  DeeplyTough: 

learning  Structural 
Comparison  of  Protein  Binding  Sites.  J  Chem  Inf  Model.  2020;60 
(4):2356–2366. 

•  An  early  attempt  to  use  deep  learning  to  extract  information 
directly  from  structures  rather  than  sequences,  this  paper 
demonstrates  how  structural  data  can  be  voxelized  and  ana-
lyzed  using  a  3D  convolutional  neural  network.

106.  Sverrisson F, Feydy J, Correia BE, et al. Fast end-to-end learning on 

protein surfaces.  bioRxiv. 2020. 

••  Although  this  is  a  preprint  and  should  be  treated  with  due 
caution  until  peer-reviewed,  it  contains  many  fascinating 
ideas.  The  authors  use  deep  geometric  convolutional  neural 
networks  to  analyse  protein  surfaces,  transforming  atomic 
coordinates 
in 
a  manner  that  preserves  end-to-end  differentiability.  The 
architecture  described  is  general  and  should  be  applicable  to 
many  analysis  tasks.

surface  point-cloud 

in-network 

into  a 

107.  Abe  H,  Braun  W,  Noguti  T,  et  al.  Rapid  calculation  of  first 
and  second  derivatives  of  conformational  energy  with  respect  to 
dihedral  angles  for  proteins  general  recurrent  equations.  Comput 
Chem. 1984;8(4):239–247.

108. Gligorijević  V,  Renfrew  PD,  Kosciolek  T,  et  al.  Structure-based  pro-
tein  function  prediction  using  graph  convolutional  networks.  Nat 
Commun.  2021;In  press.  Preprint  available  from  https://www.bior 
xiv.org/content/10.1101/786236v2 

••  This  paper  demonstrates  how  graph  representations,  which 
are  convenient  given  their  rotation-  and  permutation- 
equivariance,  can  be  used  to  extract  functional  information 
directly  from  protein  structure.  This  architecture  should  also 
be  applicable  to many  other  structural  analysis  tasks.

109.  Kuhlman  B,  Dantas  G,  Ireton  GC,  et  al.  Design  of  a  novel  globular 
protein  fold  with  atomic-level  accuracy.  Science.  2003;302(5649): 
1364–1368. 

•  A  seminal  paper,  this  was  the  first  to  use  simulation-based 
computational  methods  to  design  an  amino  acid  sequence 
that  folds  into  a  new  structure  not  found  in  nature.

110.  Koga N, Tatsumi-Koga R, Liu G, et al. Principles for designing ideal 

protein structures.  Nature. 2012;491(7423):222–227.

111.  Bhardwaj G, Mulligan VK, Bahl CD, et al. Accurate de novo design of 
constrained  peptides.  Nature.  2016;538(7625): 

hyperstable 
329–335. 

••  This  paper  demonstrated 

that  simulation-based  design 
approaches could be used to extrapolate beyond the examples 
provided by nature, and to design folding molecules built from 
mixtures  of  natural  and  non-natural  chemical  building  blocks. 
Achieving 
learning-based 
approaches  is  likely  to  remain  a  challenge  for  some  time, 
both since the representation of inputs and outputs is challen-
ging  and  because  available  training  data  is  rare  for  the  realm 
beyond  the  canonical  amino  acids.

equivalent  with  deep 

the 

112.  Hosseinzadeh  P,  Bhardwaj  G,  Mulligan  VK,  et  al.  Comprehensive 
computational  design  of  ordered  peptide  macrocycles.  Science. 
2017;358(6369):1461–1466.

113.  Mulligan  VK,  Kang  CS,  Sawaya  MR,  et  al.  Computational  design  of 
internal  symmetry. 

mixed  chirality  peptide  macrocycles  with 
Protein Sci.  2020;29(12):2433–2445.

114.  Fleishman SJ, Whitehead TA, Ekiert DC, et al. Computational design 
of  proteins  targeting  the  conserved  stem  region  of  influenza 
hemagglutinin. Science. 2011;332(6031):  816–821. 

•  The  power  of 

simulation-based 

computational  design 
approaches  for  drug  development  was  demonstrated  by  this 
paper, in which the authors produce synthetic proteins able to 
block  the  function  of  influenza  hemagglutinin  and  to  neutra-
lize  the  influenza  virus.

115.  Strauch  E-M,  Bernard  SM,  La  D,  et  al.  Computational  design  of 
trimeric  influenza-neutralizing  proteins  targeting  the  hemaggluti-
nin receptor binding site. Nat Biotechnol. 2017;35(7):667–671.

116.  Gonen  S,  DiMaio  F,  Gonen  T,  et  al.  Design  of  ordered 
two-dimensional  arrays  mediated  by  noncovalent  protein-protein 
interfaces.  Science. 2015;348(6241):1365–1368.

117.  Hsia  Y, Bale  JB,  Gonen  S, et  al. Design  of  a  hyperstable 60-subunit 

protein icosahedron. Nature. 2016;535(7610):136–139.

118.  Dang  B,  Wu  H,  Mulligan  VK,  et  al.  De  novo  design  of  covalently 
constrained  mesosize  protein  scaffolds  with  unique  tertiary 
structures.  Proc Natl  Acad Sci.  2017;114(41):10852–10857.

119.  Mulligan  VK,  Workman  S,  Sun  T,  et  al.  Computationally-designed 
peptide macrocycle inhibitors of New Delhi metallo-β-lactamase 1. 
Proc Natl  Acad Sci U S A. 2021;118(12):e2012800118.

120.  Anand N, Huang P. Generative modeling for protein structures. Adv 

Neural Inf Process Syst. 2018;31:7494–7505. 

••  Although  deep  learning-based  design  is  in  its  infancy,  this 
paper  describes  the  use  of  generative  adversarial  networks 
(GANs)  to  produce  new  protein  backbone  conformations 
amenable  to  simulation-based  sequence  design  approaches.

121.  Anand N, Eguchi R, Huang P-S Fully differentiable full-atom protein 
backbone  generation.  2019  [cited  2021  Jan  28];  Available  from: 
https://openreview.net/forum?id=SJxnVL8YOV

122.  Eguchi RR, Anand N, Choe CA, et al. IG-VAE: generative modeling of 
immunoglobulin  proteins  by  direct  3D  coordinate  generation. 
bioRxiv. 2020. 

•  Incorporation  of  emerging  design  technologies  into  drug 
development  pipelines  is  challenging.  This  preprint  describes 
the design of immunoglobulin-like molecules intended both to 
adopt  the  desired  fold  and  to  bind  to  a  target,  using  varia-
tional  neural  networks  to  rapidly  sample  from  the  space  of 
immunoglobulin-like folds. Although it should be treated cau-
tiously until peer-reviewed, the methods described could have 
enormous  impact  for  therapeutic  protein development.

123.  Anishchenko  I,  Chidyausiku  TM,  Ovchinnikov  S,  et  al.  De  novo 
hallucination.bioRxiv. 

network 

deep 

protein 
by 
2020;2020.07.22.211482. 

design 

•  Although  many  examples  of  proteins  designed  de  novo  now 
exist,  simulation-based  design  remains  a  laborious  trial-and- 
error  process  with  a  high  failure  rate.  This  paper  offers  early 
experimental  evidence  that  higher  success  rates  may  be  pos-
sible  by  applying  neural  networks  originally  developed  for 
structure  prediction 
in  reverse,  to  generate  sequences. 
However,  this  is  a  preprint  and  its  claims  should  be  viewed 
with  due  skepticism  until  peer-reviewed.

124.  Kullback  S,  Leibler  RA.  On  Information  and  Sufficiency.  Ann  Math 

Stat. 1951;22:79–86.

125.  Norn  C,  Wicky  BIM,  Juergens  D,  et  al.  Protein  sequence  design  by 
conformational  landscape  optimization.  Proc  Natl  Acad  Sci  USA. 
2021;118(11):e2017228118. 

•  Simulation-based  protein  design  can  only  consider  the  target 
conformation  during  the  design  process,  meaning  that 
a  common  failure  mode  is  the  production  of  sequences  that 
do  not  uniquely  stabilize  the  desired  conformation.  This  pre-
print  offers  some  evidence 
learning-based 
approaches may be better able to implicitly consider the entire 
conformational  space,  and  to  design  sequences  that  uniquely 
stabilize  the  desired  fold.

that  deep 

126.  Case  DA,  Cheatham  TE,  Darden  T,  et  al.  The  Amber  biomolecular 

simulation programs. J Comput Chem. 2005;26(16):1668–1688.

127.  Huang J, Rauscher S, Nawrocki G, et al. CHARMM36m: an improved 
force  field  for  folded  and  intrinsically  disordered  proteins.  Nat 
Methods. 2017;14(1):71–73.

128.  Gniewek  P,  Leelananda  SP,  Kolinski  A,  et  al.  Multibody 
coarse-grained potentials for native structure recognition and qual-
ity assessment of protein models. Proteins Struct Funct Bioinforma. 
2011;79(6):1923–1929.

129.  Keasar  C,  Levitt  M,  Novel  A.  Approach  to  Decoy  set  generation: 
designing  a  physical  energy  function  having  local  minima  with 
native structure  characteristics. J Mol Biol. 2003;329(1):159–174.

130.  Dill  KA,  Bromberg  S,  Yue  K,  et  al.  Principles  of  protein  folding  — 
a perspective from simple exact models. Protein Sci. 1995;4:561–602.

EXPERT  OPINION ON DRUG  DISCOVERY

19

131.  Schütt  KT,  Gastegger  M,  Tkatchenko  A,  et  al.  Unifying  machine 
learning  and  quantum  chemistry  with  a  deep  neural  network  for 
molecular wavefunctions. Nat Commun.  2019;10(1):5024.

132.  Hermann J, Schätzle Z, Noé F. Deep-neural-network solution of the 

electronic Schrödinger equation.  Nat Chem. 2020;12(10):891–897.

133.  Pfau  D,  Spencer  JS,  Matthews  Ag  De  G,  et  al.  Ab  initio  solution  of 
the  many-electron  Schrödinger  equation  with  deep  neural  net-
works. Phys  Rev Res. 2020;2(3):033429.

134. Hutchinson  ST,  Kobayashi  R.  Solvent-specific  featurization  for  pre-
learning. 

dicting  free  energies  of  solvation  through  machine 
J Chem Inf Model. 2019;59(4):1338–1346.

135.  Bennett  WFD,  He  S,  Bilodeau  CL,  et  al.  Predicting  small  molecule 
transfer  free  energies  by  combining  molecular  dynamics  simula-
tions  and  deep 
Inf  Model.  2020;60 
(11):5375–5381.

J  Chem 

learning. 

136.  Pathak  Y,  Mehta  S,  Priyakumar  UD.  Learning  atomic  interactions 
through  solvation  free  energy  prediction  using  graph  neural 
networks. J Chem Inf Model. 2021;61(2):689–698.

137.  Falke  S,  Feiler  C,  Chapman  H,  et  al.  Crystal  structures  of native 
cytochrome  c6  from  Thermosynechococcus  elongatus  in  two 
different 
its 
oligomerization.  Acta  Crystallogr  Sect  F  Struct  Biol  Commun. 
2020;76:444–452.

implications 

groups 

space 

and 

for 

138.  Kendrew JC, Bodo G, Dintzis HM, et al. A three-dimensional model 
of  the  myoglobin  molecule  obtained  by  X-ray  analysis.  Nature. 
1958;181(4610):  662–666. 

••  This was the  first publication of a protein structure, solved by 
x-ray  crystallography.  Myoglobin,  the  first  protein  solved, 
remains  intractable  for  deep  learning-based  structure  predic-
tion approaches since its structure incorporates a heme group, 
which  cannot  be  expressed  in  current  network  inputs  or 
outputs.

139.  Patel  P,  Prescott  GR,  Burgoyne  RD,  et  al.  Phosphorylation  of 
cysteine  string  protein  triggers  a  major  conformational  switch. 
Struct  Lond Engl  1993. 2016;24:1380–1386.

140. Tomaselli  S,  Esposito  V,  Vangone  P,  et  al.  The  alpha-to-beta  con-
formational  transition  of  Alzheimer’s  Abeta-(1-42)  peptide  in  aqu-
eous  media  is  reversible:  a  step  by  step  conformational  analysis 
suggests the location of beta conformation seeding. Chembiochem 
Eur J Chem Biol. 2006;7:257–267.

141.  Siegel JB, Zanghellini A, Lovick HM, et al. Computational design of 
an  enzyme  catalyst  for  a  stereoselective  bimolecular  Diels-Alder 
reaction. Science.  2010;329(5989):309–313.

142.  Gordon SR, Stanley EJ, Wolf S, et al. Computational design of an α- 

gliadin peptidase. J Am Chem Soc. 2012;134:20513–20520.

143.  King  NP,  Sheffler  W,  Sawaya  MR,  et  al.  Computational  design  of 
self-assembling  protein  nanomaterials  with  atomic  level  accuracy. 
Science.  2012;336(6085):1171–1174.

144.  Watkins  AM,  Geniesse  C,  Kladwang  W,  et  al.  Blind  prediction  of 
noncanonical  RNA  structure  at  atomic  accuracy.  Sci  Adv.  2018;(4): 
eaar5316. doi:10.1126/sciadv.aar5316.

145.  Anishchenko  I  gjoni/trRosetta  repository  [Internet].  2021  [cited 

2021 Jan 31].  Available from:  https://github.com/gjoni/trRosetta. 

•  Although not a paper, the git repository for trRosetta serves as 
a  model  for  open,  reproducible  science.  All  neural  net  para-
meters  and  scripts  needed  to  generate  inputs  or  analyze  out-
puts  are  released  under  permissive 
licences,  facilitating 
development  atop the  method.

146.  deepmind/deepmind-research  AlphaFold  v1  repository  [Internet]. 
GitHub.  [cited  2021  Jan  31].  Available  from:  https://github.com/ 
deepmind/deepmind-research

147.  BenevolentAI/DeeplyTough  licence  file  [Internet].  GitHub.  [cited 
2021  Jan  31].  Available  from:  https://github.com/BenevolentAI/ 
DeeplyTough

