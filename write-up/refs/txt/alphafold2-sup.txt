In the format provided by the 
authors and unedited

Highly accurate protein structure prediction 
with AlphaFold

ACCELERATED ARTICLE PREVIEW 

Supplementary informationhttps://doi.org/10.1038/s41586-021-03819-2Supplementary Information for: Highly accurate

protein structure prediction with AlphaFold

John Jumper11*+, Richard Evans1*, Alexander Pritzel1*, Tim Green1*, Michael
Figurnov1*, Olaf Ronneberger1*, Kathryn Tunyasuvunakool1*, Russ Bates1*,

Augustin Žídek1*, Anna Potapenko1*, Alex Bridgland1*, Clemens Meyer1*, Simon
A A Kohl1*, Andrew J Ballard1*, Andrew Cowie1*, Bernardino Romera-Paredes1*,

Stanislav Nikolov1*, Rishub Jain1*, Jonas Adler1, Trevor Back1, Stig Petersen1,
David Reiman1, Ellen Clancy1, Michal Zielinski1, Martin Steinegger2, Michalina
Pacholska1, Tamas Berghammer1, Sebastian Bodenstein1, David Silver1, Oriol
Vinyals1, Andrew W Senior1, Koray Kavukcuoglu1, Pushmeet Kohli1, and Demis

Hassabis1*+

2School of Biological Sciences and Artiﬁcial Intelligence Institute, Seoul National

1DeepMind, London, UK

University, Seoul, South Korea

*These authors contributed equally

+Corresponding authors: John Jumper, Demis Hassabis

Contents

.

.

.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

Filtering .

1 Supplementary Methods
.
.
.
.
.
.

.
1.1 Notation .
1.2 Data pipeline .

.
.
Parsing .

.
.
.
.
.
.
.
.
1.2.1
.
.
1.2.2 Genetic search .
.
.
1.2.3 Template search .
.
.
1.2.4 Training data .
1.2.5
.
.
.
1.2.6 MSA block deletion .
1.2.7 MSA clustering .
.
.
1.2.8 Residue cropping .
.
1.2.9

4
4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
Featurization and model inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
. 14

1.6.1 MSA row-wise gated self-attention with pair bias . . . . . . . . . . . . . . . . . . .

1.3 Self-distillation dataset
.
1.4 AlphaFold Inference
.
1.5
Input embeddings .
1.6 Evoformer blocks .
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.

.
.

.
.

.

.

1

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

2

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

.
.

.
.

.
.

.
.

1.8 Structure module .

1.7 Additional inputs .

Invariant point attention (IPA)

Side chain and backbone torsion angle loss
Frame aligned point error (FAPE)

1.6.2 MSA column-wise gated self-attention . . . . . . . . . . . . . . . . . . . . . . . . . 15
.
1.6.3 MSA transition .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
.
1.6.4 Outer product mean .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.6.5 Triangular multiplicative update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.6.6 Triangular self-attention .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.6.7 Transition in the pair stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.7.1 Template stack .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.7.2 Unclustered MSA stack .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
1.8.1 Construction of frames from ground truth atom positions . . . . . . . . . . . . . . . . 26
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
1.8.2
1.8.3 Backbone update .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.8.4 Compute all atom coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
1.8.5 Rename symmetric ground truth atoms
. . . . . . . . . . . . . . . . . . . . . . . . . 30
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.8.6 Amber relaxation .
1.9 Loss functions and auxiliary heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
. . . . . . . . . . . . . . . . . . . . . . . 32
1.9.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
1.9.2
1.9.3 Chiral properties of AlphaFold and its loss
. . . . . . . . . . . . . . . . . . . . . . . 34
1.9.4 Conﬁgurations with FAPE(X,Y) = 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
1.9.5 Metric properties of FAPE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
1.9.6 Model conﬁdence prediction (pLDDT)
. . . . . . . . . . . . . . . . . . . . . . . . . 37
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1.9.7 TM-score prediction .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1.9.8 Distogram prediction .
1.9.9 Masked MSA prediction .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1.9.10 “Experimentally resolved” prediction . . . . . . . . . . . . . . . . . . . . . . . . . . 39
1.9.11 Structural violations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
1.11.1 Training stages .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
1.11.2 MSA resampling and ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
1.11.3 Optimization details
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
1.11.4 Parameters initialization .
1.11.5 Loss clamping details .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
.
.
1.11.6 Dropout details .
1.11.7 Evaluator setup .
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
1.11.8 Reducing the memory consumption . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

.
.
1.12.1 Training procedure .
.
1.12.2 Inference and scoring .
.
.
.
.
.
.
.
.

.
1.13.1 Architectural details
.
1.13.2 Procedure .
.
.
1.13.3 Results
.
.
.
1.14 Network probing details
1.15 Novel fold performance .
.
1.16 Visualization of attention .
1.17 Additional results .
.

.
1.10 Recycling iterations .
.
1.11 Training and inference details .
.

1.12 CASP14 assessment .

1.13 Ablation studies .

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

3

List of Supplementary Figures

.

.

.

.
.

.
.

.
.

.
.

.
.

.
.

Input feature embeddings .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1
MSA row-wise gated self-attention with pair bias . . . . . . . . . . . . . . . . . . . . . . . . 15
2
MSA column-wise gated self-attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3
MSA transition layer
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Outer product mean .
5
Triangular multiplicative update using “outgoing” edges
. . . . . . . . . . . . . . . . . . . . 18
6
Triangular self-attention around starting node . . . . . . . . . . . . . . . . . . . . . . . . . . 19
7
Invariant Point Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
8
9
Accuracy distribution of a model trained with dRMSD instead of the FAPE loss . . . . . . . . 36
10 Accuracy of ablations depending on the MSA depth . . . . . . . . . . . . . . . . . . . . . . . 50
11
Performance on a set of novel structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
12 Visualization of row-wise pair attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
. . . . . . . . . . . . . . . . . . . . . 55
13 Visualization of attention in the MSA along sequences
14 Median all-atom RMSD95 on the CASP14 set
. . . . . . . . . . . . . . . . . . . . . . . . . . 56
15 RMSD histograms on the template-reduced recent PDB set. . . . . . . . . . . . . . . . . . . . 57

List of Supplementary Tables

1
2
3
4
5
6

.

.

.

Input features to the model
.
8
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. 24
Rigid groups for constructing all atoms from given torsion angles . . . . . . . . . . . . . . .
Ambiguous atom names due to 180◦-rotation-symmetry . . . . . . . . . . . . . . . . . . . . . 31
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
AlphaFold training protocol
. 43
Training protocol for CASP14 models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. 46
. . . . . . . . . . . 57
Quartiles of RMSD distributions on the template-reduced recent PDB set.

.

.

List of Algorithms

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

. . . . . . . . . . . . . . . . . . . . . . . .

MSABlockDeletion MSA block deletion . . . . .
6
Inference AlphaFold Model Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
. 13
InputEmbedder Embeddings for initial representations . . . . . . . . . . . . . . . . . . . .
. 13
relpos Relative position encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
one_hot One-hot encoding with nearest bin . . . . . . . . . . . . . . . . . . . . . . . . . .
. 13
EvoformerStack Evoformer stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
MSARowAttentionWithPairBias MSA row-wise gated self-attention with pair bias . . . . . . 15
MSAColumnAttention MSA column-wise gated self-attention . . . . . . . . . . . . . . . . . . 16
MSATransition Transition layer in the MSA stack . . . . . . . . . . . . . . . . . . . . . . . 17
OuterProductMean Outer product mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
18
TriangleMultiplicationOutgoing Triangular multiplicative update using “outgoing” edges
18
TriangleMultiplicationIncoming Triangular multiplicative update using “incoming” edges
TriangleAttentionStartingNode Triangular gated self-attention around starting node . . .
. 19
TriangleAttentionEndingNode Triangular gated self-attention around ending node . . . . . 20
PairTransition Transition layer in the pair stack . . . . . . . . . . . . . . . . . . . . . . .
. 20
TemplatePairStack Template pair stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
TemplatePointwiseAttention Template pointwise attention . . . . . . . . . . . . . . . . . . 21
ExtraMsaStack Extra MSA stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
MSAColumnGlobalAttention MSA global column-wise gated self-attention . . . . . . . . . . 22
StructureModule Structure module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

4

21
22
23
24
25
26
27
28
29
30
31
32

rigidFrom3Points Rigid from 3 points using the Gram–Schmidt process . . . . . . . . . . . 26
InvariantPointAttention Invariant point attention (IPA) . . . . . . . . . . . . . . . . . . . 28
BackboneUpdate Backbone update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
computeAllAtomCoordinates Compute all atom coordinates . . . . . . . . . . . . . . . . . . 30
makeRotX Make a transformation that rotates around the x-axis . . . . . . . . . . . . . . . . . 30
renameSymmetricGroundTruthAtoms Rename symmetric ground truth atoms . . . . . . . . . 31
torsionAngleLoss Side chain and backbone torsion angle loss . . . . . . . . . . . . . . . . . 33
computeFAPE Compute the Frame aligned point error . . . . . . . . . . . . . . . . . . . . . . 34
predictPerResidueLDDT Predict model conﬁdence pLDDT . . . . . . . . . . . . . . . . . . 37
RecyclingInference Generic recycling inference procedure . . . . . . . . . . . . . . . . . . 41
RecyclingTraining Generic recycling training procedure . . . . . . . . . . . . . . . . . . . 41
RecyclingEmbedder Embedding of Evoformer and Structure module outputs for recycling . . 42

1 Supplementary Methods

1.1 Notation

We denote the number of residues in the input primary sequence by Nres (cropped during training), the number
of templates used in the model by Ntempl, the number of all available MSA sequences by Nall_seq, the number of
clusters after MSA clustering by Nclust, the number of sequences processed in the MSA stack by Nseq (where
Nseq = Nclust + Ntempl), and the number of unclustered MSA sequences by Nextra_seq (after sub-sampling,
see subsubsection 1.2.7 for details). Concrete values for these parameters are given in the training details
(subsection 1.11). On the model side, we also denote the number of blocks in Evoformer-like stacks by Nblock
(subsection 1.6), the number of ensembling iterations by Nensemble (subsubsection 1.11.2), and the number of
recycling iterations by Ncycle (subsection 1.10).

We present architectural details in Algorithms, where we use the following conventions. We use capitalized
operator names when they encapsulate learnt parameters, e.g. we use Linear for a linear transformation with
a weights matrix W and a bias vector b, and LinearNoBias for the linear transformation without the bias
vector. Note that when we have multiple outputs from the Linear operator at the same line of an algorithm,
we imply different trainable weights for each output. We use LayerNorm for the layer normalization [85]
operating on the channel dimensions with learnable per-channel gains and biases. We also use capitalized
names for random operators, such as those related to dropout. For functions without parameters we use lower
case operator names, e.g. sigmoid, softmax, stopgrad. We use (cid:12) for the element-wise multiplication, ⊗
for the outer product, ⊕ for the outer sum, and a(cid:62)b for the dot product of two vectors.
Indices i, j, k
always operate on the residue dimension, indices s, t on the sequence dimension, and index h on the attention
heads dimension. The channel dimension is implicit and we type the channel-wise vectors in bold, e.g. zij.
Algorithms operate on sets of such vectors, e.g. we use {zij} to denote all pair representations.
In the structure module, we denote Euclidean transformations corresponding to frames by T = (R,(cid:126)t), with
R ∈ R3×3 for the rotation and (cid:126)t ∈ R3 for the translation components. We use the ◦ operator to denote
application of a transformation to an atomic position (cid:126)x ∈ R3:
(cid:126)xresult = T ◦ (cid:126)x

= (R,(cid:126)t) ◦ (cid:126)x
= R(cid:126)x + (cid:126)t .

The ◦ operator also denotes composition of Euclidean transformations:

Tresult = T1 ◦ T2

(Rresult,(cid:126)tresult) = (R1,(cid:126)t1) ◦ (R2,(cid:126)t2)
= (R1R2, R1(cid:126)t2 + (cid:126)t1)

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

5

We use T −1 to denote the group inverse of the transform T :
T −1 = (R,(cid:126)t)−1

= (R−1, −R−1(cid:126)t)

1.2 Data pipeline

The data pipeline is the ﬁrst step when running AlphaFold. It takes an mmCIF ﬁle (in the training mode) or a
FASTA ﬁle (in the inference mode) and produces input features for the model. In the training mode, a single
mmCIF ﬁle can produce multiple separate training examples, one for each chain in the ﬁle. The data pipeline
includes the following steps.

1.2.1 Parsing

The input ﬁle is parsed and basic metadata is extracted from it. For FASTA, this is only the sequence and
name; for mmCIF this is the sequence, atom coordinates, release date, name, and resolution. We also resolve
alternative locations for atoms/residues, taking the one with the largest occupancy, change MSE residues into
MET residues, and ﬁx arginine naming ambiguities (making sure that NH1 is always closer to CD than NH2).

1.2.2 Genetic search

Multiple genetic databases are searched using JackHMMER v3.3 [86] and HHBlits v3.0-beta.3 [87]. We used
JackHMMER with MGnify [88], JackHMMER with UniRef90 [89], HHBlits with Uniclust30 [90] + BFD
(see Input and Data Sources in the main text methods for details). The output multiple sequence alignments
(MSAs) are de-duplicated and stacked.

The MSA depth was limited to 5,000 sequences for JackHMMER with MGnify, 10,000 sequences for
JackHMMER with UniRef90 and unlimited for HHBlits. The following ﬂags were set to a non-default value
for each of the tools:

JackHMMER: -N 1 -E 0.0001 --incE 0.0001 --F1 0.0005 --F2 0.00005 --F3 0.0000005.
HHBlits: -n 3 -e 0.001 -realign_max 100000 -maxﬁlt 100000 -min_preﬁlter_hits 1000 -maxseq 1000000.

1.2.3 Template search

The structural templates fed to the model are retrieved with the following steps:

1. The UniRef90 MSA obtained in the previous step is used to search PDB70 using HHSearch [91]. The

only ﬂag set to a non-default value for HHSearch runs was -maxseq 1000000.

2. During training we exclude all templates that were released after the query training structure. We also
ﬁlter out templates that are identical to (or a subset of) the input primary sequence, or that are too small
(less than 10 residues or of length less than 10% of the primary sequence).

3. At inference time we provide the model the top 4 templates, sorted by the expected number of correctly
aligned residues (the “sum_probs” feature output by HHSearch). At training time we ﬁrst restrict the
available templates to up to 20 with the highest “sum_probs”. Then we choose random k templates out
of this restricted set of n templates, where k = min(Uniform[0, n], 4). This has the effect of showing
the network potentially bad templates, or no templates at all, during training so the network cannot rely
on just copying the template.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

6

1.2.4 Training data

With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25%
probability the training example is a known structure from the Protein Data Bank. We loop over this hybrid
set multiple times during training and we apply a number of stochastic ﬁlters (subsubsection 1.2.5), MSA pre-
processing steps (subsubsection 1.2.6 and subsubsection 1.2.7), and residue cropping (subsubsection 1.2.8)
every time when a protein is encountered. This means, we may observe different targets in training epochs,
with different samples of the MSA data, and also cropped to different regions.

1.2.5 Filtering

The following ﬁlters are applied to the training data:

• Input mmCIFs are restricted to have resolution less than 9 Å. This is not a very restrictive ﬁlter and only

removes around 0.2% of structures.

• Protein chains are accepted with probability 1

512 max(min(Nres, 512), 256), where Nres is the length of
the chain. This re-balances the length distribution and makes the network train on crops from the longer
chains more often.

• Sequences are ﬁltered out when any single amino acid accounts for more than 80% of the input primary

sequence. This ﬁlter removes about 0.8% of sequences.

• Protein chains are accepted with the probability inverse to the size of the cluster that this chain falls

into. We use 40% sequence identity clusters of the Protein Data Bank clustered with MMSeqs2 [92].

1.2.6 MSA block deletion

During training contiguous blocks of sequences are deleted from the MSA (see Algorithm 1). The MSA is
grouped by tool and ordered by the normal output of each tool, typically e-value. This means that similar
sequences are more likely to be adjacent in the MSA and block deletions are more likely to generate diversity
that removes whole branches of the phylogeny.

Algorithm 1 MSA block deletion
def MSABlockDeletion(msa) :
1: block_size = (cid:98)0.3 · Nall_seq(cid:99)
2: to_delete = {}
3: for all j ∈ [1, . . . , 5] do
4:
5:
6: end for
7: keep ← [1, . . . , Nall_seq] \ to_delete
8: msa ← msakeep
9: return msa

block_start ← uniform(1, Nall_seq)
to_delete ← to_delete∪ [block_start, . . . , block_start + block_size−1]

1.2.7 MSA clustering

seq × Nres, so it is highly
The computational and peak memory cost of the main Evoformer module scales as N 2
desirable to reduce the number of sequences used in the main Evoformer module for purely computational
reasons. Our ﬁrst version of this procedure randomly chose a ﬁxed-size subset of sequences without replace-
ment when the MSA was too large (originally 128 sequences). This procedure has the clear downside that

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

7

sequences not included in the random subset have no inﬂuence on the prediction. The presented version is a
modiﬁcation of this procedure where we still choose a random subset of sequences without replacement to be
representatives, but for each sequence in the full MSA we associate that sequence with the nearest sequence in
the set of representatives (we call this “clustering” with random cluster centres though no attempt is made to
ensure the cluster centres are well-distributed). To maintain the ﬁxed-size and bounded cost properties, so we
use only the amino acid and deletion frequencies of all sequences associated with each representative sequence
and provide these mini-proﬁles as extra features in addition to the representative sequence (we roughly double
the number of input features of each representative sequence without increasing the number of representative
sequences). This allows all sequences to have some inﬂuence on the ﬁnal prediction, which seems desirable.
Since this procedure achieves the goal of bounding computational cost, we have not experimented much
with alternatives to the procedure. We have attempted a couple of methods to encourage diversity among the
sampled sequences at inference time (e.g. a bias to pick representatives far from each other), but gains were
very small to none so we did not pursue them further.

In detail the MSA is clustered (grouped) using the following procedure:

1. Nclust sequences are selected randomly as MSA cluster centres, the ﬁrst cluster centre is always set to

the query amino acid sequence.

2. A mask is generated such that each position in a MSA cluster centre has a 15% probability of being
included in the mask. Each element in the MSA that is included in the mask is replaced in the following
way:

• With 10% probability amino acids are replaced with a uniformly sampled random amino acid.
• With 10% probability amino acids are replaced with an amino acid sampled from the MSA proﬁle

for a given position.

• With 10% probability amino acids are not replaced.
• With 70% probability amino acids are replaced with a special token (masked_msa_token).

These masked positions are the prediction targets used in subsubsection 1.9.9. Note that this masking
is used both at training time, and at inference time.

3. The remaining sequences are assigned to their closest cluster by Hamming distance (ignoring masked
out residues and gaps). For each sequence cluster, several statistics are computed, such as the per residue
amino acid distribution). See “cluster” features in Table 1 for a full description.

4. The MSA sequences that have not been selected as cluster centres at step 1 are used to randomly sample
Nextra_seq sequences without replacement. If there are less than Nextra_seq remaining sequences available,
all of them are used. These sequences form “extra” MSA features in Table 1.

Please note that throughout the rest of the manuscript we use the term “MSA clusters” to refer to the clusters

produced at step 3 above.

1.2.8 Residue cropping

During training the residue dimension in all data is cropped in the following way.

1. As described in subsubsection 1.11.5 mini-batches are processed in two modes. In the unclamped loss
mode the crop start position is sampled from Uniform[1, n − x + 1] where n is seq_length minus
crop_size and x is sampled from Uniform[0, n]. In clamped loss mode the crop position is sampled
from Uniform[1, n + 1].

2. The residues dimension is cropped to a single contiguous region, with the crop start position deﬁned

above. The ﬁnal crop size is denoted by Nres and its concrete value is provided in subsection 1.11.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

8

1.2.9 Featurization and model inputs

Features in Table 1 are computed and aggregated into the following main inputs to the model:

• target_feat This is a feature of size [Nres, 21] consisting of the “aatype” feature.
• residue_index This is a feature of size [Nres] consisting of the “residue_index” feature.
• msa_feat This is a feature of size [Nclust, Nres, 49] constructed by concatenating “cluster_msa”, “clus-
ter_has_deletion”, “cluster_deletion_value”, “cluster_deletion_mean”, “cluster_proﬁle”. We draw Ncycle×
Nensemble random samples from this feature to provide each recycling/ensembling iteration of the net-
work with a different sample (see subsubsection 1.11.2).

• extra_msa_feat This is a feature of size [Nextra_seq, Nres, 25] constructed by concatenating “extra_msa”,
“extra_msa_has_deletion”, “extra_msa_deletion_value”. Together with “msa_feat’ above we also draw
Ncycle × Nensemble random samples from this feature (see subsubsection 1.11.2).
• template_pair_feat This is a feature of size [Ntempl, Nres, Nres, 88] and consists of concatenation of the
pair residue features “template_distogram”, “template_unit_vector”, and also several residue features,
which are transformed into pair features. The “template_aatype” feature is included via tiling and stack-
ing (this is done twice, in both residue directions). Also the mask features “template_pseudo_beta_mask”
and “template_backbone_frame_mask” are included, where the feature fij = maski · maskj.
• template_angle_feat This is a feature of size [Ntempl, Nres, 51] constructed by concatenating the follow-
ing features: “template_aatype”, “template_torsion_angles”, “template_alt_torsion_angles”, and “tem-
plate_torsion_angles_mask”.

Input features to the model. Feature dimensions: Nres is the number of residues, Nclust

Table 1
is the
number of MSA clusters, Nextra_seq is the number of additional unclustered MSA sequences, and Ntempl is the
number of templates.

Feature & Shape

Description

aatype
[Nres, 21]
cluster_msa
[Nclust, Nres, 23]
cluster_has_deletion
[Nclust, Nres, 1]
cluster_deletion_value
[Nclust, Nres, 1]

cluster_deletion_mean
[Nclust, Nres, 1]

cluster_proﬁle
[Nclust, Nres, 23]

One-hot representation of the input amino acid sequence (20 amino
acids + unknown).
One-hot representation of the msa cluster centre sequences (20 amino
acids + unknown + gap + masked_msa_token).
A binary feature indicating if there is a deletion to the left of the residue
in the MSA cluster centres.
The raw deletion counts (the number of deletions to the left of every
position in the MSA cluster centres) are transformed to the range [0, 1]
using 2
The mean deletions for every residue in every cluster are computed as
i=1 dij where n is the number of sequences in the cluster and dij
1
n
is the number of deletions to the left of the ith sequence and jth residue.
These are then transformed into the range [0, 1] in the same way as for
the cluster_deletion_value feature above.
The distribution across amino acid types for each residue in each MSA
cluster (20 amino acids + unknown + gap + masked_msa_token).

3 where d are the raw counts.

π arctan d

(cid:80)n

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

9

Feature & Shape

Description

extra_msa
[Nextra_seq, Nres, 23]
extra_msa_has_deletion
[Nextra_seq, Nres, 1]
extra_msa_deletion_value
[Nextra_seq, Nres, 1]

template_aatype
[Ntempl, Nres, 22]
template_mask
[Ntempl, Nres]
template_pseudo_beta_mask
[Ntempl, Nres]
template_backbone_frame_mask
[Ntempl, Nres]

template_distogram
[Ntempl, Nres, Nres, 39]

template_unit_vector
[Ntempl, Nres, Nres, 3]

template_torsion_angles
[Ntempl, Nres, 14]
template_alt_torsion_angles
[Ntempl, Nres, 14]
template_torsion_angles_mask
[Ntempl, Nres, 14]
residue_index
[Nres]

One-hot representation of all MSA sequences not selected as cluster
centres (20 amino acids + unknown + gap + masked_msa_token).
A binary feature indicating if there is a deletion to the left of the residue
in the extra MSA sequences.
The raw deletion counts to the left of every residue in the extra_msa,
converted to the range [0, 1] using the same formula as for clus-
ter_deletion_value.
One-hot representation of the amino acid sequence (20 amino acids +
unknown and gap).
Mask indicating if a template residue exists and has coordinates.

Mask indicating if the beta carbon (alpha carbon for glycine) atom has
coordinates for the template at this residue.
A mask indicating if the coordinates of all the required atoms to com-
pute the backbone frame (used in the template_unit_vector feature) ex-
ist.
A one-hot pairwise feature indicating the distance between beta car-
bons (alpha carbon used for glycine) atoms. The pairwise distances are
discretized into 38 bins of equal width between 3.25 Å and 50.75 Å;
and one more bin contains any larger distances.
The unit vector of the displacement of the alpha carbon atom of all
residues within the local frame of each residue. These local frames are
computed in the same way as for the target structure, see subsubsec-
tion 1.8.1.
The 3 backbone torsion angles and up to 4 side-chain torsion angles for
each residue represented as sine and cosine encoding.
Alternative torsion angles for side chain parts with 180o-rotation sym-
metry.
A mask indicating if the torsion angle is present in the template struc-
ture.
The index into the original amino acid sequence.

1.3 Self-distillation dataset

We perform a self-distillation procedure similar to [93] on unlabelled protein sequences. To create the se-
quence dataset we compute multiple sequence alignments of every cluster in Uniclust30 (version 2018-08)
against the same database. We then greedily remove sequences which appear in another sequence’s MSA to
give 6,318,986 sequences in total. We further ﬁlter this by removing sequences with more than 1,024 or fewer

(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)pi,j(r)
(cid:17)

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

10

than 200 amino acids, or whose MSA contain fewer than 200 alignments. This gives a ﬁnal dataset of 355,993
sequences.

For building the distillation set predicted structures, we use a single (no re-ranking) “undistilled” model,
i.e. trained on just the PDB dataset. Using this undistilled model, we predict the structure of every sequence
in the set constructed above to create a dataset of structures to use at training time. For every pair of predicted
residues we calculate a conﬁdence metric by computing the Kullback–Leibler divergence between the pairwise
distance distribution and a reference distribution for that residue separation:

cij = DKL

pref|i−j|(r)

.

(1)

pled sequences in Uniclust30 and computing the mean distribution for a given sequence separation.

The reference distribution is computed by taking distance distribution predictions for 1,000 randomly sam-
This pairwise metric cij is then averaged over j corresponding to a maximum sequence separation of ±128
and excluding i = j to give a per-residue conﬁdence metric ci. Similar to pLDDT, we ﬁnd that higher values
of this conﬁdence correlate with higher prediction accuracy. At training time we mask out residues with
ci < 0.5. This KL-based metric was introduced into the model before we developed pLDDT, and we expect
that ﬁltering the distillation set based on pLDDT would work equally well or better.

At training time, extra augmentation is added to distillation dataset examples by uniformly sampling the
MSA to 1000 sequences without replacement (this is on top of any sampling that happens in the data pipeline,
see subsection 1.2).

1.4 AlphaFold Inference

For inference, the AlphaFold receives input features derived from the amino-acid sequence, MSA, and tem-
plates (see subsubsection 1.2.9) and outputs features including atom coordinates, the distogram, and per-
residue conﬁdence scores. Algorithm 2 outlines the main steps (see also Fig 1e and the corresponding de-
scription in the main article).

The whole network is executed sequentially Ncycle times, where the outputs of the former execution are
recycled as inputs for the next execution (see subsection 1.10 for details). The ﬁrst part of the network (feature
embedding and Evoformer, see Suppl. Fig. 1) is executed independently Nensemble times with differently
sampled inputs (see subsubsection 1.11.2). Their outputs are averaged to create the inputs for the Structure
module and for recycling. These averaged Evoformer embeddings are denoted with an extra hat, e.g. ˆzij for
pair representations and ˆsi for single representations. The Ncycle × Nensemble random samples of “msa_feat”
. Please note that
and “extra_msa_feat” are denoted as a set of sets
ensembling is used only during inference and it has a very minor impact on the accuracy in the ﬁnal system
(subsubsection 1.12.2), thus can be considered as an optional technique.
The ﬁrst part of the network (Algorithm 2 line 5) starts with embedding a new sample from the MSA
(see subsection 1.5 for details) to create the initial version of the MSA representation {msi}, and the pair
representation {zij}. The ﬁrst row of the MSA representation and the full pair representation are updated
(Algorithm 2 line 6) by the recycled outputs from the previous iteration (see subsection 1.10 for details) – for
the ﬁrst iteration the recycled outputs are initialized to zero.

(cid:110){f extra_msa_feat

(cid:110){f msa_feat

(cid:111)

(cid:111)

, and

}

}

sei

sci

c,n

c,n

The next steps integrate the information from the templates (Algorithm 2 line 7). The “template_angle_feat”
(derived from amino acid types and the torsion angles) are embedded by a shallow MLP and concatenated
to the MSA representation. The “template_pair_feat” (derived from residue pairs, see subsubsection 1.2.9)
are embedded by a shallow attention network (see subsubsection 1.7.1 for details) and added to the pair
representation.

The ﬁnal step of the embedding procedure (Algorithm 2 line 14) processes the extra MSA features (derived
from individual unclustered MSA sequences) by shallow Evoformer-like network that is optimized for the
large number of sequences (see subsubsection 1.7.2) to update the pair representation.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

11

Supplementary Figure 1
Input feature embeddings. Dimension names: r: residues, f: features, c: chan-
nels, sc: clustered MSA sequences, se: extra MSA sequences, st: template sequences. Operator relpos is
deﬁned in Algorithm 4. Template embedding and extra MSA stack are explained in subsection 1.7. Recycling
injection is denoted with R, see subsection 1.10 for details.

The main trunk of the network is the Evoformer stack (subsection 1.6) that produces the ﬁnal represen-
tations (Algorithm 2 line 17). The pair representation, the ﬁrst row of the MSA representation and a single
representation derived from that row are kept for recycling and the structure module.
as input and predicts the atom coordinates {(cid:126)xa
i } and the per-residue conﬁdence {rpLDDT

The second part of the network, the Structure Module (Algorithm 2 line 21), takes the ﬁnal representations
} (see subsection 1.8).

i

target_feat (r, f) msa_feat(sc, r, fc)Linear f → cmLinear fc→ cmtile → (sc, r, cm)+Linear f → czLinear f → czouter sumpair representation(r, r, cz)extra_msa_feat(se, r, fe)template_angle_feat(st, r, fa)template_pair_feat(st, r, r, fp)Linear fa→ cmreluLinear cm→ cm concatMSA representation (sc, r, cm)embed → (r, r, cz)+Linear fe→ ceextra MSArepresentation(se, r, ce)Extra MSA StackMain EvoformerStackRRresidue_index (r) +relpos → (r, r, cz)Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

12

Algorithm 2 AlphaFold Model Inference
},{f residue_index
def Inference

},

i

(cid:16){f target_feat

i

{f template_angle_feat

sti

},{f template_pair_feat

stij

(cid:110){f msa_feat

sci

(cid:111)

(cid:110){f extra_msa_feat

,

c,n

}

}
}, Ncycle = 4, Nensemble = 3

sei

(cid:17)

c,n

(cid:111)

:

,

, ˆzprev
ij

# Recycling iterations:
1: ˆmprev
, (cid:126)xprev,Cβ
= 0, 0, (cid:126)0
1i
2: for all c ∈ [1, . . . , Ncycle] do
#

i

Average embeddings in ensemble:
{ ˆm1i},{ˆzij},{ˆsi} = 0, 0, 0
for all n ∈ [1, . . . , Nensemble] do

# shared weights

# shared weights

3:

4:
#

5:

#

6:

#

7:

8:

9:

10:
11:

12:

13:

#

14:

15:

16:

#

17:

18:

},{f msa_feat

i

i

sci

},{f residue_index

Embed clustered MSA (use different MSA samples in each iteration):
{msci},{zij} = InputEmbedder({f target_feat
Inject previous outputs for recycling:
{m1i},{zij} += RecyclingEmbedder({ ˆmprev
Embed templates:
asti ← Linear(relu(Linear(f template_angle_feat
msi = concats(msci, asti)
tstij = Linear(f template_pair_feat
for all st ∈ [1, . . . , Ntempl] do

ij },{(cid:126)xprev,Cβ

1i },{ˆzprev

# shared weights

)))

stij

sti

)

i

})

{tstij} ← TemplatePairStack({tstij})

sei

}

end for
{zij} += TemplatePointwiseAttention({tstij},{zij})
Embed extra MSA features (use different samples in each iteration):
{asei} ← {f extra_msa_feat
esei = Linear(asei)
{zij} ← ExtraMsaStack({esei},{zij})
Main trunk of the network:
{msi},{zij},{si} ← EvoformerStack({msi},{zij})
{ ˆm1i},{ˆzij},{ˆsi} += {m1i},{zij},{si}

c,n

}

c,n

)

asti ∈ Rcm, cm = 256

tstij ∈ Rct, ct = 64

esei ∈ Rce, ce = 64

#

20:

19:

end for
{ ˆm1i},{ˆzij},{ˆsi} /= Nensemble
Structure and conﬁdence prediction:
{(cid:126)xa
{ ˆmprev
22:
23: end for
24: return {(cid:126)xa

i },{rpLDDT
1i },{ˆzprev

ij },{(cid:126)xprev,Cβ

i },{rpLDDT

21:

}

i

i

i

} = StructureModule({ˆsi},{ˆzij})
} ← { ˆm1i},{ˆzij},{(cid:126)xCβ
i }

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

13

1.5 Input embeddings

Input features listed in subsubsection 1.2.9 are embedded into the network according to the Suppl. Fig. 1.
Initial embedding details are also presented in Algorithm 3.
Input primary sequence and MSA features
are transformed to form MSA representation {msi} with msi ∈ Rcm, cm = 256, and s ∈ {1 . . . Nseq},
i ∈ {1 . . . Nres}. Primary sequence features are transformed to form pair representation {zij} with zij ∈ Rcz,
cz = 128, and i, j ∈ {1 . . . Nres}. These two representations are also informed by the templates and unclus-
tered MSA features. The details of their corresponding embedding modules are provided in subsection 1.7.

},{f msa_feat

sci

i

i

},{f residue_index

Algorithm 3 Embeddings for initial representations
def InputEmbedder({f target_feat
1: ai, bi = Linear(f target_feat
2: zij = ai + bj
3: {zij} += relpos({f residue_index
4: msci = Linear(f msa_feat
5: return {msci},{zij}

) + Linear(f target_feat

})

sci

)

)

i

i

i

}) :

ai, bi ∈ Rcz , cz = 128
zij ∈ Rcz , cz = 128

msci ∈ Rcm, cm = 256

To provide the network with information about the positions of residues in the chain, we also encode
relative positional features (Algorithm 4) into the initial pair representations. Speciﬁcally, for a residue pair
i, j ∈ {1 . . . Nres} we compute the clipped relative distance within a chain, encode it as a one-hot vector,
and add this vector’s linear projection to zij. Since we are clipping by the maximum value 32, any larger
distances within the residue chain will not be distinguished by this feature. This inductive bias de-emphasizes
primary sequence distances. Compared to the more traditional approach of encoding positions in the frequency
space [94], this relative encoding scheme empirically allows the network to be evaluated without quality
degradation on much longer sequences than it was trained on. A related construction was used in [95].

i

Algorithm 4 Relative position encoding
def relpos({f residue_index
}, vbins = [−32,−31, . . . , 32]) :
− f residue_index
1: dij = f residue_index
2: pij = Linear(one_hot(dij, vbins))
3: return {pij}

j

i

Algorithm 5 One-hot encoding with nearest bin
def one_hot(x, vbins) :
1: p = 0
2: b = arg min(|x − vbins|)
3: pb = 1
4: return p

dij ∈ Z
pij ∈ Rcz

x ∈ R, vbins ∈ RNbins
p ∈ RNbins

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

14

1.6 Evoformer blocks

The network has a two tower architecture with axial self-attention in the MSA stack; triangular multiplicative
updates and triangular self-attention in the pair stack; and outer product mean and attention biasing to allow
communication between the stacks.
The main trunk of the network consists of Nblock = 48 Evoformer blocks (Fig. 1e, 3a, Algorithm 6). Each
block has an MSA representation {msi} and a pair representation {zij} as its input and output and processes
them with several layers. Each layer output is added via a residual connection to the current representations.
Some layer outputs are passed through Dropout [96] before they are added (see subsubsection 1.11.6 for
details).
The ﬁnal Evoformer block provides a highly processed MSA representation {msi} and a pair representa-
tion {zij}, which contain information required for the structure module (subsection 1.8) and auxiliary network
heads (subsection 1.9). The prediction modules are also using a “single” sequence representation {si} with
si ∈ Rcs, cs = 384 and i ∈ {1 . . . Nres}. This single representation is derived by a linear projection of the
ﬁrst row of the MSA representation.

We now present details of the individual layers.

Algorithm 6 Evoformer stack
def EvoformerStack({msi},{zij}, Nblock = 48, cs = 384) :
1: for all l ∈ [1, . . . , Nblock] do
# MSA stack

2:

3:

4:

#

5:

#

6:

7:

8:

9:

{msi} += DropoutRowwise0.15(MSARowAttentionWithPairBias({msi},{zij}))
{msi} += MSAColumnAttention({msi})
{msi} += MSATransition({msi})
Communication
{zij} += OuterProductMean({msi})
Pair stack
{zij} += DropoutRowwise0.25(TriangleMultiplicationOutgoing({zij}))
{zij} += DropoutRowwise0.25(TriangleMultiplicationIncoming({zij}))
{zij} += DropoutRowwise0.25(TriangleAttentionStartingNode({zij}))
{zij} += DropoutColumnwise0.25(TriangleAttentionEndingNode({zij}))
{zij} += PairTransition({zij})

10:
11: end for

# Extract the single representation

12: si = Linear(m1i)
13: return {msi}, {zij}, {si}

si ∈ Rcs

1.6.1 MSA row-wise gated self-attention with pair bias

MSA representations are processed with consecutive blocks of gated row-wise and column-wise self-attention
blocks. The row-wise version (Suppl. Fig. 2 and Algorithm 7) builds attention weights for residue pairs
and integrates the information from the pair representation as an additional bias term (line 3). This allows
communication from the pair stack into the MSA stack to encourage consistency between them.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

15

Supplementary Figure 2 MSA row-wise gated self-attention with pair bias. Dimensions: s: sequences, r:
residues, c: channels, h: heads.

Algorithm 7 MSA row-wise gated self-attention with pair bias
def MSARowAttentionWithPairBias({msi},{zij}, c = 32, Nhead = 8) :
# Input projections
1: msi ← LayerNorm(msi)
2: qh
3: bh
4: gh
# Attention
5: ah

si = sigmoid(cid:0)Linear(msi)(cid:1)

si, kh
ij = LinearNoBias(LayerNorm(zij))

si = LinearNoBias(msi)

(cid:16) 1√

si, vh

(cid:17)

si, kh
qh

kh
sj + bh
ij

(cid:62)
si

c qh
sijvh
sj

sij = softmaxj
si = gh

j ah
6: oh
# Output projection

si (cid:12)(cid:80)
(cid:16)

7: ˜msi = Linear
8: return { ˜msi}

concath(oh
si)

(cid:17)

si ∈ Rc, h ∈ {1, . . . , Nhead}

si, vh

si ∈ Rc
gh

˜msi ∈ Rcm

1.6.2 MSA column-wise gated self-attention

The column-wise attention (and Suppl. Fig. 3 and Algorithm 8) lets the elements that belong to the same
target residue exchange information. In both attention blocks the number of heads Nheads = 8, and dimension
of the keys, queries, and values c = 32.

dot-product affinities(rq,rv,h)queries (rq,h,c) keys (rv,h,c) output (rq,h,c) values (rv,h,c) attention weights(rq,rv,h)pair representation(r,r,cz)+Linear cm→(h,c)softmax rvpair bias(rq,rv,h)Linear cz→hLinear cm→(h,c)Linear cm→(h,c)Linear (h,c)→cmMSA repr. (s,r,cm)input (r,cm)MSA repr. (s,r,cm)updates (r,cm)Linear cm→(h,c)sigmoidgating (rq,h,c) Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

16

Supplementary Figure 3 MSA column-wise gated self-attention. Dimensions: s: sequences, r: residues,
c: channels, h: heads.

Algorithm 8 MSA column-wise gated self-attention
def MSAColumnAttention({msi}, c = 32, Nhead = 8) :
# Input projections
1: msi ← LayerNorm(msi)
2: qh
3: gh

si = sigmoid(cid:0)Linear(msi)(cid:1)
(cid:17)

si = LinearNoBias(msi)

si, vh

si, kh

# Attention
4: ah

(cid:16) 1√

sti = softmaxt

(cid:62)
si

kh
ti

c qh
stivh
st

si (cid:12)(cid:80)
(cid:16)

5: oh

si = gh

t ah
# Output projection

6: ˜msi = Linear
7: return { ˜msi}

concath(oh
si)

(cid:17)

qh
si, kh

si, vh

si ∈ Rc, h ∈ {1, . . . , Nhead}
si ∈ Rc
gh

˜msi ∈ Rcm

1.6.3 MSA transition

After row-wise and column-wise attention the MSA stack contains a 2-layer MLP as the transition layer
(Suppl. Fig. 4 and Algorithm 9). The intermediate number of channels expands the original number of
channels by a factor of 4.

Supplementary Figure 4 MSA transition layer. Dimensions: s: sequences, r: residues, c: channels.

MSA repr.(s,r,cm)MSA repr.(s,r,cm)Linear cm→(h,c)Linear cm→(h,c)Linear cm→(h,c)Linear cm→(h,c)sigmoidsoftmax svinput (s,cm)Linear (h,c)→cmupdate (s,cm)que. (sq,h,c) keys (sv,h,c) dot-prod. affinities(sq,sv,h)out (sq,h,c) val. (sv,h,c) gat. (sq,h,c) attention weights(sq,sv,h)Linear cm→ 4cmreluLinear 4cm→ cmMSA representation (s,r,cm)MSA representation (s,r,cm)Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

17

Algorithm 9 Transition layer in the MSA stack
def MSATransition({msi}, n = 4) :
1: msi ← LayerNorm(msi)
2: asi = Linear(msi)
3: msi ← Linear(relu(asi))
4: return {msi}

1.6.4 Outer product mean

asi ∈ Rn·cm

The “Outer product mean” block transforms the MSA representation into an update for the pair representation
(Suppl. Fig. 5 and Algorithm 10). All MSA entries are linearly projected to a smaller dimension c = 32 with
two independent Linear transforms. The outer products of these vectors from two columns i and j are averaged
over the sequences and projected to dimension cz to obtain an update for entry ij in the pair representation.
This is a memory intensive operation, as it requires constructing high-dimensional intermediate tensors. See
section 1.11.8 for implementation details.

Supplementary Figure 5 Outer product mean. Dimensions: s: sequences, r: residues, c: channels.

Algorithm 10 Outer product mean
def OuterProductMean({msi}, c = 32) :
1: msi ← LayerNorm(msi)
2: asi, bsi = Linear(msi)

3: oij = ﬂatten(cid:0)means(asi ⊗ bsj)(cid:1)

4: zij = Linear(oij)
5: return {zij}

asi, bsi ∈ Rc
oij ∈ Rc·c
zij ∈ Rcz

1.6.5 Triangular multiplicative update
The triangular multiplicative update (Fig. 3c) updates the pair representation in the Evoformer block by
combining information within each triangle of graph edges ij, ik, and jk. Each edge ij receives an update
from the other two edges of all triangles, where it is involved. There are two symmetric versions, one for the
“outgoing” edges (Suppl. Fig. 6 and Algorithm 11) and one for the “incoming” edges (Algorithm 12). The
differences are highlighted in yellow.

input (s,cm)input (s,cm)Linear cm→cLinear cm→cijouter product prods (s,c,c)mean sLinear (c,c)→ cz pair repr. (r, r, cz)jiMSA repr. (s,r,cm)Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

18

Supplementary Figure 6 Triangular multiplicative update using “outgoing” edges. Dimensions: r: residues,
c: channels.

Algorithm 11 Triangular multiplicative update using “outgoing” edges
def TriangleMultiplicationOutgoing({zij}, c = 128) :
1: zij ← LayerNorm(zij)

2: aij, bij = sigmoid(cid:0)Linear(zij)(cid:1) (cid:12) Linear(zij)
3: gij = sigmoid(cid:0)Linear(zij)(cid:1)
4: ˜zij = gij (cid:12) Linear(LayerNorm((cid:80)

k aik (cid:12) bjk))

5: return {˜zij}

Algorithm 12 Triangular multiplicative update using “incoming” edges
def TriangleMultiplicationIncoming({zij}, c = 128) :
1: zij ← LayerNorm(zij)

2: aij, bij = sigmoid(cid:0)Linear(zij)(cid:1) (cid:12) Linear(zij)
3: gij = sigmoid(cid:0)Linear(zij)(cid:1)
4: ˜zij = gij (cid:12) Linear(LayerNorm((cid:80)

k aki (cid:12) bkj ))

5: return {˜zij}

aij, bij ∈ Rc
gij ∈ Rcz
˜zij ∈ Rcz

aij, bij ∈ Rc
gij ∈ Rcz
˜zij ∈ Rcz

1.6.6 Triangular self-attention
The triangular self-attention (Fig. 3c) updates the pair representation in the Evoformer block. The “starting
node” version (Suppl. Fig. 7 and Algorithm 13) updates the edge ij with values from all edges that share the
same starting node i, (i.e. all edges ik). The decision whether edge ij will receive an update from edge ik is
not only determined by their query-key similarity (as in standard attention [94]), but also modulated by the
information bjk derived from the third edge jk of this triangle. Furthermore we extend the update with an
additional gating gij derived from edge ij. The symmetric pair of this module operates on the edges around
the ending node (Algorithm 14). The differences are highlighted in yellow.

pair repr. (r,r,cz)Linear cz→cijpair repr. (r,r,cz)jileft edges (r,c)Linear cz→cright edges (r,c)Linear cz→csigmoidsum rLinear c→cz Linear cz→czsigmoidjLayerNormLinear cz→csigmoidSuppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

19

Supplementary Figure 7 Triangular self-attention around starting node. Dimensions: r: residues, c: chan-
nels, h: heads

Algorithm 13 Triangular gated self-attention around starting node
def TriangleAttentionStartingNode({zij}, c = 32, Nhead = 4) :
# Input projections
1: zij ← LayerNorm(zij)
2: qh

ij = LinearNoBias(zij)

ij, vh

ij, kh
ij = LinearNoBias(zij)

ij = sigmoid(cid:0)Linear(zij)(cid:1)

3: bh

4: gh

(cid:16) 1√

ijkvh
ik

(cid:17)

concath(oh
ij)

# Attention
5: ah

ijk = softmaxk
ij = gh

k ah
# Output projection

ij (cid:12)(cid:80)
(cid:16)

6: oh

7: ˜zij = Linear
8: return {˜zij}

(cid:62)
ij

c qh

ik + bh
kh
jk

qh
ij, kh

ij, vh

ij ∈ Rc, h ∈ {1, . . . , Nhead}

(cid:17)

ij ∈ Rc
gh

˜zij ∈ Rcz

dot-product affinities(rq,rv,h)central edges (rq,h,c) left edges (rv,h,c) output (rq,h,c) left edges (rv,h,c) attention weights(rq,rv,h)pair representation(r,r,cz)+Linear cz→(h,c)softmax rvright edges(rq,rv,h)Linear cz→hLinear cz→(h,c)Linear cz→(h,c)Linear (h,c)→czinput row (r,cz)Linear cz→(h,c)sigmoidcentral edges  (rq,h,c) pair representation(r,r,cz)updates (r,cz)Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

20

Algorithm 14 Triangular gated self-attention around ending node
def TriangleAttentionEndingNode({zij}, c = 32, Nhead = 4) :
# Input projections
1: zij ← LayerNorm(zij)
2: qh

ij = LinearNoBias(zij)

ij, vh

ij, kh
ij = LinearNoBias(zij)

ij = sigmoid(cid:0)Linear(zij)(cid:1)

3: bh

4: gh

(cid:18)

# Attention

5: ah

ijk = softmaxk

1√
c qh

(cid:62)
ij

kh
kj + bh
ki

ij (cid:12)(cid:80)
(cid:18)

6: oh

ij = gh

k ah

ijk vh
kj

# Output projection

7: ˜zij = Linear
8: return {˜zij}

concath

(cid:16)

(cid:17)(cid:19)

oh
ij

qh
ij, kh

ij, vh

ij ∈ Rc, h ∈ {1, . . . , Nhead}

(cid:19)

ij ∈ Rc
gh

˜zij ∈ Rcz

1.6.7 Transition in the pair stack

The transition layer in the pair stack (Algorithm 15) is equivalent to that in the MSA stack: a 2-layer MLP
where the intermediate number of channels expands the original number of channels by a factor of 4.

Algorithm 15 Transition layer in the pair stack
def PairTransition({zij}, n = 4) :
1: zij ← LayerNorm(zij)
2: aij = Linear(zij)
3: zij ← Linear(relu(aij))
4: return {zij}

1.7 Additional inputs

aij ∈ Rn·cz

As outlined in Suppl. Fig. 1, the additional model inputs include templates and unclustered MSA sequences.
In this section we explain how they are embedded in more detail.

1.7.1 Template stack
Pairwise template features are linearly projected to form initial template representations tstij, with tstij ∈
Rct, ct = 64, and i, j ∈ {1 . . . Nres}, st ∈ {1 . . . Ntempl}. Each template representation is independently
processed with the template pair stack (Algorithm 16) with all trainable parameters shared across templates.
The output representations are aggregated with the template point-wise attention (Algorithm 17), where the
pair representations {zij} are used to form the queries and attend over the individual templates. The outputs
of this module are added to the pair representations (see Suppl. Fig. 1 or line 13 in Algorithm 2).

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

21

Furthermore, template torsion angle features are embedded with a small MLP and concatenated with the
MSA representations as additional sequence rows (see Suppl. Fig. 1 or lines 7-8 in Algorithm 2). These
additional rows are participating in all MSA stack operations, but not in the masked MSA loss (subsubsec-
tion 1.9.9). While template torsion angle and MSA features are conceptually different quantities, they are
embedded with different sets of weights, thus the learning process presumably drives the embeddings to be
comparable since they are processed by the same downstream modules with the same weights.

Algorithm 16 Template pair stack
def TemplatePairStack({tij}, Nblock = 2) :
1: for all l ∈ [1, . . . , Nblock] do
2:

3:

4:

5:

{tij} += DropoutRowwise0.25(TriangleAttentionStartingNode({tij}, c = 64, Nhead = 4))
{tij} += DropoutColumnwise0.25(TriangleAttentionEndingNode({tij}, c = 64, Nhead = 4))
{tij} += DropoutRowwise0.25(TriangleMultiplicationOutgoing({tij}, c = 64))
{tij} += DropoutRowwise0.25(TriangleMultiplicationIncoming({tij}, c = 64))
{tij} += PairTransition({tij}, n = 2)

6:
7: end for

8: return LayerNorm(cid:0){tij}(cid:1)

Algorithm 17 Template pointwise attention
def TemplatePointwiseAttention({tstij},{zij}, c = 64, Nhead = 4) :
1: qh

ij = LinearNoBias(zij)
stij, vh

stij = LinearNoBias(tstij)

2: kh

(cid:16) 1√

(cid:62)
ij

c qh

kh

stij

concath({oh

stij

(cid:17)
(cid:17)
ij})

3: ah

ij =(cid:80)

stij = softmaxst
4: oh
ah
stijvh
5: {˜zij} = Linear
6: return {˜zij}

(cid:16)

st

ij ∈ Rc, h ∈ {1, . . . , Nhead}
qh
stij ∈ Rc

kh
stij, vh

˜zij ∈ Rcz

1.7.2 Unclustered MSA stack
Unclustered MSA sequence features are linearly projected to form initial representations {esei} with esei ∈
Rce, ce = 64, and se ∈ {1 . . . Nextra_seq}, i ∈ {1 . . . Nres}. These representations are processed with the Extra
MSA stack containing 4 blocks (Algorithm 18). They are highly similar to the main Evoformer blocks, with
the notable difference of using global column-wise self-attention (Algorithm 19) and smaller representation
sizes to allow processing of the large number of sequences. The ﬁnal pair representations are used as inputs
for the main Evoformer stack (Algorithm 6), while the ﬁnal MSA activations are unused (see Suppl. Fig. 1).

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

22

Algorithm 18 Extra MSA stack
def ExtraMsaStack({esei},{zij}, Nblock = 4) :
1: for all l ∈ [1, . . . , Nblock] do
# MSA stack

2:

3:

4:

#

5:

#

6:

7:

8:

9:

{esei} += DropoutRowwise0.15(MSARowAttentionWithPairBias({esei},{zij}, c = 8 ))
{esei} += MSAColumn Global Attention({esei}
{esei} += MSATransition({esei})
Communication
{zij} += OuterProductMean({esei})
Pair stack
{zij} += DropoutRowwise0.25(TriangleMultiplicationOutgoing({zij}))
{zij} += DropoutRowwise0.25(TriangleMultiplicationIncoming({zij}))
{zij} += DropoutRowwise0.25(TriangleAttentionStartingNode({zij}))
{zij} += DropoutColumnwise0.25(TriangleAttentionEndingNode({zij}))
{zij} += PairTransition({zij})

10:
11: end for
12: return {zij}

Algorithm 19 MSA global column-wise gated self-attention
def MSAColumnGlobalAttention({msi}, c = 8, Nhead = 8) :
# Input projections
1: msi ← LayerNorm(msi)
2: qh

si, ksi, vsi = LinearNoBias(msi)

4: gh

3: qh

i = means qh
si

si = sigmoid(cid:0)Linear(msi)(cid:1)
(cid:17)

# Attention
5: ah

ti = softmaxt

kti

(cid:16) 1√
si (cid:12)(cid:80)
(cid:18)

i

(cid:62)
c qh
tivti

6: oh

si = gh

t ah
# Output projection

7: ˜msi = Linear
8: return { ˜msi}

concath

(cid:17)(cid:19)

(cid:16)

oh
si

si, ksi, vsi ∈ Rc, h ∈ {1, . . . , Nhead}
qh

si ∈ Rc
gh

˜msi ∈ Rcm

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

23

1.8 Structure module
The Structure Module (Algorithm 20 and Fig. 3d in the main article) maps the abstract representation of
the protein structure (created by the Evoformer stack) to concrete 3D atom coordinates. Evoformer’s single
representation is used as the initial single representation {sinitial
∈ Rcs and Evoformer’s pair
representation {zij} with zij ∈ Rcz and i, j ∈ {1, ..., Nres} biases the afﬁnity maps in the attention operations.
The module has 8 layers with shared weights. Each layer updates the abstract single representation {si} as
well the concrete 3D representation (the “residue gas”) which is encoded as one backbone frame per residue
{Ti}. We represent frames via a tuple Ti := (Ri,(cid:126)ti). The tuple represents an Euclidean transform from the
local frame to a global reference frame. I.e. it transforms a position in local coordinates (cid:126)xlocal ∈ R3 to a
position in global coordinates (cid:126)xglobal ∈ R3 as

} with sinitial

i

i

(cid:126)xglobal = Ti ◦ (cid:126)xlocal

= Ri(cid:126)xlocal + (cid:126)ti .

(2)

The backbone frames are initialized to an identity transform (Algorithm 20 line 4). We call this approach
the ‘black hole initialization’. This initialization means that at the start of the Structure module all residues
are located at the same point (the origin of the global frame) with the same orientation. One “layer” of the
structure module is composed by the following operations: First, the abstract single representation is updated
by the Invariant Point Attention (Algorithm 20 line 6) (see subsubsection 1.8.2 for details) and a transition
layer. Then the abstract single representation is mapped to concrete update frames that are composed to the
backbone frames (Algorithm 20 line 10). The composition of two Euclidean transforms is denoted as

Tresult = T1 ◦ T2

In the parameterization with a rotation matrix and translation vector this is:

(Rresult,(cid:126)tresult) = (R1,(cid:126)t1) ◦ (R2,(cid:126)t2)
= (R1R2, R1(cid:126)t2 + (cid:126)t1)

To obtain all atom coordinates, we parameterize each residue by torsion angles. I.e., the torsion angles are
the only degrees of freedom, while all bond angles and bond lengths are fully rigid. The individual atoms
are identiﬁed by their names Satom names = {N, Cα, C, O, Cβ, Cγ, Cγ1, Cγ2, . . .}. The torsions are named as
Storsion names = {ω, φ, ψ, χ1, χ2, χ3, χ4} We group the atoms according to their dependence on the torsion
angles into “rigid groups” (Table 2). E.g., the position of atoms in the χ2-group depend on χ1 and χ2, but
do not depend on χ3 or χ4. We deﬁne a rigid group for every of the 3 backbone and 4 side chain torsion
angles ω, φ, ψ, χ1, χ2, χ3, χ4, though with our construction of the backbone frames, no heavy atoms positions
depend on the torsion angles ω or φ. The χ5 angle is not included, because it only appears in arginine and
only slightly varies around 0 degrees. Atoms that just depend on the backbone frame are in the “backbone
rigid group”, denoted as “bb”.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

24

Table 2 Rigid groups for constructing all atoms from given torsion angles. Boxes highlight groups that are
symmetric under 180◦ rotations.

aatype
ALA
ARG
ASN
ASP
CYS
GLN
GLU
GLY
HIS
ILE
LEU
LYS
MET
PHE
PRO
SER
THR
TRP
TYR
VAL

bb

N, Cα, C

χ1
-
Cγ
Cγ
Cγ
Sγ
Cγ
Cγ
-
Cγ

ψ
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
O
N, Cα, C, Cβ O
N, Cα, C, Cβ O Cγ1, Cγ2
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O Cγ2, Oγ1
N, Cα, C, Cβ O
N, Cα, C, Cβ O
N, Cα, C, Cβ O Cγ1, Cγ2

Cγ
Cγ
Cγ
Cγ
Cγ
Oγ

Cγ
Cγ

χ2
-
Cδ

Nδ2, Oδ1
Oδ1, Oδ2

-
Cδ
Cδ
-

Cδ
Sδ

Cδ2, Nδ1, C1, N2

Cδ1

Cδ1, Cδ2

Cδ1, Cδ2, C1, C2, Cζ

Cδ
-
-

Cδ1, Cδ2, C2, C3, N1, Cη2, Cζ2, Cζ3

Cδ1, Cδ2, C1, C2, Oη, Cζ

-

χ3
-
N
-
-
-

N2, O1
O1, O2

-
-
-
-
C
C
-
-
-
-
-
-
-

Nη1, Nη2, Cζ

χ4
-

-
-
-
-
-
-
-
-
-
Nζ
-
-
-
-
-
-
-
-

A shallow ResNet (Algorithm 20 line 11) predicts the torsion angles (cid:126)αf

i ∈ R2. They get mapped to points on
the unit circle via normalization whenever they are used as angles. Furthermore we introduce a small auxiliary
loss (implemented in Algorithm 27) that encourages a unit norm of the raw vector to avoid degenerate values.
In contrast to a [0, 2π] angle representation this representation has no discontinuity and can be directly used
to construct rotation matrices without the need for trigonometric functions. The predicted torsion angles are
converted to frames for the rigid groups of atoms (see subsubsection 1.8.4 for details).

During training, the last step of each layer computes auxiliary losses for the current 3D structure (Algo-
rithm 20 line 17). The intermediate FAPE loss operates only on the backbone frames and Cα atom positions to
keep the computational costs low. For the same reason the side chain are here only supervised by their torsion
angles. The 180◦ rotation symmetry of some rigid groups (highlighted by boxes in Table 2) is addressed by
providing the alternative angle (cid:126)αalt truth,f

as well (see subsubsection 1.9.1 for details).

We found it helpful to zero the gradients into the orientation component of the rigid bodies between itera-
tions (Algorithm 20 line 20), so any iteration is optimized to ﬁnd an optimal orientation for the structure in
the current iteration, but is not concerned by having an orientation more suitable for the next iteration. Em-
pirically, this improves the stability of training, presumably by removing the lever effects arising in a chained
composition frames. After the 8 layers, the ﬁnal backbone frames and torsion angles are mapped to frames
for all rigid groups (backbone and side chain) T f

i and all atom coordinates (cid:126)xa

i (Algorithm 20 line 24).

During training, the predicted frames and atom coordinates are compared against the ground truth by the
FAPE loss (Algorithm 20 line 28) that assesses all atom coordinates (backbone and side chain) relative to
all rigid groups. The 180◦-rotation-symmetries of a few rigid groups are handled by a globally consistent
renaming of the ambiguous atoms in the ground truth structure (see subsubsection 1.8.5 for details).

i

Finally the model predicts its conﬁdence in form of a predicted lDDT-Cα score (pLDDT) per residue. This
score is trained with the true per-residue lDDT-Cα score computed from the predicted structure and the ground
truth. For details see subsubsection 1.9.6.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

25

(cid:16){sinitial

i

Algorithm 20 Structure module

def StructureModule

)

i

i

{T true,f
i ← LayerNorm(sinitial

1: sinitial
2: zij ← LayerNorm(zij)
3: si = Linear(sinitial
4: Ti = (I, (cid:126)0)
5: for all l ∈ [1, . . . , Nlayer] do
6:

)

i

},{zij}, Nlayer = 8, c = 128,
},{T alt truth,f

},{(cid:126)αtrue,f

i

i

},{(cid:126)αalt truth,f

i

},{(cid:126)xtrue,a

i

},{(cid:126)xalt truth,a

i

sinitial
i

∈ Rcs

}(cid:17)

:

si ∈ Rcs
I ∈ R3×3, (cid:126)0 ∈ R3

# shared weights

{si} += InvariantPointAttention({si},{zij},{Ti})
si ← LayerNorm(Dropout0.1(si))
Transition.
si ← si + Linear(relu(Linear(relu(Linear(si)))))
si ← LayerNorm(Dropout0.1(si))
Update backbone.
Ti ← Ti ◦ BackboneUpdate(si)
Predict side chain and backbone torsion angles ω, φ, ψ, χ1, χ2, χ3, χ4
ai = Linear(si) + Linear(sinitial
ai ← ai + Linear(relu(Linear(relu(ai))))
ai ← ai + Linear(relu(Linear(relu(ai))))
(cid:126)αf
i = Linear(relu(ai))
Auxiliary losses in every iteration.
(Ri,(cid:126)ti) = Ti
(cid:126)xCα
i = (cid:126)ti
Ll
aux =

},  = 10−12)
})
No rotation gradients between iterations to stabilize training.
if l < Nlayer then

computeFAPE({Ti},{(cid:126)xCα
+ torsionAngleLoss({(cid:126)αf

},{(cid:126)xtrue,Cα
},{(cid:126)αalt truth,f

i },{T true
i },{(cid:126)αtrue,f

(cid:16)

(cid:17)

)

i

i

i

i

i

Ti ← (stopgrad(Ri),(cid:126)ti)

7:

#
8:

9:

#
10:

#
11:

12:

13:

14:

#
15:

16:

17:

18:

#
19:
20:

all intermediate activations ∈ Rcs

ai ∈ Rc
all intermediate activations ∈ Rc
all intermediate activations ∈ Rc
i ∈ R2, f ∈ Storsion names
(cid:126)αf

end if
21:
22: end for
23: Laux = meanl({Ll
24: T f
25: T f

i , (cid:126)xa
i ← concat(Ti, T f
i )

aux})

i = computeAllAtomCoordinates(Ti, (cid:126)αf
i )

} ← renameSymmetricGroundTruthAtoms(
i },{T true,f
},  = 10−4)

{T f
i },{(cid:126)xa
},{(cid:126)xtrue,a

i },{T true,f

i },{(cid:126)xa

i

i

i

},{T alt truth,f

i

i

i

},{(cid:126)xtrue,a

# Final loss on all atom coordinates.
26: {T true,f
27:
28: LFAPE = computeFAPE({T f
# Predict model conﬁdence.
29: {rtrue LDDT
30: {rpLDDT
31: return {(cid:126)xa

i },{rpLDDT

i

i

i

} = perResidueLDDT_Ca({(cid:126)xa

i },{(cid:126)xtrue,a

i

})

},Lconf = predictPerResidueLDDT_Ca({si},{rtrue LDDT

})

i

},LFAPE,Lconf,Laux

a ∈ Satom names

},{(cid:126)xtrue,a

i

},{(cid:126)xalt truth,a

i

})

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

26

1.8.1 Construction of frames from ground truth atom positions

We construct frames using the position of three atoms from the ground truth PDB structures using a Gram–
Schmidt process (Algorithm 21). Note that the translation vector (cid:126)t is assigned to the centre atom (cid:126)x2. For
backbone frames, we use N as (cid:126)x1, Cα as (cid:126)x2 and C as (cid:126)x3, so the frame has Cα at the centre. For the side chain
frames, we use the atom before the torsion bond as (cid:126)x1, the atom after the torsion bond as (cid:126)x2 and the next atom
after that as (cid:126)x3.

Algorithm 21 Rigid from 3 points using the Gram–Schmidt process
def rigidFrom3Points((cid:126)x1, (cid:126)x2, (cid:126)x3) :
1: (cid:126)v1 = (cid:126)x3 − (cid:126)x2
2: (cid:126)v2 = (cid:126)x1 − (cid:126)x2

(cid:13)(cid:13)
3: (cid:126)e1 = (cid:126)v1/(cid:13)(cid:13)(cid:126)v1
(cid:16)
(cid:13)(cid:13)
5: (cid:126)e2 = (cid:126)u2/(cid:13)(cid:13)(cid:126)u2

4: (cid:126)u2 = (cid:126)v2 − (cid:126)e1

(cid:17)

(cid:126)e(cid:62)
1 (cid:126)v2

6: (cid:126)e3 = (cid:126)e1 × (cid:126)e2
7: R = concat((cid:126)e1, (cid:126)e2, (cid:126)e3)
8: (cid:126)t = (cid:126)x2
9: return (R,(cid:126)t)

(cid:126)x1, (cid:126)x2, (cid:126)x3 ∈ R3

R ∈ R3×3

1.8.2 Invariant point attention (IPA)

Invariant Point Attention (IPA) (Suppl. Fig. 8 and Algorithm 22) is a form of attention that acts on a set
of frames (parametrized as Euclidean transforms Ti) and is invariant under global Euclidean transformations
Tglobal on said frames. We represent all the coordinates within IPA in nanometres; the choice of units affects
the scaling of the point component of the attention afﬁnities.

To deﬁne the initial weightings for the different terms, we assume that all queries and keys come iid from
a unit normal distribution N (0, 1) and compute the variances of the attention logits. Each scalar pair q, k

contributes Var[qk] = 1. Each point pair ((cid:126)q, (cid:126)k) contributes Var[0.5(cid:13)(cid:13)(cid:126)q(cid:13)(cid:13)2 − (cid:126)q(cid:62)(cid:126)k] = 9/2. The weighting

factors wL and wC are computed such that all three terms contribute equally and that the resulting variance is
1. The weight per head γh ∈ R is the softplus of a learnable scalar.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

27

Supplementary Figure 8
Invariant Point Attention Module. (top, blue arrays) modulation by the pair rep-
resentation. (middle, red arrays) standard attention on abstract features. (bottom, green arrays) Invariant
point attention. Dimensions: r: residues, c: channels, h: heads, p: points.

dot-product affinities(rq,rv,h)queries (rq,h,c) keys (rv,h,c) output (rq,h,c) values (rv,h,c) attention weights(rq,rv,h)pair representation(r,r,cz)single repr. (r,cm) update (r,cm) query pts. (rq,h,p,3) squared distance affinities(rq,rv,h)key pts. (rv,h,p,3) out pts. (rq,h,p',3) value pts. (rv,h,p',3) attention weights(rq,rv,h)key pts. (r,h,p,3) value pts. (r,h,p',3) ++backbone frames (r, 3x3) and (r,3)query pts. (r,h,p,3) out pts. (r,h,p',3) attention weights(rq,rv,h)output (rq,h,cz) invertLinear (h,cz)→cmLinear cm→(h,c)softmax rvpair bias(rq,rv,h)Linear cz→hLinear cm→(h,c)Linear cm→(h,c)Linear cm→(h,p,3)Linear cm→(h,p,3)Linear cm→(h,p',3)pair values(r,r,cz)Linear (h,c)→cmLinear (h,p',3)→cmgammas (h,)coordinates in local framescoordinates in the global framecoordinates in local framesSuppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

28

i , kh

Algorithm 22 Invariant point attention (IPA)
def InvariantPointAttention({si},{zij},{Ti}, Nhead = 12, c = 16, Nquery points = 4, Npoint values = 8) :
1: qh
2: (cid:126)qhp
i
3: (cid:126)vhp
i = LinearNoBias(si)
4: bh
ij = LinearNoBias(zij)

i ∈ Rc, h ∈ {1, . . . , Nhead}
,∈ R3, p ∈ {1, . . . , Nquery points}, units: nanometres
, (cid:126)khp
i
i ∈ R3, p ∈ {1, . . . , Npoint values}, units: nanometres
(cid:126)vhp

i , vh
, (cid:126)khp
i = LinearNoBias(si)

i = LinearNoBias(si)

qh
i , kh

i , vh

(cid:126)qhp
i

(cid:32)

(cid:18)

wL

(cid:62)
1√
c qh

i

kh
j + bh

ij − γhwC

2

(cid:80)

p

(cid:13)(cid:13)(cid:13)Ti ◦ (cid:126)qhp

i − Tj ◦ (cid:126)khp

j

(cid:13)(cid:13)(cid:13)2(cid:19)(cid:33)

5: wC =

9Nquery points

,

6: wL =

7: ah

ij = softmaxk

3

(cid:113) 2
(cid:113) 1
i =(cid:80)
i =(cid:80)

j ah
j ah
i = T −1

i

ijzij
ijvh
j

◦(cid:80)
(cid:18)

8: ˜oh

9: oh

10: (cid:126)ohp

11: ˜si = Linear
12: return {˜si}

(cid:16)

Tj ◦ (cid:126)vhp

j

(cid:17)

j ah
ij

concath,p(˜oh

i , oh

i , (cid:126)ohp

i

(cid:19)

(cid:13)(cid:13)(cid:13)(cid:126)ohp

i

(cid:13)(cid:13)(cid:13))

,

The proof for invariance is straight-forward: The global transformation cancels out in the afﬁnity computa-

tion (Algorithm 22 line 7), because the L2-norm of a vector is invariant under rigid transformations:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)

(cid:17) ◦ (cid:126)qhp
i −(cid:16)

Tglobal ◦ Ti

Tglobal ◦ Tj

frame:(cid:16)

(cid:18)(cid:16)

(cid:17)−1 ◦(cid:88)

ah
ij

j

Tglobal ◦ Ti

Tglobal ◦ Tj

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)2

i − Tj ◦ (cid:126)khp

j

(cid:17) ◦ (cid:126)khp

j

(cid:13)(cid:13)(cid:13)(cid:13)2

=

=

Ti ◦ (cid:126)qhp
i − Tj ◦ (cid:126)khp

(cid:13)(cid:13)(cid:13)(cid:13)Tglobal ◦(cid:16)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Ti ◦ (cid:126)qhp
global ◦ Tglobal ◦(cid:88)
(cid:17)
◦(cid:88)

Tj ◦ (cid:126)vhp

◦ T −1

(cid:16)

ah
ij

j

j

j

.

= T −1

i

= T −1

i

(cid:19)

(cid:17) ◦ (cid:126)vhp

j

(cid:16)

ah
ij

Tj ◦ (cid:126)vhp

j

(cid:17)

(3)

(4)

(5)

(6)

In the computation of the output points (Algorithm 22 line 10) it cancels out when mapping back to the local

The invariance with respect to the global reference frame in turn implies that applying a shared rigid motion
to all residues, while keeping the embeddings ﬁxed, will lead to the same update in the local frames. Therefore,
the updated structure will be transformed by the same shared rigid motion showing that this update rule is
equivariant under rigid motions. Here and elsewhere, “rigid motion” includes proper rotation and translation
but not reﬂection.

j

1.8.3 Backbone update

The updates for the backbone frames are created by predicting a quaternion for the rotation and a vector for the
translation (Algorithm 23). The ﬁrst component of the non-unit quaternion is ﬁxed to 1. The three components

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

29

deﬁning the Euler axis are predicted by the network. This procedure guarantees a valid normalized quaternion
and furthermore favours small rotations over large rotations (quaternion (1,0,0,0) is the identity rotation).

Algorithm 23 Backbone update
def BackboneUpdate(si) :
1: bi, ci, di,(cid:126)ti = Linear(si)

(cid:113)

# Convert (non-unit) quaternion to rotation matrix.
2: (ai, bi, ci, di) ← (1, bi, ci, di)/

i , +d2
i

a2

i − c2

i − d2
i + b2
2bici + 2aidi
2bidi − 2aici

i

i + c2
1 + b2
2bici − 2aidi
i − b2
i − d2
a2
2cidi + 2aibi

i + c2

i

3: Ri =

2bidi + 2aici
2cidi − 2aibi
i − b2
a2

i − c2

i + d2
i

bi, ci, di ∈ R, (cid:126)ti ∈ R3



4: Ti = (Ri,(cid:126)ti)
5: return Ti

1.8.4 Compute all atom coordinates
The Structure Module predicts backbone frames Ti and torsion angles (cid:126)αf
i . The atom coordinates are then
constructed by applying the torsion angles to the corresponding amino acid structure with idealized bond
angles and bond lengths. We attach a local frame to each rigid group (see Table 2), such that the torsion axis
is the x-axis, and store the ideal literature atom coordinates [97] for each amino acid relative to these frames
r,f,a, where r ∈ {ALA, ARG, ASN, . . .} denotes the residue type, f ∈ Storsion names denotes the
in a table (cid:126)xlit
frame and a the atom name. We further pre-compute rigid transformations that transform atom coordinates
from each frame to the frame that is higher up in the hierarchy. E.g. T lit
r,(χ2→χ1) maps atoms in amino-acid
type r from the χ2-frame to the χ1-frame. As we are only predicting heavy atoms, the extra backbone rigid
groups ω and φ do not contain atoms, but the corresponding frames contribute to the FAPE loss for alignment
to the ground truth (like all other frames).

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

30

Algorithm 24 Compute all atom coordinates

i

i

) :

i , F aatype

i = (cid:126)αf

i /(cid:107)(cid:126)αf
i (cid:107)

ri,(ω→bb) ◦ makeRotX((cid:126)ωi)
ri,(φ→bb) ◦ makeRotX((cid:126)φi)
ri,(ψ→bb) ◦ makeRotX( (cid:126)ψi)

def computeAllAtomCoordinates(Ti, (cid:126)αf
1: ˆ(cid:126)αf
2: ((cid:126)ωi, (cid:126)φi, (cid:126)ψi, (cid:126)χ1i, (cid:126)χ2i, (cid:126)χ3i, (cid:126)χ4i) = ˆ(cid:126)αf
i
# Make extra backbone frames.
3: ri = F aatype
4: Ti1 = Ti ◦ T lit
5: Ti2 = Ti ◦ T lit
6: Ti3 = Ti ◦ T lit
# Make side chain frames (chain them up along the side chain).
7: Ti4 = Ti ◦ T lit
8: Ti5 = Ti4 ◦ T lit
9: Ti6 = Ti5 ◦ T lit
10: Ti7 = Ti6 ◦ T lit
# Map atom literature positions to the global frame.

ri,(χ1→bb) ◦ makeRotX((cid:126)χ1i)
ri,(χ2→χ1) ◦ makeRotX((cid:126)χ2i)
ri,(χ3→χ2) ◦ makeRotX((cid:126)χ3i)
ri,(χ4→χ3) ◦ makeRotX((cid:126)χ4i)

(cid:16){T f

ri,f,a(cid:48)}(cid:17)

i ◦ (cid:126)xlit

11: (cid:126)xa

i = concatf,a(cid:48)

12: return T f

i , (cid:126)xa
i

Algorithm 25 Make a transformation that rotates around the x-axis
def makeRotX((cid:126)α) :



0

0
0 α1 −α2
α1
0 α2

1
0


0
0

1: R =

2: (cid:126)t =

(cid:126)α ∈ R2 with (cid:107)(cid:126)α(cid:107) = 1

3: T = (R,(cid:126)t)
4: return T

1.8.5 Rename symmetric ground truth atoms
The 180◦-rotation-symmetry for some of the rigid groups (see Table 2) leads to naming ambiguities for the
atoms in this group for all atoms that are not on the rotation axis (see Table 3).

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

31

Table 3 Ambiguous atom names due to 180◦-rotation-symmetry for some of the rigid groups

aatype

renaming swaps

ASP
GLU
PHE
TYR

Oδ1 ↔ Oδ2
O1 ↔ O2

Cδ1 ↔ Cδ2, C1 ↔ C2
Cδ1 ↔ Cδ2, C1 ↔ C2

Algorithm 26 resolves the naming ambiguities in a globally consistent way by renaming the ground truth
structure: For each residue, it computes the lDDT of the atoms against all non-ambiguous atoms for both possi-
ble namings (“true” and “alt truth”) of the ground truth atoms. The set of non-ambiguous atoms Snon-ambiguous atoms
is the set of all (residue-type, atom-type) tuples from Table 2 minus the set of ambiguous atoms from Table 3.
Subsequently the algorithm renames the ambiguous ground truth atoms such that they ﬁt best to the predicted
structure.

Algorithm 26 Rename symmetric ground truth atoms
def renameSymmetricGroundTruthAtoms({T f

i },{(cid:126)xa

i },{T true,f

i

},{T alt truth,f

i

i

i

},{(cid:126)xalt truth,a

},{(cid:126)xtrue,a
}) :
∀(j, b) ∈ Snon-ambiguous atoms
∀(j, b) ∈ Snon-ambiguous atoms
∀(j, b) ∈ Snon-ambiguous atoms

(cid:12)(cid:12)(cid:12)(cid:19)

< suma,(j,b)

(cid:18)(cid:12)(cid:12)(cid:12)d(i,a),(j,b) − dtrue

(cid:12)(cid:12)(cid:12)(cid:19)

(i,a),(j,b)

then

j

(cid:13)(cid:13)(cid:13)

i − (cid:126)xb

− (cid:126)xtrue,b

(cid:13)(cid:13)(cid:13)(cid:126)xa
(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:126)xtrue,a
(cid:13)(cid:13)(cid:13)(cid:126)xalt truth,a
(cid:13)(cid:13)(cid:13)
(cid:18)(cid:12)(cid:12)(cid:12)d(i,a),(j,b) − dalt truth

− (cid:126)xtrue,b

j

j

i

(i,a),(j,b)

1: d(i,a),(j,b) =

2: dtrue

(i,a),(j,b) =

i

(i,a),(j,b) =

3: dalt truth
4: for all i ∈ [1 . . . Nres] do
5:

if suma,(j,b)

i

6:

i ← (cid:126)xalt truth,a
(cid:126)xtrue,a
← T alt truth,f
T true,f
7:
i
end if
8:
9: end for
10: return {T true,f

},{(cid:126)xtrue,a

i

i

i

}

1.8.6 Amber relaxation

In order to resolve any remaining structural violations and clashes [98], we relax our model predictions by
an iterative restrained energy minimization procedure. At each round, we perform minimization of the AM-
BER99SB [99] force ﬁeld with additional harmonic restraints that keep the system near its input structure.
The restraints are applied independently to heavy atoms, with a spring constant of 10 kcal/mol Å2. Once the
minimizer has converged, we determine which residues still contain violations. We then remove restraints
from all atoms within these residues and perform restrained minimization once again, starting from the min-
imized structure of the previous iteration. This process is repeated until all violations are resolved. In the
CASP14 assessment we used a single iteration; targets with unresolved violations were re-run.

Full energy minimization and hydrogen placement was performed using the OpenMM simulation pack-
age [100]. The minimization procedure was run with a tolerance of 2.39 kcal/mol and an unbounded maxi-
mum number of steps, which are OpenMM’s default values.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

32

1.9 Loss functions and auxiliary heads

training

The network is trained end-to-end, with gradients coming from the main Frame Aligned Point Error (FAPE)
loss LFAPE and a number of auxiliary losses. The total per-example loss can be deﬁned as follows

0.5LFAPE + 0.5Laux + 0.3Ldist + 2.0Lmsa + 0.01Lconf

L =

0.5LFAPE + 0.5Laux + 0.3Ldist + 2.0Lmsa + 0.01Lconf + 0.01Lexp resolved + 1.0Lviol ﬁne-tuning

,

(7)
where Laux is the auxiliary loss from the Structure Module (averaged FAPE and torsion losses on the inter-
mediate structures, deﬁned in Algorithm 20 line 23), Ldist is an averaged cross-entropy loss for distogram
prediction, Lmsa is an averaged cross-entropy loss for masked MSA prediction, Lconf is the model conﬁdence
loss deﬁned in subsubsection 1.9.6, Lexp resolved is the “experimentally resolved” loss deﬁned in subsubsec-
tion 1.9.10, and Lviol is the violation loss deﬁned in subsubsection 1.9.11. The last two losses are only used
during ﬁne-tuning.

To decrease the relative importance of short sequences, we multiply the ﬁnal loss of each training example
by the square root of the number of residues after cropping. This implies equal weighting for all proteins that
are longer than the crop size, and a square-root penalty for the shorter ones.

The purpose of the FAPE, aux, distogram, and MSA losses is to attach an individual loss to each major
subcomponent of the model (including both the pair and MSA ﬁnal embeddings) as a guide during the training
of the “purpose” of each unit. The FAPE and aux are direct structural terms for the Structure module. The
distogram loss assures that all entries in the pair representation have a clear relationship to the associated ij
residue pair and to make sure that the pair representation will be useful for the structure module (ablations
show that this is only a minor effect however). The distogram is also a distributional prediction, such that
it is a method by which we interpret the model’s conﬁdence in interdomain interactions. The MSA loss
is intended to force the network to consider inter-sequence or phylogenetic relationships to complete the
BERT task, which we intend as a way to encourage the model to consider co-evolution-like relationships
without explicitly encoding covariance statistics (this is the intention but we only observe the outcome that
it increases model accuracy). The very small conﬁdence loss allows the construction of the pLDDT value
without compromising the accuracy of the structures themselves – we had previously ﬁne-tuned this loss after
training but it is just as accurate to train from the beginning with a small loss. Finally, the “violation” losses
encourage the model to produce a physically plausible structure with correct bond geometry and avoidance of
clashes, even in cases where the model is highly unsure of the structure. This avoids rare failures or accuracy
loss in the ﬁnal AMBER relaxation. Using the violation losses early in training causes a small drop in ﬁnal
accuracy since the model overly optimizes for the avoidance of clashes, so we only use this during ﬁne-tuning.
The various loss weights were hand-selected and only lightly-tuned over the course of AlphaFold develop-
ment (typically a couple of values for the coefﬁcient of each loss was tried when the loss term was introduced
and the weights rarely adjusted after that). We did some tuning early in model development on the ratio of
FAPE, distogram, and MSA losses, but did not re-tune much as the model developed. It is possible that auto-
mated or more extensive tuning of these weights could improve accuracy, but we have generally not observed
strong sensitivity to the precise values that would motivate us to do so.

Below we provide details of the individual losses and transformations that are applied to the Evoformer

output representations to obtain auxiliary predictions.

1.9.1 Side chain and backbone torsion angle loss
The predicted side chain and backbone torsion angles are represented as points on the unit circle, i.e., ˆ(cid:126)αf
R2 with (cid:107)ˆ(cid:126)αf
i (cid:107) = 1. They are compared to the true torsion angles (cid:126)αtrue,f
mathematically equivalent to the cosine of the angle difference (see below).

i ∈
with an L2 loss in R2, which is
Some side chains parts are 180◦-rotation-symmetric (see Table 2), such that the predicted torsion angle χ
and χ + π result in the same physical structure (just the internal atom names are exchanged). We allow the

i

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

33

network to produce either of these torsion angles by providing the alternative angle as (cid:126)αalt truth,f
For all non-symmetric conﬁgurations we set (cid:126)αalt truth,f

+ π.
We also introduce a small auxiliary loss Langlenorm that keeps the predicted points close to the unit circle.
There are two reasons for this: One is to avoid vectors too close to the origin, which can lead to numerically
unstable gradients. The other is that while the norm of the vector does not inﬂuence the output it does inﬂuence
the learning dynamics of the network. When looking at how the gradients transform in the backward pass of
the normalization the gradients will get rescaled by the norm of the unnormalized vector.

= (cid:126)αtrue,f

= (cid:126)αtrue,f

.

i

i

i

i

Given that the model is highly nonlinear the length of these vectors can change strongly during training,
leading to undesirable learning dynamics. The weighting factor was selected on an ad hoc basis, testing a few
values and picking the lowest one that keeps the norm of the vectors stable. We have not observed any strong
dependence on the precise value in terms of model performance.

i

i

}) :

i },{(cid:126)αtrue,f

},{(cid:126)αalt truth,f

i = (cid:107)(cid:126)αf
i (cid:107)
i /(cid:96)f
i = (cid:126)αf

Algorithm 27 Side chain and backbone torsion angle loss
def torsionAngleLoss({(cid:126)αf
1: (cid:96)f
2: ˆ(cid:126)αf
3: Ltorsion = meani,f (minimum((cid:107)ˆ(cid:126)αf
4: Langlenorm = meani,f ({|(cid:96)f
i − 1|})
5: return Ltorsion + 0.02Langlenorm

i − (cid:126)αtrue,f

(cid:107)2, (cid:107)ˆ(cid:126)αf

i

i

i − (cid:126)αalt truth,f

i

(cid:107)2))

The comparison of two angles (α and β) represented as points on the unit circle by an L2 norm is mathe-

matically equivalent to the cosine of the angle difference:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) cos α

sin α

(cid:17) −(cid:16) cos β

sin β

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)2

=

=

cos(α − β) = cos α · cos β + sin α · sin β

sin α

(cid:17)(cid:62)(cid:16) cos β
(cid:16) cos α
(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) cos α
(cid:16) cos α

(cid:17)
(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)2 − 2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16) cos β
(cid:17)
(cid:17)(cid:62)(cid:16) cos β

sin α

sin β

sin β

+

= 2 − 2
= 2 − 2 cos(α − β),

sin α

sin β

(cid:16) cos α

(cid:17)(cid:62)(cid:16) cos β

(cid:17)

sin α

sin β

(8)

(9)

where the ﬁrst identity is just the normal cosine difference formula.

1.9.2 Frame aligned point error (FAPE)
The Frame Aligned Point Error (FAPE) (see Algorithm 28, and Fig. 3f with its corresponding section in
the main article) scores a set of predicted atom coordinates {(cid:126)xj} under a set of predicted local frames {Ti}
against the corresponding ground truth atom coordinates {(cid:126)xtrue
}. The
ﬁnal FAPE loss (Algorithm 20 line 28) scores all atoms in all backbone and side chain frames. Additionally a
cheaper version (scoring only all Cα atoms in all backbone frames) is used as an auxiliary loss in every layer
of the Structure Module (Algorithm 20 line 17).

j } and ground truth local frames {T true

i

In order to formulate the loss we compute the atom position (cid:126)xj relative to frame Ti (Algorithm 28 line 1)
and the location of the corresponding true atom position (cid:126)xtrue
(Algorithm 28
line 2). The deviation is computed as a robust L2 norm (Algorithm 28 line 3) ( is a small constant added
to ensure that gradients are numerically well behaved for small differences. The exact value of this constant
does not matter, as long as it is small enough. We used values of 10−4 and 10−12 in our experiments.). The

relative to the true frame T true

j

i

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

34

resulting Nframes × Natoms deviations are penalized with a clamped L1-loss (Algorithm 28 line 4) with a length
scale Z = 10 Å to make the loss unitless.

In this section, we represent the point positions and the hyperparameters in Å, although the loss is invariant

to the choice of units.

We now discuss the behaviour of the loss under global rigid transformations of both the true and the pre-
dicted structure. Firstly we should note that (cid:126)xij is invariant under rigid motions (not including reﬂection); as
such if the predicted structure differs from the ground truth by an arbitrary rotation and translation the loss
will stay constant. However, the loss is not invariant under reﬂections due to the way the local frames are
constructed, as the z-axis of the local frames transforms as a pseudo-vector. Furthermore, we can see that the
error is invariant when applying the same rigid motion to both target frames and predicted frames. This im-
plies that the way frames are constructed from the structure is not constrained to the precise choice we made,
but can differ as long as they are constructed in a consistent manner between predicted and target structure.

Algorithm 28 Compute the Frame aligned point error
def computeFAPE({Ti},{(cid:126)xj},{T true

},{(cid:126)xtrue

i

j }, Z = 10Å, dclamp = 10Å,  = 10−4Å2

) :
Ti, T true

i

∈ (R3×3, R3)
j ∈ R3,
(cid:126)xj, (cid:126)xtrue
i ∈ {1, ..., Nframes}, j ∈ {1, ..., Natoms}
(cid:126)xij ∈ R3
ij ∈ R3
(cid:126)xtrue
dij ∈ R

i

1: (cid:126)xij = T −1
ij = T true
2: (cid:126)xtrue

◦ (cid:126)xj
−1 ◦ (cid:126)xtrue

(cid:113)(cid:107)(cid:126)xij − (cid:126)xtrue

j

i

3: dij =
4: LFAPE = 1
5: return LFAPE

ij (cid:107)2 + 

Z meani,j(minimum(dclamp, dij))

1.9.3 Chiral properties of AlphaFold and its loss

In this section we will look in detail at the transformation properties of the various components under global
reﬂections

(10)
Under this global reﬂection, the coordinates of the frames also change in a non-trivial way (use Algorithm 21
with negated positions). Simple algebra shows that

(cid:126)xi

reﬂection

(cid:55)→ −(cid:126)xi

(cid:32)

(cid:18) −1

(cid:19)

(cid:33)

Ti = (Ri,(cid:126)ti)

reﬂection

(cid:55)→

Ri

0
0 −1
0
0

0
0
1

,−(cid:126)ti

,

(11)

where the non-trivial transformation of the rotation matrix Ri arises from the cross-product in Algorithm 21.
The non-trivial transformation of the rotation matrix also means that the local points T −1
◦(cid:126)xj are not invariant
under reﬂection,

i

T −1

i

◦ (cid:126)xj = R−1

i

reﬂection

(cid:55)→

(cid:18) 1
(cid:18) 1

0
0

0
0

=

=

(cid:18) −1

((cid:126)xj − (cid:126)ti)
0
0 −1
0
0

(cid:19)

0
0
1

(cid:19)
(cid:19)(cid:16)

0
0
1
0
0 −1

0
0
1
0
0 −1

R−1

i

((cid:126)xj − (cid:126)ti)

T −1

i

◦ (cid:126)xj

.

(cid:17)

R−1

i

(−(cid:126)xj + (cid:126)ti)

(12)

(13)

(14)

(15)

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

35

The effect of a global reﬂection is to reﬂect only the z-component of the local coordinates T −1
In the following we denote a set of frames and points by large Roman letters, for example X =
In particular, this means that both FAPE and IPA can distinguish global reﬂections of the protein assuming

◦ (cid:126)xj.

i

that the rigid frames are related to the underlying points by Algorithm 21, e.g.

(cid:16)(cid:8)(cid:126)xi

(cid:9) ,(cid:8)Tj

(cid:9)(cid:17)

.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)T −1
(cid:88)
(cid:12)(cid:12)(cid:12)(T −1
(cid:88)

ij

i

i

ij

= 2

(cid:18) 1
(cid:12)(cid:12)(cid:12) ,

0
0

◦ (cid:126)xj −

◦ (cid:126)xj)z

(cid:19)(cid:16)

0
0
1
0
0 −1

T −1

i

◦ (cid:126)xj

(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(16)

(17)

FAPE(X, Xreﬂection) =

which is a large positive value outside of degenerate cases. For a more general proof that FAPE can distinguish
chirality regardless of how the frames are constructed, see the next section.

There are other sources of chirality in AlphaFold. The atom positions are computed from a combination
of the backbone frames and the predicted χ angles, and this procedure will always generate a left-handed
molecule since it uses ideal values outside of the χ angles (i.e. the CB will always be in the left-handed
location). AlphaFold can produce almost exactly the chiral pair for the backbone atoms, but it cannot build
the chiral pair of the side chains. The model also has a small loss on the χ-angle values and those values are
not invariant under reﬂection.

In order to test the importance of the chirality of FAPE we trained a model using the dRMSD loss [101]
instead of FAPE, we show the results on the CASP14 set in Figure 9. Here we can see that the lDDT-Cα
performance is still good, where we note that lDDT-Cα is a metric that cannot distinguish molecules of the
opposite chirality.

However GDT shows a bimodal distribution if trained with a dRMSD loss, where one of the two modes
is only somewhat worse than baseline AlphaFold and the other mode has very low accuracy (Suppl. Fig. 9).
This suggests that the second mode composes molecules of reverse chirality. To test this we compute GDT ,
which is the GDT of the mirror reﬂection of the structure used to compute GDT. These mirror structures
also show bimodality of the GDT values. Finally taking the maximum over the GDT of the structure and its
mirror produces consistently high GDT. This conﬁrms that AlphaFold trained with a dRMSD loss frequently
produces mirror structures, and that FAPE is the main component that ensures the correct chirality of predicted
structures.

1.9.4 Conﬁgurations with FAPE(X,Y) = 0

To understand the points at which zero FAPE loss is achieved, we will introduce an RMSD-like auxiliary
measure that operates only on points and not frames
S({(cid:126)xi},{(cid:126)yi}) = min

(18)

T align∈SE(3)

where T is a proper rigid transformation. Then we can show a lower bound of FAPE irrespective of the values
of the frames,

(cid:13)(cid:13)(cid:13)

j

1

Npoint

(cid:88)
(cid:13)(cid:13)(cid:13)T −1
(cid:88)
 1
(cid:88)

(cid:13)(cid:13)(cid:13)(cid:126)xj − T align ◦ (cid:126)yj
(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:126)xj − (Ti ◦ ˜T −1

◦ (cid:126)xj − ˜T −1

◦ (cid:126)˜xj

ij

i

i

i

Npoint

j

) ◦ (cid:126)˜xj

FAPE(X, ˜X) =

1

NframesNpoint

=

1

Nframes

(cid:88)
(cid:88)

i

S({(cid:126)xj},{(cid:126)˜xj})

1

Nframes

≥
= S({(cid:126)xj},{(cid:126)˜xj}),

i


(cid:13)(cid:13)(cid:13)

(19)

(20)

(21)

(22)

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

36

Supplementary Figure 9 Accuracy distribution of a model trained with dRMSD instead of the FAPE loss
(N = 87 protein domains). Here GDT denotes the GDT score obtained when comparing to the chirally
reﬂected target structure.

since the S function minimizes the quantity in parentheses over all proper rigid transformations and Ti ˜T −1
is a proper rigid transformation. This inequality simply shows that the point errors averaged over all local
frames are no less than the point error for the best single global alignment under the same distance function.
The value S({(cid:126)xi},{(cid:126)˜xi}) is zero iff RMSD({(cid:126)xi},{(cid:126)˜xi}) = 0, which shows that FAPE(X, ˜X) = 0 is only
possible if the points have RMSD({(cid:126)xi},{(cid:126)yi}) = 0. We can characterize all pairs such that FAPE(X, ˜X) = 0
as those where RMSD({(cid:126)xi},{(cid:126)˜xi}) = 0 and T −1
◦ ˜Ti is one of rigid motions that achieves this zero RMSD
(which is unique unless the point sets are degenerate). In particular, the FAPE loss always has non-zero values
chiral pairs if the point sets are non-degenerate, regardless of the how the frames are constructed.

i

i

1.9.5 Metric properties of FAPE

In this section, we show that idealized FAPE (i.e. using  = 0) has all the properties of a pseudometric, which
is a generalized distance function used in topology. To reduce clutter, the proofs are shown without a cutoff but
hold equally well when (cid:107)·(cid:107) is treated as a clipped vector norm. The reader uninterested in the mathematical
details may skip this section.

It is simple to show from the deﬁning equation that

FAPE(X, Y ) ≥ 0
FAPE(X, X) = 0

(23)
(24)
(25)
so the only remaining pseudometric property to prove is the triangle inequality, FAPE(X, Z) ≤ FAPE(X, Y )+
FAPE(Y, Z). The triangle inequality is proven below, using tildes and overbars to designate the different con-

FAPE(X, Y ) = FAPE(Y, X),

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

37

ﬁgurations.

FAPE(X, ˜X) =

i,j

(cid:88)
(cid:88)
≤(cid:88)
(cid:88)

=

i,j

i,j

=

i,j

i

(cid:13)(cid:13)(cid:13)T −1
(cid:13)(cid:13)(cid:13)T −1
(cid:18)(cid:13)(cid:13)(cid:13)T −1
(cid:13)(cid:13)(cid:13)T −1

i

i

i

(cid:13)(cid:13)(cid:13)

◦ (cid:126)˜xj

◦ (cid:126)xj − ˜T −1

i

(cid:13)(cid:13)(cid:13)

◦ (cid:126)˜xj

◦ (cid:126)xj − ¯T −1

◦ (cid:126)¯xj + ¯T −1

◦ (cid:126)¯xj − ˜T −1

i

i

◦ (cid:126)xj − ¯T −1

i

◦ (cid:126)¯xj

◦ (cid:126)¯xj − ˜T −1

◦ (cid:126)˜xj

i

◦ (cid:126)xj − ¯T −1

◦ (cid:126)¯xj

i

◦ (cid:126)¯xj − ˜T −1

i

◦ (cid:126)˜xj

i

(cid:13)(cid:13)(cid:13) +
(cid:13)(cid:13)(cid:13) ¯T −1
(cid:13)(cid:13)(cid:13) ¯T −1
(cid:13)(cid:13)(cid:13) +
(cid:88)

i

i

i,j

(cid:13)(cid:13)(cid:13)(cid:19)
(cid:13)(cid:13)(cid:13)

= FAPE(X, ¯X) + FAPE( ¯X, ˜X).

(26)

(27)

(28)

(29)

(30)

1.9.6 Model conﬁdence prediction (pLDDT)

Our model provides intrinsic model accuracy estimates by predicting the per-residue lDDT-Cα [98] scores.
We call this conﬁdence measure pLDDT. Algorithm 29 computes it by taking the ﬁnal single representation
from the structure module (Algorithm 20 line 30), and projecting it into 50 bins (each bin covering a 2 lDDT-
Cα range).
For training, we score the ﬁnal predicted structure with per-residue lDDT-Cα against the ground truth
structure. This score is discretized into 50 bins and used as the target value for the cross-entropy loss Lconf
averaged over the residues. Additionally, we only train on the examples with resolution between 0.1 Å and
3.0 Å, and ignoring any NMR structure, to ensure high-quality ground truth data.

For evaluation, we compute the expected value of the per-residue pLDDT distribution (Algorithm 29 line 5).

The chain pLDDT is obtained as an average of the per-residue pLDDT scores within the chain.

i

Algorithm 29 Predict model conﬁdence pLDDT
def predictPerResidueLDDT_Ca({si}, vbins = [1, 3, 5, . . . , 99](cid:62),{rtrue LDDT
1: ai = relu(Linear(relu(Linear(LayerNorm(si)))))
2: ppLDDT
3: ptrue LDDT
(cid:62)
4: Lconf = meani(ptrue LDDT
i
(cid:62)
5: rpLDDT
= ppLDDT
vbins
},Lconf
6: return {rpLDDT

= softmax(Linear(ai))
= one_hot(rtrue LDDT

, vbins)
log ppLDDT

)

i

i

i

i

i

i

i

}, c = 128) :

ai, and intermediate activations ∈ Rc
∈ R|vbins|

ppLDDT
i

rpLDDT
i

∈ R

1.9.7 TM-score prediction

The pLDDT head above predicts the value of lDDT-Cα, which is a local error metric that operates pairwise
but by design is not sensitive to what fraction of the residues can be aligned using a single global rotation
and translation. This can be disadvantageous for assessing whether the model is conﬁdent in its overall
domain packing for large chains. In this section we develop a predictor of the global superposition metric
TM-score [102].
}, the predicted
structure’s Cα atoms via X = {(cid:126)xj}, and the corresponding backbone frames {Ti}. Let the number of residues
be Nres, and assume all residues are resolved in the ground truth. Denoting T align = (Ralign,(cid:126)talign) an arbitrary

We denote the ground truth structure’s Cα atoms Xtrue = {(cid:126)xtrue

j }, its backbone frames {T true

i

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

38

rigid transformation, the TM-score is deﬁned as

TM(X, Xtrue) = max

T align∈SE(3)

(cid:16)(cid:107)(cid:126)xj − T align ◦ (cid:126)xtrue
j (cid:107)(cid:17)
d0(Nres) = 1.24 3(cid:112)maximum(Nres, 19) − 15 − 1.8

Nres(cid:88)
(cid:17)2 ,

1
Nres

f (d) =

(cid:16)

d0(Nres)

1 +

j=1

1

f

d

,

(31)

(32)

(33)

In the last equation, Nres is clipped to avoid negative or undeﬁned values for very short proteins. In practice,
global optimisation over T align is infeasible, so computational methods rely on approximations, e.g. TM-
align [103] uses an iterative procedure of choosing a best-ﬁt alignment on a subset of residues and then
re-computing this set of residues.

We now develop an approximation to the TM-score that relies on a pairwise accuracy measure which can
be predicted from a pair representation. Crucially, this approximation allows prediction of the TM-score for
an arbitrary subset of residues (such as a domain) without rerunning the structure prediction.

TM(X, Xtrue) = max

T align∈SE(3)

1
Nres

≥ max

i∈[1,...,Nres]

= max

i

1
Nres

1
Nres

(cid:88)

f

j

j

f

(cid:16)(cid:107)(cid:126)xj − T align ◦ (cid:126)xtrue
j (cid:107)(cid:17)
(cid:88)
j (cid:107)(cid:17)
(cid:16)(cid:107)(cid:126)xj − (TiT true
(cid:88)
(cid:16)(cid:107)T −1
j (cid:107)(cid:17)

◦ (cid:126)xj − T true

−1) ◦ (cid:126)xtrue

−1 ◦ (cid:126)xtrue

f

.

j

i

i

i

Above, we replace the continuous maximum over all possible alignments with a discrete maximum set of
−1). This clearly lower-bounds the original maximum, with the bound
Nres single-residue alignments (TiT true
becoming tight when the global superposition exactly aligns the backbone of any residue. Then, we sym-
metrize the vector difference by applying the rigid transformation Ti. Note that the expression in Equation 36
is closely related to FAPE, except that the f function is somewhat different and FAPE has a mean instead of
maximum (i.e. FAPE considers all 1-residue alignments instead of only the best one).

Next, we assume that the correct structure Xtrue is unknown and that we have a distribution of probable

i

structures for which we seek the expected value of the TM-score for our prediction X:

E(cid:104)

TM(X, Xtrue)

(cid:105) ≥ E

max

i

≥ max

i

1
Nres

1
Nres

(cid:88)

j

(cid:88)
(cid:20)

j

E

f

f

(cid:16)(cid:107)T −1
(cid:16)(cid:107)T −1

i

i

j (cid:107)(cid:17)
j (cid:107)(cid:17)(cid:21)

.

◦ (cid:126)xj − T true

i

−1 ◦ (cid:126)xtrue

◦ (cid:126)xj − T true

i

−1 ◦ (cid:126)xtrue

i

−1 ◦ (cid:126)xtrue

◦ (cid:126)xj − T true

The pairwise matrix eij = (cid:107)T −1

In the ﬁnal line, we replace the expectation-of-maximum with a maximum-of-expectation (a lower bound due
to Jensen’s inequality), and use the linearity of expectation.
j (cid:107) is a non-symmetric matrix that captures the error in
the position of the Cα atom of residue j when the predicted and true structures are aligned using the backbone
frame of residue i. The probability distribution of its elements can be readily predicted by a neural network.
To do that, we discretize the distribution of eij into 64 bins, covering the range from 0 to 31.5 Å with 0.5 Å
bin width. During training, the ﬁnal bin also captures any larger errors. We compute eij as a linear projection
of the pair representation zij, followed by a softmax. We ﬁne-tune the CASP models to additionally predict
eij using the average categorical cross-entropy loss, with weight 0.1 (weights 0.01 and 1.0 were also tried
but those weights lowered either pTM accuracy or structure prediction accuracy, respectively). Just as for

i

(34)

(35)

(36)

(37)

(38)

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

39

the pLDDT prediction, we only train this prediction module on non-NMR examples with resolution between
0.1 Å and 3.0 Å. The ﬁne-tuning took roughly 16 hours and processed 3 · 105 samples.

Using eij, we approximate the TM-score as

pTM = max

i

1
Nres

E(cid:2)f (eij)(cid:3) ,

(cid:88)

j

where the expectation is taken over the probability distribution deﬁned by eij. We ﬁnd that pTM is a pes-
simistic predictor of the true TM-score, which is as expected, given that both approximations used in the
derivation of pTM are lower bounds.
D (such as a particular domain) simply by restricting the range of residues considered:

Note that given a full-chain prediction of eij, we can obtain a TM-score prediction for any subset of residues

pTM(D) = max
i∈D

1
|D|

(cid:88)

E

j∈D

1 +

(cid:16) eij

1

d0(|D|)

(cid:17)2 ,

(39)

(40)

where |D| is the number of residues in the set D. By small modiﬁcations, a similar expression could be derived
to estimate GDT, FAPE, or RMSD from the eij matrix, though we do not pursue this further. Additionally,
the 2-D image of the expected values of f (eij) serves as a good visualization of conﬁdent domain packings
within the structure.

1.9.8 Distogram prediction

We linearly project the symmetrized pair representations (zij + zji) into 64 distance bins and obtain the bin
probabilities pb
ij with a softmax. The bins cover the range from 2 Å to 22 Å; and they have equal width apart
from the last bin, which also includes any more distant residue pairs. For prediction targets yb
ij we use one-hot
encoded binned residue distances, which are computed from the ground-truth beta carbon positions for all
amino acids except glycine where use alpha carbon instead. We use the cross-entropy loss averaged over all
residue pairs:

Ldist = − 1
N 2
res

1.9.9 Masked MSA prediction

(cid:88)

64(cid:88)

i,j

b=1

yb
ij log pb

ij .

(41)

Similarly to the common masked language modelling objectives [104], we use the ﬁnal MSA representations
to reconstruct MSA values that have been previously masked out (see subsubsection 1.2.7). We consider 23
classes, which include 20 common amino acid types, an unknown type, a gap token, and a mask token. MSA
representations {msi} are linearly projected into the output classes, passed through a softmax, and scored
with the cross-entropy loss:

yc
si log pc

si ,

(42)

Lmsa = − 1
Nmask

(cid:88)

23(cid:88)

s,i∈mask

c=1

where pc
pens over the masked positions.

si are predicted class probabilities, yc

si are one-hot encoded ground-truth values, and averaging hap-

1.9.10 “Experimentally resolved” prediction

The model contains a head that predicts if an atom is experimentally resolved in a high-resolution structure.
The input for this head is the single representation {si} produced by the Evoformer stack (subsection 1.6). The
single representation is projected with a Linear layer and a sigmoid to atom-wise probabilities {pexp resolved,a
}

i

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

40

(cid:17)

(cid:16)−ya

with i ∈ [1, . . . , Nres] and a ∈ Satom names. We train this head during ﬁne-tuning (see subsubsection 1.12.1) on
high-resolution X-ray crystals and cryo-EM structures (resolution better than 3Å) with standard cross-entropy,

Lexp resolved = mean(i,a)

i log pexp resolved,a

i

− (1 − ya

i ) log(1 − pexp resolved,a

i

)

,

(43)

where ya

i ∈ {0, 1} denotes the ground truth, i.e. if atom a in residue i was resolved in the experiment.

1.9.11 Structural violations
The construction of the atom coordinates from independent backbone frames and torsion-angles (Fig. 3e),
subsubsection 1.8.4) produces idealized bond lengths and bond angles for most of the atom bonds, but the
geometry for inter-residue bonds (peptide bonds) and the avoidance of atom clashes need to be learned. We
introduce extra losses that penalize these structural violations to ensure these constraints everywhere, i.e. also
in regions where no ground truth atom coordinates are available. We constructed the losses in a way that
loss-free structures will pass the stereochemical quality checks in the lDDT metric [98].

The losses use a ﬂat-bottom L1 loss that penalizes violations above a certain tolerance threshold τ. The

bond length violation loss is computed as

Lbondlength =

1

Nbonds

Nbonds(cid:88)

i=1

max

(cid:16)|(cid:96) i

(cid:17)

pred − (cid:96) i

lit| − τ, 0

,

(44)

pred is a bond length in the predicted structure and (cid:96) i

where (cid:96) i
lit is the literature value [97] for this bond length.
Nbonds is the number of all bonds in this structure. We set the tolerance τ to 12σlit, where σlit is the literature
standard deviation of this bond length. We chose the factor 12 to ensure that the produced bond lengths pass
the stereochemical quality checks lDDT metric [98], which also uses by default a tolerance factor of 12.
of the bonds, cos α = ˆ(cid:126)v(cid:62)

The bond angle violation loss uses the cosine of the angle computed from the dot-product of the unit vectors

ˆ(cid:126)v2 and also applies a ﬂat-bottom L1 loss on the deviations:

1

Nangles(cid:88)

(cid:16)|cos α i

(cid:17)

Lbondangle =

1

max

Nangles

i=1

pred − cos α i

lit| − τ, 0

(45)

pred is a bond angle in the predicted structure and α i

where α i
lit is the literature value for this bond angle. Nangles
is the number of all bond angles in this structure. The tolerance τ is computed such that the ﬂat bottom extends
from -12 to 12 times the literature standard deviation of this bond angle.

The clash loss uses a one-sided ﬂat-bottom-potential, that penalizes too short distances only,

Nnbpairs(cid:88)

(cid:16)

(cid:17)

Lclash =

max

lit − τ − d i
d i

pred, 0

,

(46)

i=1

pred is the distance of two non-bonded atoms in the predicted structure and d i

where d i
lit is the “clashing distance”
of these two atoms according to their literature Van der Waals radii. Nnbpair is the number of all non-bonded
atom pairs in this structure. The tolerance τ is set to 1.5 Å.

All these losses together build the violation loss

Lviol = Lbondlength + Lbondangle + Lclash .

(47)

We apply this violation loss only during the ﬁne-tuning training phase. Switching it on in the early training
leads to strong instabilities in the training dynamics.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

41

1.10 Recycling iterations

We ﬁnd it helpful to execute the network multiple times, each time embedding the previous outputs as addi-
tional inputs. We call this technique “recycling”, and we ﬁnd it relatively important in the ablation studies
(subsection 1.13). In this section we ﬁrst explain how recycling operates in general, both at training and the
inference time. Then, we describe the speciﬁc features being recycled in the AlphaFold model. Algorithm 2
shows the full inference of AlphaFold with recycling and ensembling (MSA resampling and ensembling is
further explained in subsubsection 1.11.2).

Algorithm 30 Generic recycling inference procedure

def RecyclingInference((cid:8)inputsc

(cid:9) , Ncycle) :

1: outputs = 0

outputs ← Model(inputsc, outputs)

# Recycling iterations
2: for all c ∈ [1, . . . , Ncycle] do
3:
4: end for
5: return outputs

# shared weights

Algorithm 31 Generic recycling training procedure

def RecyclingTraining((cid:8)inputsc

(cid:9) , Ncycle) :

1: N(cid:48) = uniform(1, Ncycle)
2: outputs = 0

# shared value across the batch

# Recycling iterations
3: for all c ∈ [1, . . . , N(cid:48)] do
4:

outputs ← stopgrad(outputs)
outputs ← Model(inputsc, outputs)

5:
6: end for
7: return loss(outputs)

# shared weights

# no gradients between iterations

# only the ﬁnal iteration’s outputs are used

Recycling allows to make the network deeper and to process multiple versions of the input features (e.g. in-
corporate the MSA resampling) without signiﬁcantly increasing the training time or the number of parameters.
At the inference time, recycling yields a recurrent network with shared weights, where each iteration c takes
in the input features inputsc and the previous iteration’s outputs, and produces the new reﬁned outputs, see
Algorithm 30. The initial “outputs” are constant zeros, since we ﬁnd that networks can easily accommodate
this special case. We will denote the number of iterations of this network via Ncycle. We use Ncycle = 4 in
AlphaFold.

While it is possible to train recycling by simply unrolling the Ncycle iterations, the computational and mem-
ory cost of that may be prohibitive. Instead, we introduce an approximate training scheme, which empirically
works well and signiﬁcantly reduces the extra cost of recycling during training, Algorithm 31. We ﬁrst deﬁne
an objective, the average loss of the model over all iterations:

Ncycle(cid:88)

1

Ncycle

c=1

loss(outputsc)

(48)

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

42

(cid:80)Ncycle

where outputsi are the model’s outputs at iteration c. Next, we construct an unbiased Monte Carlo estimate of
this objective by uniformly sampling the number of iterations N(cid:48) between 1 an Ncycle, and only train the loss
for that step. This means that any following iterations can be skipped. The random choice of the number of
iterations N(cid:48) is synchronized across the batch (i.e. every batch element trains the same iteration on each step)
to increase efﬁciency. Finally, we stop the gradients ﬂowing from the chosen iteration to the previous ones,
allowing to skip the backward pass (backpropagation) for the previous iterations c < N(cid:48). As an example, if
we sampled N(cid:48) = 3, and Ncycle = 4, then we perform just the forward pass for the ﬁrst two iterations; both
the forward and the backward pass for the third iteration; and the fourth iteration is completely skipped.

2

Ncycle

c=1 c = Ncycle+1

We now explain the motivation behind this procedure, compared to simple unrolling. Randomly sampling
the iteration N(cid:48) improves the efﬁciency, since we now perform 1
iterations on average
instead of Ncycle. Importantly, it also acts as auxiliary loss for the model, requiring to provide plausible outputs
mid-way through the inference. We hypothesize that this allows the network to reﬁne its own predictions
multiple times. Stopping the gradients of the intermediate outputs makes the procedure much cheaper, both
memory-wise and computationally. Memory-wise, it allows to eschew storing the intermediate activations
of the previous iterations. Computationally, this procedure requires a single backward pass and N(cid:48) forward
passes. With rematerialization (see subsubsection 1.11.8), a forward pass is approximately 4 times cheaper
than the full forward and backward round, thus each forward pass adds only 25% of the baseline training time.
Therefore, training a model with Ncycle iterations has an additional average cost of (12.5 (Ncycle − 1))%. For
Ncycle = 4 used in AlphaFold, this corresponds to just 37.5% extra training time, compared to 300% with
unrolling. While stopping the gradients between iterations leads to a bias in the gradients, we ﬁnd that does
not hamper training.

In AlphaFold, we recycle the predicted backbone atom coordinates from the Structure module (Algo-
rithm 20), and output pair and ﬁrst row MSA representations from the Evoformer (Algorithm 6). Algorithm 32
shows their embedding details. Both types of representations are passed through LayerNorm to prepare up-
dates for the corresponding input representations {zij} and {m1i}. Predicted coordinates of beta carbon
atoms (alpha carbon for glycine) are used to compute pairwise distances, which are then discretized into 15
bins of equal width 1.25Å that span the range to approx. 20Å (Due to historical reasons the precise bin values
range from 33⁄8 Å to 213⁄8 Å). The resulting one-hot distogram is linearly projected and added to the pair rep-
resentation update. The recycling updates are injected to the network as shown in line 6 of Algorithm 2 (for
a visual scheme, see R sign in Suppl. Fig. 1). This is the only mechanism employed for recycling previous
predictions, and the rest of the network is identical on all recycling iterations.

Algorithm 32 Embedding of Evoformer and Structure module outputs for recycling
def RecyclingEmbedder({m1i},{zij},{(cid:126)xCβ
# Embed pair distances of backbone atoms:

i }) :

(cid:13)(cid:13)(cid:13)(cid:126)xCβ

(cid:13)(cid:13)(cid:13)

1: dij =

i − (cid:126)xCβ

j

2: dij = Linear(one_hot(dij, vbins = [33⁄8 Å, 51⁄8 Å, . . . , 213⁄8 Å]))

# Embed output Evoformer representations:

3: ˜zij = dij + LayerNorm(zij)

4: ˜m1i = LayerNorm(m1i)
5: return { ˜m1i},{˜zij}

Cα used for glycin
dij ∈ Rcz

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

43

1.11 Training and inference details

Table 4 AlphaFold training protocol. We train each stage until convergence with the approximate timings
and number of samples provided.

Model
Number of templates Ntempl
Sequence crop size Nres
Number of sequences Nseq
Number of extra sequences Nextra_seq
Parameters initialized from
Initial learning rate
Learning rate linear warm-up samples
Structural violation loss weight
Training samples (·106)
Training time

Initial training

Fine-tuning

4
256
128
1024
Random
10−3
128000
0.0
≈ 10
≈ 7 days

4
384
512
5120
5 · 10−4

Initial training

0
1.0
≈ 1.5
≈ 4 days

1.11.1 Training stages

The training proceeds in two stages outlined in Table 4. We train with sequence crop size 256 and then ﬁne-
tune with the larger crop size 384, where the second stage starts from the pre-trained model weights and uses
the lower learning rate and no warm-up steps. Additionally, we use the increased number of MSA clusters as
well as unclustered (extra) sequences. We also apply the structural violation loss.

1.11.2 MSA resampling and ensembling

There are several sources of stochastic behaviour in the network. The most important one is MSA pre-
processing, which includes MSA block deletion (Algorithm 1) and MSA clustering procedure (subsubsec-
tion 1.2.7). To reduce the stochastic effects of these steps, we perform them multiple times, resulting in a set
of sampled MSA and extra MSA features. Note that the residue cropping (subsubsection 1.2.8), which follows
these steps, is done consistently across all samples, i.e. the same residue ranges are taken. Also note that we
do not perform template re-sampling, i.e. the same templates (up to 4) are used for one training example.

To take advantage of the multiple samples of these features, we embed them multiple times with the Evo-
former network and average the output embeddings. Concretely, at each recycling iteration we perform mul-
tiple passes of the Evoformer (including all preceding input embedding transformations) for the different
instances of ”msa_feat” and ”extra_msa_feat”, and we average the output pair and single representations be-
fore providing them to the structure module or any other heads of the network (see Algorithm 2). We call this
technique ensembling (although it does not ensemble ﬁnal predictions) and we employ it during inference, but
not during training. Finally, since we also use different samples of the MSA and extra MSA features at each
recycling iteration, the full system processes the total of Ncycle × Nensemble samples.

We use Nensemble = 3 for the best checkpoint selection in the evaluator (subsubsection 1.11.7) and also for
the inference on CASP14 targets performed for this paper. However, with the recycling and pLDDT-based
selection from 5 model runs, we have found that the additional ensembling has only a minor impact on the
accuracy of the system (see subsubsection 1.12.2 for details). Given that Nensemble is almost a linear multiplier
for the inference time, we are gradually deprecating it in our systems, and the inference on the recent PDB
dataset for this paper has been performed with Nensemble = 1.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

44

1.11.3 Optimization details
For optimization we use Adam [105] with the base learning rate 10−3, β1 = 0.9, β2 = 0.999,  = 10−6.
We linearly increase (warm-up) the learning rate over the ﬁrst 0.128 · 106 samples, and further decrease the
learning rate by a factor of 0.95 after 6.4 · 106 samples. During the ﬁne-tuning stage we have no learning rate
warm-up, but we reduce the base learning rate by half.

We train using the mini-batch size of 128, one example per TPU-core. To stabilize the training, we apply
gradient clipping by the global norm [106] on the parameters independently to every training example in a
mini-batch, with a clipping value 0.1.

1.11.4 Parameters initialization
Linear layers. By default, the weights of the Linear layers are initialized using the LeCun (fan-in) initial-
ization strategy [107] with a truncated normal distribution. By default, the biases of the Linear layers are
initialized by zero. For the layers immediately followed by a ReLU activations, we use He initializer [108].
The queries, keys and values projection layers in self-attention layers are initialized using the ’fan-average’
Glorot uniform scheme [109].

To improve stability of training, we zero-initialize the weights of the ﬁnal weight layers [110] in every
residual layer. This ensures that every residual layer acts as an identity operation at initialization. Further-
more, we zero-initialize all the ﬁnal projection weight layers of the network: masked MSA prediction logits,
residue distance prediction logits, model conﬁdence prediction logits. We also identity-initialize the rigid
frame updates in the Structure module.

Gating Linear layers, i.e. the Linear layers immediately followed by a sigmoid, are initialized with zero
weights. The biases are initialized with a constant value of 1, ensuring that at initialisation, the gates are
mostly-opened with a value of sigmoid(1) = 1/(1 + exp(−1)) ≈ 0.73.

Layer normalization layers are initialized using the standard scheme: gains set to 1 and biases set to 0.
Point weights in Invariant Point Attention γh are initialized to be 1 after the softplus transformation.

1.11.5 Loss clamping details

In 90% of training mini-batches the FAPE backbone loss is clamped by emax = 10 Å, in the remaining 10%
it is not clamped, emax = +∞. For side-chains it is always clamped by emax = 10 Å.

1.11.6 Dropout details

We denote the standard Dropout [96] with operator Dropoutx, where x is the dropout rate, i.e. the probability
to zero an entry. In residual updates for the self-attention operations, we modify it such that dropout masks are
shared across one of the dimensions. We use DropoutRowwisex operator to denote the version with the mask
of shape [1, Nres, Nchannel], which is shared across rows. It is used in the residual updates following the MSA
row-wise self-attention and the triangular self-attention around starting node. For a given residue (column)
the same channels are set to zero across all rows in these updates. Similarly, we use DropoutColumnwisex
operator to denote the version with the mask of the shape [Nres, 1, Nchannel], which is shared across columns.
It is used in the residual update following the triangular self-attention around ending node.
In the main
Evoformer stack we also apply the row-wise Dropout when performing the residual updates for the triangular
multiplicative operations in the pair stack. We keep the same conﬁguration for the unclustered MSA stack
and the Template pair stack. In the Structure module, we apply Dropout to the results of the Invariant Point
Attention and the Transition layer. Please refer to Algorithm 6, Algorithm 16, Algorithm 18, and Algorithm 20
for the exact locations of all Dropout operations and their rates.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

45

1.11.7 Evaluator setup

During evaluation, we use an exponential moving average of the trained parameters [111] with the de-
cay 0.999. To select the best model during training, we monitor lDDT-Cα performance on a validation set
of targets collected from CAMEO [112] over 3 months period (2018-12-14 to 2019-03-09). It is ﬁltered to a
maximum sequence length of 700, and there is no residue cropping at evaluation time.

1.11.8 Reducing the memory consumption

AlphaFold model has high memory consumption, both in training and during inference. This is largely driven
by the logits in the self-attention layers and the large pair and MSA activiations. For example, ah
ijk in Algo-
res · Nhead) ﬂoating point numbers. We now describe the techniques
rithm 13 and Algorithm 14 consists of (N 3
we employ to perform memory-efﬁcient training and inference of this model.

Training. We store all intermediate activations of the model in bfloat16 format, which only requires two
bytes per ﬂoating point number. Training is performed on relatively short sequence crop sizes, up to Nres =
384. The cubic scaling of the self-attention logits makes it impractical to store them all for backpropagation,
as is done in naive backpropagation. Speciﬁcally, storing just the attention logits of the largest sublayer type
for all blocks requires more memory than the 16 GiB available on TPUv3 core:

3843
N 3
res

· 4

Nhead

· 48

Nblock

·

2

bytes per bﬂoat16

= 20.25 GiB.

(49)

To solve this, we leverage a standard technique called gradient checkpointing, or rematerialization [113,
114]. Concretely, during the forward pass, we store the activations that are passed between the Nblock = 48
Evoformer blocks. During the backward pass, we recompute all activations within the blocks. This means we
need to store the logits for only one layer at a time, bringing down the memory consumption to just 0.4 GiB.
Storing the intermediate pair activations requires an extra

3842
N 2
res

· 128

cz

· 48

Nlayer

·

2

bytes per bﬂoat16

= 1.7 GiB.

(50)

Rematerialization’s computational cost is equivalent to an extra forward pass through the network. Empiri-
cally, the backward pass is twice as expensive as the forward pass, so rematerialization increases the training
time by 33%.

Inference. The intermediate activations are stored in float32 format, requiring four bytes per ﬂoating
point number. During inference, we do not need to store the intermediate activations, which reduces the
memory consumption. However, we also consider proteins with dramatically larger Nres than during training.
For example, T1044 has Nres = 2180, so a single layer’s logits have the size of

21803
N 3
res

· 4

Nhead

·

4

bytes per ﬂoat32

= 154.4 GiB,

(51)

which again exceeds the memory size of modern accelerators. To reduce this burden, for each type of layer in
the network, we identify a ‘batch-like’ dimension where the computation is independent along that dimension.
We then execute the layer one ‘chunk’ at a time, meaning that only the intermediate activations for that chunk
need to be stored in memory at a given time. Thus, reducing the chunk size improves memory efﬁciency, but
at a cost of performance, as small chunks cannot fully exploit the parallelism of the hardware. In practice, we
often use a chunk size of 4, resulting in much lower memory consumption for the logits:

21802
N 2
res

· 4

Nhead

·

·

4

chunk size

4

bytes per ﬂoat32

= 0.3 GiB.

(52)

The same technique was used in a similar context in [115].

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

46

1.12 CASP14 assessment
1.12.1 Training procedure

Table 5 Training protocol for CASP14 models. The models in bold (i.e. 1.1.1 – 1.2.3) were used in the
assessment. We report the number of training samples and the training time (in days and hours) until the
best validation score. Three dots (··· ) indicate the same value as in the former column.

initial training

Model
Parameters initialized from
Number of templates Ntempl
Sequence crop size Nres
Number of sequences Nseq
Number of extra sequences Nextra_seq
Initial learning rate
Learning rate linear warm-up samples
Structural violation loss weight
“Experimentally resolved” loss weight
Training samples (·106)
Training time

1

Random

4
256
128
1024
10−3
128000

0.0
0.0
9.2
6d 6h

1.1

1.1.1

Model 1

4
···
512
···
5 · 10−4

ﬁrst ﬁne-tuning
1.2
··· Model 1.1
0
···
···
···
···
···
···
···
1.7
2d 3h

4
384
···
5120
···
···
···
···
0.3
20h

0
1.0
0.01
1.1

1d 10h

second ﬁne-tuning

1.1.2
···
···
···
···
1024
···
···
···
···
0.6

1d 13h

1.2.1

Model 1.2

0
···
···
5120
···
···
···
···
1.4
4d 1h

1.2.2
···
···
···
···
···
···
···
···
···
1.1
3d

1.2.3
···
···
···
···
1024
···
···
···
···
2.4

5d 12h

We now describe the exact training process for the models used in the CASP14 assessment. The training of
the model proceeds in three stages with exact details presented in Table 5. First, we train model 1 with a lower
crop size and number of sequences for about a week. Then, we ﬁne-tune this model with more sequences,
enabled violation losses, and a lower learning rate. For historical reasons, we also added the “experimentally
resolved” predictor (subsubsection 1.9.10) at this stage. We ﬁne-tune two models: model 1.1 uses templates,
while model 1.2 does not (the parameters pertaining to templates are dropped in this case). To obtain the ﬁnal
ﬁve models, we perform a second round of ﬁne-tuning with larger sequence crop size and more extra MSA
sequences from these two models. All CASP14 models are trained with distillation (subsection 1.3) from a
slightly earlier version of the model. Empirically, we ﬁnd that small changes in the model details have little
effect on the distillation procedure. The training process, excluding the preparation of the distillation dataset,
takes about two weeks. Since the assessment, we have found that the intermediate ﬁne-tuning stage can be
omitted without degradation of quality, and we follow the simpliﬁed training protocol (Table 4) in our ablation
studies.

We continued to improve the models during the CASP14 assessment, thus some of our submissions used
slightly different details, and some submissions had manual interventions. For more details please refer to the
short description in [116] and also an upcoming publication in “Proteins: Structure, Function, and Bioinfor-
matics”. For the accuracy comparison of the systems see subsubsection 1.12.2 below.

1.12.2 Inference and scoring

At inference time, the ﬁve models are independently executed on the same set of inputs. Then, the predictions
are re-ranked according to the chain pLDDT conﬁdence measure. The original CASP14 submissions were
inferenced with Nensemble = 8, which has been later reduced in our system.

After the CASP14 assessment we have re-evaluated the ﬁnal system described in subsubsection 1.12.1 with
Nensemble = 3 and obtained a mean domain GDT of 87.65, compared to 87.66 GDT that our actual submissions
get on the same set of domains. The median Cα RMSD at 95% coverage is 0.96 for both the described system
and the actual submissions. Furthermore, the described system without ensembling (Nensemble = 1) obtains
GDT 87.56, which is only 0.1 points less than the full system, while yielding 8 times speedup for the inference.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

47

For this evaluation, we are predicting the structure of all non-cancelled, non-server targets as full chains
(except T1085 and T1086, for which the structures are embargoed). Note that the long (2180 residues) target
T1044 structure has also been predicted as a whole chain and then split into the domain targets (T1031, T1033,
T1035, T1037, T1039–T1043) for the evaluation. We used all FM, TBM-easy, TBM-hard and FM/TBM
domains (87 in total):

T1024-D1, T1024-D2, T1026-D1, T1027-D1, T1029-D1, T1030-D1, T1030-D2, T1031-D1, T1032-D1,
T1033-D1, T1034-D1, T1035-D1, T1037-D1, T1038-D1, T1038-D2, T1039-D1, T1040-D1, T1041-D1,
T1042-D1, T1043-D1, T1045s2-D1, T1046s1-D1, T1046s2-D1, T1047s1-D1, T1047s2-D1, T1047s2-D2,
T1047s2-D3, T1049-D1, T1050-D1, T1050-D2, T1050-D3, T1052-D1, T1052-D2, T1052-D3, T1053-D1,
T1053-D2, T1054-D1, T1055-D1, T1056-D1, T1057-D1, T1058-D1, T1058-D2, T1060s2-D1, T1060s3-D1,
T1061-D1, T1061-D2, T1061-D3, T1064-D1, T1065s1-D1, T1065s2-D1, T1067-D1, T1068-D1, T1070-D1,
T1070-D2, T1070-D3, T1070-D4, T1073-D1, T1074-D1, T1076-D1, T1078-D1, T1079-D1, T1080-D1,
T1082-D1, T1083-D1, T1084-D1, T1087-D1, T1089-D1, T1090-D1, T1091-D1, T1091-D2, T1091-D3,
T1091-D4, T1092-D1, T1092-D2, T1093-D1, T1093-D2, T1093-D3, T1094-D1, T1094-D2, T1095-D1,
T1096-D1, T1096-D2, T1099-D1, T1100-D1, T1100-D2, T1101-D1, T1101-D2.

We use this set for all CASP14-related results reported in the paper.

1.13 Ablation studies
1.13.1 Architectural details

We estimate the relative importance of key components of the architecture by training and evaluating a number
of ablation models:

Baseline. Baseline model as described in the paper without noisy-student self-distillation. All other abla-

tions should be understood relative to this baseline model.

With self-distillation training. Full model as described in the paper including noisy-student self-distillation

training.

No templates. We remove the template stack and the template torsion angle features, see subsubsec-
tion 1.7.1 and Suppl. Fig. 1. The number of MSA clusters Nclust is increased by Ntempl = 4 to keep the overall
size of the MSA representation.

No raw MSA (use MSA pairwise frequencies). AlphaFold builds representations by performing attention
on individual MSA sequences. Here we ablate whether this is important and modify the system to provide
only ﬁrst and second order MSA statistics.

We denote the raw MSA data as M, where Msi is the amino acid of sequence s at position i. We also denote
the total number of sequences as S. We deﬁne the MSA proﬁle for position i as the empirical distribution of
amino acids occurring at this position:

S(cid:88)

s=1

pi(A) =

1
S

S(cid:88)

s=1

fij(A, B) =

1
S

[Msi = A],

(53)

where [ ] denotes the Iverson bracket and is equal to 1 if the expression inside is true and 0 otherwise. Further-
more we deﬁne the pairwise frequency table as the probability of observing amino acid A at position i and
amino acid B at position j:

[Msi = A][Msj = B].

(54)

We note that these type of MSA features are commonly used in the literature, e.g. by the trRosetta pipeline
[117] (with further down-weighting of redundant sequences in the averages). Usually these ﬁrst and second
order statistics are transformed by computing the inverse covariation matrix or ﬁtting a statistical model like

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

48

a Potts Model before the features are passed to the network. Here we will just use the raw features without
sequence nonlocal preprocessing and allow the model to process them as needed.

This ablation will have access to the following MSA information:

• MSA proﬁle, i.e. the marginal distribution pi(A);
• Expected number of deletions for a given position;
• MSA pairwise frequency table fij(A, B);
• Second order statistics for the number of deletions at position i and amino acid types at position j;
• Second order statistics for the number of deletions at different positions.

We implement this ablation in terms of a set of small interventions in our system. We will show that with

these interventions all provided MSA information can be directly derived from the features listed above.

In the data pipeline, we set the number of clusters Nclust = 1 so that the MSA representation only has a
single row. The ﬁrst cluster is always set to the sequence of the modelled protein. Furthermore in the clustering
stage all sequences in the MSA will be assigned to this single cluster, this means the cluster proﬁle feature is
just the full MSA proﬁle.

sei

We also remove the template torsion angle features that otherwise would be concatenated as additional
rows. Since columnwise MSA attention would be trivial in this setup, we remove this operation from the
Evoformer stack.
rithm 10) on the extra MSA features {f extra_msa_feat

Furthermore we replace the extra MSA stack (Algorithm 18) with a single outer product mean (Algo-

} and use it to initialize the pair representation.

We denote the set of features (MSA sequences and deletions) usually passed to the extra MSA stack by fsi
and analyse what information of the MSA can be accessed by the model. Applying the outer product mean

operation on fsi yields linear projections of the following kind of terms: (cid:8)(cid:80)

s fsi ⊗ fsj,(cid:80)

s fsi, (cid:80)

The ﬁrst kind of terms comes from the outer product on the linearly projected features, the other two come
from the mixed bias and features terms in the outer product, since we use linear transformations with a bias.
When writing out the MSA features explicitly, we see that the ﬁrst term contains the pairwise frequencies.
Additionally, we have the second order statistics involving deletions. The other two terms result in an outer
sum of proﬁles and expected number of deletions.

(cid:9).

No triangles, biasing, or gating (use axial attention). First, we remove the triangle inductive bias. For
this we replace the pair bias bh
ij in Algorithm 13 and Algorithm 14 with a projection of the relative distances
along the chain, i.e. relposij from Algorithm 4. We also replace the two triangular multiplicative updates
(Algorithm 11 and Algorithm 12) with two self-attention layers identical to the ones described above.

s fsj

Furthermore, we replace the pair bias in the row-wise MSA attention Algorithm 7 with the projected relative
distances relposij and remove the gating in all attention operations. After these modiﬁcations, the attention
used in the main part of the model closely matches the standard axial (or criss-cross) attention [118].

These modiﬁcations also imply that the pair representation does not inﬂuence the MSA representation
directly. Though it is important to note that we keep Algorithm 10 without any modiﬁcations, so the MSA
representation still inﬂuences the pair representation. We also do not modify the structure module, as such the
pair representation still participates in building the structure.

No recycling. We remove recycling (see subsection 1.10) during training and inference, i.e. we only make

a single pass through the model.

No invariant point attention (use direct projection). We replace the prediction of backbone frames,
including all of the IPA Algorithm 22, with a linear direct projection of the backbone frame similar to Algo-
rithm 23. The direct projection consists of linear layer applied to the ﬁnal Evoformer single embedding to
produce a Nres × 7 matrix. The seven coordinates are converted into a backbone frame in the following way:
The ﬁrst 4 coordinates (qw, qx, qy, qz) are converted to the rotation matrix by creating a normalized quaternion

(a, b, c, d) = (1 + qw, qx, qy, qz)/

(1 + qw)2 + q2

x + q2

y + q2
z ,

(55)

(cid:113)

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

49

and applying the standard formula for converting a quaternion to a rotation matrix:

a2 + b2 − c2 − d2

2bc + 2ad
2bd − 2ac

R =

2bc − 2ad

a2 − b2 + c2 − d2

2cd + 2ab

2bd + 2ac
2cd − 2ab

a2 − b2 − c2 + d2

 .

(56)

The last 3 predicted coordinates (tx, ty, tz) are used as the translation vector. We also apply the side chain
torsion module directly to the single embedding in the same manner as in the full model. In this ablation,
there is no iterative process in the Structure module. Additionally, the ﬁnal pair representation is not used in
any way in the Structure module in this ablation but is used for the distogram loss.

No invariant point attention and no recycling. This ablation simply combines the changes of the two

individual ablations described above.

No end-to-end structure gradients (keep auxiliary heads). We stop the gradients from the structure
module into the main part of the network (i.e. the structure losses have no effect on Evoformer parameters),
which means the Evoformer embeddings are trained only using the distogram loss and other auxiliary losses.
No auxiliary distogram head. We remove the auxiliary loss that predicts distograms (see subsubsec-

tion 1.9.8).

No auxiliary masked MSA head. We remove the auxiliary BERT-style loss that imputes the masked values
in the MSA (see subsubsection 1.9.9). The ablation still uses the same MSA processing, i.e. the MSA is still
randomly masked.

1.13.2 Procedure
Models training. For all ablations we kept hyperparameters from the main model conﬁguration, which we
have not re-tuned. We trained the models in two stages following the main training protocol shown in Table 4.
All models apart from one ablation were trained without the distillation dataset. Including examples from the
full AlphaFold distillation dataset would mean leaking architectural choices of the models that were used to
create this dataset, while re-creating the dataset for each individual ablation would be too computationally
expensive. For each ablation we trained 3 or 4 identical models with different random seeds and discarded
any unstable runs using the validation set from subsubsection 1.11.7.
Datasets and metrics. We used two different test sets for ablations analysis. Firstly, we used the set of
CASP14 domains described in subsubsection 1.12.2 and evaluated individual domain predictions with the
Global Distance Test (GDT) [119]. Secondly, we used the redundancy-reduced set of recent PDB chains
described in Methods, further restricted to protein chains with template coverage ≤ 30% at 30% identity
(N = 2261 targets). We scored the full chain predictions with lDDT-Cα [98].

1.13.3 Results

Ablation results are presented in Fig. 4b of the main paper. We show the difference in mean backbone accuracy
for each ablation relative to an average of baseline seeds using both the CASP14 targets and the redundancy-
and template-reduced set from recent PDB. Conﬁdence intervals for each training seed are given by bootstrap
over domains for CASP and full chains for PDB. Additionally, in Suppl. Fig. 10 we ﬁnd that the different
ablations have quite non-uniform dependence on the MSA depth. Some ablations like No Masked MSA head
primarily affect shallow MSAs whereas others such as No Recycling have effects at all MSA depths.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

50

Supplementary Figure 10 Ablations accuracy relative to the baseline for different values of the MSA depth
on the recent PDB set of chains, ﬁltered by template coverage ≤ 30% at 30% identity (N = 2261 protein
chains). MSA depth is computed by counting the number of non-gap residues for each position in the MSA
(using the Neff weighting scheme with a threshold of 80% identity measured on the region that is non-gap in
either sequence) and taking the median across residues. Plots are restricted to the range [-15, 15]; the red
lines represent LOESS mean estimates with 95% conﬁdence intervals for the LOESS.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

51

The presented ablations remove relatively independent components of the system, such as end-to-end train-
ing, iterative structure reﬁnement, triangular inductive bias, and others from the no-self-distillation baseline.
While this analysis helps determine the relative importance of different ideas in the model, there are two ma-
jor limitations. First, hyperparameters were not re-tuned when performing the ablations which could make
ablations appear more signiﬁcant than in a properly tuned model. Second, interactions between different
components may be strongly non-linear as the components of the AlphaFold system can have overlapping or
interacting roles. As a dramatic example, removal of Invariant Point Attention (IPA) from the model has only
a tiny effect on accuracy for recent PDB, but removing both recycling and IPA from the model has a much
larger effect than removing recycling alone. This double-ablation must be interpreted with caution, however,
as it is not clear which missing features of recycling (embedding of intermediate structures, much deeper
network, intermediate losses, or weight tying) explain the severe ablation.

In general, the ablations conﬁrm that many architecture and training features have an effect on ﬁnal Al-
phaFold accuracy and the relative magnitude of effects are broadly consistent whether measured on the human-
curated CASP set or the much larger recent PDB set.

The strong contributions of self-distillation training and masked MSA losses suggest that these techniques
are making effective use of unlabelled data within the supervised training framework (and despite the lack of
unsupervised pre-training). The various architecture ablations conﬁrm that many different architectural inno-
vations contribute to the ﬁnal AlphaFold accuracy. Finally, careful handling of the structure and intermediate
losses (using both end-to-end structure gradients and providing intermediate loss gradients in recycling) is
important to achieving full accuracy. This is likely due to pushing the network towards having a concrete
representation of the structure as appears to be present in the trajectories of intermediate structure predictions.
A surprisingly small role is played by the distogram head, suggesting that it is not a necessary output to
obtain high-accuracy structures. Another surprise is that equivariance in the network is not essential as the
single ablation of IPA, which removes all equivariant components, is still a very accurate structure prediction
network. Similarly, the single ablation of raw MSA leads to a drop in accuracy but it is not catastrophic.
We hypothesize though that each of these features would show much larger effects within multiple ablations
similar to the effect seen in ablating both IPA and recycling.

1.14 Network probing details

This paper includes supplementary videos with structures evolving over 4 recycling iterations and 48 Evo-
former blocks. Static versions of those videos are also presented as GDT curves per 4· 48 = 192 intermediate
points and discussed in the main part of the paper. In this section we provide additional details about how
these results were produced.

We started with a full network architecture and added additional probing modules after each Evoformer
block. Concretely, we incorporated copies of the Structure module (subsection 1.8) with the same architecture
and conﬁguration and optimized the probing loss Lprobe, deﬁned as the average of the losses of these modules.
For inputs, we provided the current versions of the MSA and pair representations produced up to this point in
the network. As with the main network, we had different trainable parameters across Evoformer blocks, but
tied across recycling iterations. In other words, we trained 48 additional sets of weights for these modules.
Note that we have not attached any probing modules to the template pair stack or the extra MSA stack. Impor-
tantly, when training the probing network, we stopped all gradients coming to the main network parameters
both from the probing and the main losses. Outputs of the probing modules have not been recycled in any
way, i.e. they were used for the analysis only.

In the presented analysis, we were probing one of the ﬁve CASP14 models (model 1.1.2 from Table 5
to be precise). We used the same data sizes as in this model, i.e. Ntempl = 4, Nres = 384, Nseq = 512,
and Nextra_msa = 1024. For the learning rate, we used the standard schedule from our training stage (see
subsubsection 1.11.3). Following the main training stage setup, we have not used violation losses (subsubsec-
tion 1.9.11) for the probing. In the evaluator (subsubsection 1.11.7) we used the validation probing loss Lprobe
for the model selection and we changed the exponential moving average decay to 0.99 for a faster training

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

52

startup. We trained the probing network for about 5 days until convergence. We used Nensemble = 1 both for
the build-in evaluator and for the presented inference to simplify setup and reduce memory usage.

1.15 Novel fold performance

Across CASP14 and CASP_Commons, ﬁve of the assessed targets were sufﬁciently distinct from prior PDB
entries to be called novel folds. Targets T1035, T1037, T1040 and T1042 (PDB:6vr4) were highlighted as
new folds by the CASP14 assessors [120], while C1905 (PDB:6xdc) is called novel in its primary citation
[121]. Here we evaluate AlphaFold’s performance on an expanded test set of recent structures selected for
their novelty.

Three heuristics were used to identify recent PDB chains that might be novel with respect to AlphaFold’s

training set:

1. The PDB abstract or structure title contains the phrase "novel fold".

2. The chain contains a Pfam domain from a clade not associated with any structure released before 2018-

04-30.

3. The chain maps to a SCOP fold not associated with any structure released before 2018-04-30.

The SIFTS resource was used to map PDB chains to Pfam and SCOP [122], with clade and fold annotations
downloaded from the respective project websites [123, 124]. Where multiple similar structures were surfaced
from the same study only one was kept, and trivial structures consisting of a single helix were also discarded.
Where it was clear from the literature that a speciﬁc domain was novel, or where the chain consisted of
multiple domains joined by an unresolved linker, the structure was cropped to a more reasonable unit for TM-
align comparison. The ﬁnal test set was selected by TM-align comparison against all chains in the training set
[103]. Any target with a TM-score greater than 0.6 was eliminated (normalising with respect to the length of
the candidate novel structure). Of the remaining 24 targets, 15 have a TM-score of less than 0.5 to the closest
training set chain.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

53

Supplementary Figure 11 Performance on a set of novel structures (N = 24 protein chains). Where the
target is not a full chain, the evaluated region is given as residue ranges in PDB numbering. 6fes_A has the
highest TM-score to a training set chain (0.57); the structure comparison shows this target (green) aligned to
its closest match with cealign [125] (5jxf_B, red, cropped for easier comparison). The bottom graph shows the
relationship between performance and full chain MSA depth, with low lDDT-Cα outliers labelled. For outlier
6w6w_B, we also show the prediction (blue) aligned to two possible ground truth structures (green): 6w6w_B
and the corresponding subsequence of 6w6w_A which strongly suggests that AlphaFold has simply produced
an alternative conformation of this sequence.

Prediction used Nensemble = 3 and disallowed templates released after 2018-04-30, but otherwise followed
our CASP14 procedure. The input to AlphaFold was the full chain fasta sequence for each target, extracted
from the relevant mmCIF ﬁle. Figure 11 shows the results. The median lDDT-Cα on this set was 90.3,
with mean lDDT-Cα 82.6. Investigating the outliers, we found that almost all could be explained by a small
MSA containing fewer than 20 sequences. In the case of 6w6w_B, the ground truth structure may be an
alternate conformation adopted by this subsequence in complex. The AlphaFold prediction closely matches
the conformation seen in 6w6w_A, which forms part of a larger domain. We conclude that AlphaFold is able
to predict novel structures well, subject to the same constraints described elsewhere in this work (a sufﬁciently
large MSA, and minimal dependence of the conformation on missing context from surrounding chains).

6w6w_B6l2w_A6j3q_06xdc_A6sp2_A6zyv_A6vr4_A6fes_A6z0u_Z6sj9_A7bqb_A6vr4_A6c70_D6fcx_A7ctp_A6vr4_A6lqn_A6vkf_A5w7l_A6bui_D6vr4_A6a51_A6xfi_A6l6h_A30405060708090lDDT-Cα(1372-1501)(172-375)(3-403)(147-319)(1-100)(1744-1996,2145-2167)(363-644)(337-369,531-901)(235-336)(52-464)101102103104Sequencesin full chain MSA30405060708090lDDT-Cα6w6w_B6w6w_A6w6w_B6xdc_A6j3q_06l2w_ASuppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

54

1.16 Visualization of attention

In this section we are going to analyse a small subset of the attention patterns we see in the main part of
the model. This will be restricted to relatively short proteins with fairly shallow MSA’s in order for easier
visualization.

We are going to examine the attention weights in the attention on the pair representation in the “Triangular
gated self-attention around starting node” (Algorithm 13). The attention pattern for a given head h is a third
order tensor ah
ijk (line 5 of Algorithm 13), here we will investigate different slices along the i axis as well as
averages along this axis, and display the jk array as a heat map. We specify the attention pattern for a speciﬁc
head by the recycling index r, layer index l and head index h. We show the attention patterns in Suppl. Fig. 12

d

a

b

c

Supplementary Figure 12 Visualization of row-wise pair attention. (a) Attention patterns in layer 0, head
0. (b) Attention patterns in layer 2, head 2. (c) Attention patterns in layer 11, head 1. (d) Predicted Cα – Cα
distances

In layer 0, head 0 (Suppl. Fig. 12a) we see a pattern that is similar to a convolution, i.e. the pair representa-
tion at (i, i + j) attends to the pair representation (i, i + k), where j and k are relatively small and the pattern
is fairly independent of i. The radius of the pattern is 4 residues, we note that the hydrogen bonded residues
in an alpha helix are exactly 4 residues apart. Patterns like this are expected in early layers due to the relative
position encoding, however one would naively expect it to be wider, as the relative positions run to 32.

In layer 2, head 2 (Suppl. Fig. 12b) we see a very speciﬁc attention pattern. We see that positions 14, 28
and 80 play a special role. These correspond to the positions of cysteine residues in the protein. We see that
at the cysteine positions the attention is localized to the other cysteine positions as exempliﬁed by the bands
in the ﬁrst two panels. Meanwhile away at positions different from the cysteine like position 20, we see that
the attention is devoid of features suggesting that the model does not use the attention at these positions. We
found this behaviour to be consistent across different positions and proteins, this possibly indicates ﬁnding
possible disulﬁdes as a key feature of some heads.

A head later in the network (Suppl. Fig. 12c) shows a rich, non-local attention pattern resembling the

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

55

distance between pairs in the structure (Suppl. Fig. 12d).

In Suppl. Fig. 13 we show a visualization of the attention pattern in the MSA along the columns ah

sti (line 4

of Algorithm 8). We slice along the last axis i and display the st array as heat map.

a

b

c

d

e

Supplementary Figure 13 Visualization of attention in the MSA along sequences. (a) Attention patterns in
layer 0, head 7. (b) Attention patterns in layer 0, head 7 in the last recycling iteration. (c) Attention patterns in
layer 6, head 6. (d) Predicted Cα – Cα distances (e) Hamming distances between the sequences of the MSA

The original MSA subset shown to the main part of the model is randomly sampled. As such the order is
random except for the ﬁrst row. The ﬁrst row is special because it contains the target sequence and is recycled
in consecutive iterations of the model. Due to the shallow MSA of this protein, the random subset leads to
a random permutation of the sequences. In order to facilitate easier interpretation of the attention patterns
here we reorder the attention tensor by using a more suitable order for the MSA. We perform a hierarchical
clustering using the Ward method with simple Hamming distance as a metric and use the output to re-index
the sequence dimension in the MSA attention tensor. We resort the indices from the hierarchical clustering
manually to keep the target sequence in the ﬁrst row. This manual sorting is done in such a way as to keep the
tree structure valid. The Hamming distances between the reordered sequences (see Suppl. Fig. 13e) show a
block-like structure quite clearly after the reordering.

The attention pattern in the ﬁrst layer of the network in the ﬁrst recycling iteration (e.g. layer 0, head 7 in
Suppl. Fig. 13a) is not very informative and is largely averaging as can be seen by looking at the range of the
attention weights.

In the same head, but in the last recycling iteration (Suppl. Fig. 13b) we see that all sequences at all positions
attend to the ﬁrst row. Therefore this head behaves differently upon recycling and is presumably important for
distribution the information in the recycled ﬁrst row to the rest of the MSA representation.

Layer 6, head 6 (Suppl. Fig. 13c) shows a pattern that is fairly common in the column-wise MSA attention,
here the pattern only varies lightly as one goes along the sequence and there is a clear structure in blocks of
sequences that attend to each other. We note that these seem somewhat similar to the blocks derived from
hierarchical clustering using Hamming distance.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

56

Whether attention patterns provide a good explanation for the behaviour of a given model or are predictive
of interventions into the model is a topic of debate in the community, see [126], [127], [128]. A detailed
analysis of the generality and predictivity of these attention patterns is beyond the scope of this paper.

1.17 Additional results

We present median all-atom RMSD95 on the CASP14 set in Suppl. Fig. 14. Figure 1 of the main paper shows
the corresponding results on just the Cα atoms (RMSD95-Cα).

Supplementary Figure 14 Median all-atom RMSD95 on the CASP14 set of protein domains (N = 87 pro-
tein domains) relative to the top-15 entries (out of 146), group numbers correspond to the numbers assigned
to entrants by CASP; error bars represent the 95% conﬁdence interval of the median, estimated with 10,000
bootstrap samples.

In addition to the main Figure 2a, we show all-atom RMSD and RMSD for 100% coverage in Suppl.
Fig. 15. We also report the quartiles of the distributions in Table 6. For this analysis, the recent PDB set
of 10795 chains described in Methods was further ﬁltered to exclude proteins with a template (identiﬁed by
hmmsearch) from the training set with more than 40% sequence identity covering more than 1% of the chain.

G427  (AlphaFold)G473G129G403G480G009G498G368G488G032G362G420G324G042G253012345Median RMSD95-All atom in ÅSuppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

57

Supplementary Figure 15 Histograms of all-atom and backbone RMSD at 95% and 100% coverage for
full-chain predictions on the template-reduced set (N = 3144 protein chains). Error bars are 95% conﬁdence
intervals (Poisson).

Table 6 Quartiles of distributions of all-atom and backbone RMSD at 95% and 100% coverage for full-chain
predictions on the template-reduced set (N = 3144 protein chains).

Quantity (in Å)
RMSD - Cα
RMSD - All Atom.
RMSD95 - Cα
RMSD95 - All Atom

lower quart. median
2.32
2.80
1.46
1.89

1.31
1.80
0.79
1.17

upper quart.

6.49
6.78
4.33
4.72

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

58

References

[85] Jimmy Ba, Jamie R Kiros, and Geoffrey E Hinton.

Layer normalization.

arXiv preprint

arXiv:1607.06450, 2016.

[86] L Steven Johnson, Sean R Eddy, and Elon Portugaly. Hidden markov model speed heuristic and iterative

hmm search procedure. BMC Bioinformatics, 11(1):1–8, 2010.

[87] Michael Remmert, Andreas Biegert, Andreas Hauser, and Johannes Söding. Hhblits:

lightning-fast

iterative protein sequence searching by hmm-hmm alignment. Nature Methods, 9(2):173–175, 2012.

[88] Alex L Mitchell, Alexandre Almeida, Martin Beracochea, Miguel Boland, Josephine Burgin, Guy
Cochrane, Michael R Crusoe, Varsha Kale, Simon C Potter, Lorna J Richardson, Ekaterina Sakharova,
Maxim Scheremetjew, Anton Korobeynikov, Alex Shlemov, Olga Kunyavskaya, Alla Lapidus, and
the microbiome analysis resource in 2020. Nucleic Acids Research,
Robert D Finn. MGnify:
48(D1):D570–D578, 11 2019.

[89] Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium.
Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches.
Bioinformatics, 31(6):926–932, 2015.

[90] Milot Mirdita, Lars von den Driesch, Clovis Galiez, Maria J Martin, Johannes Söding, and Martin
Steinegger. Uniclust databases of clustered and deeply annotated protein sequences and alignments.
Nucleic Acids Research, 45(D1):D170–D176, 2017.

[91] Martin Steinegger, Markus Meier, Milot Mirdita, Harald Vöhringer, Stephan J Haunsberger, and Jo-
hannes Söding. Hh-suite3 for fast remote homology detection and deep protein annotation. BMC Bioin-
formatics, 20(1):1–15, 2019.

[92] Martin Steinegger and Johannes Söding. Clustering huge protein sequence sets in linear time. Nature

Communications, 9(1):1–8, 2018.

[93] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves
imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10687–10698, 2020.

[94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
In 31st Conference on Neural Information

Kaiser, and Illia Polosukhin. Attention is all you need.
Processing Systems (NeurIPS 2017), pages 6000–6010, 2017.

[95] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics, pages 2978–2988, Florence, Italy,
July 2019. Association for Computational Linguistics.

[96] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine Learn-
ing Research, 15(1):1929–1958, 2014.

[97] RA Engh and R Huber. Structure quality and target parameters. In International Tables for Crystallog-

raphy, Vol. F. John Wiley & Sons, Ltd, 2006.

[98] Valerio Mariani, Marco Biasini, Alessandro Barbato, and Torsten Schwede. lddt: A local superposition-
free score for comparing protein structures and models using distance difference tests. Bioinformatics,
29(21):2722–2728, 2013.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

59

[99] Viktor Hornak, Robert Abel, Asim Okur, Bentley Strockbine, Adrian Roitberg, and Carlos Simmerling.
Comparison of multiple amber force ﬁelds and development of improved protein backbone parameters.
Proteins: Structure, Function, and Bioinformatics, 65(3):712–725, 2006.

[100] Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp,
Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, Rafal P Wiewiora,
Bernard R Brooks, and Vijay S Pande. Openmm 7: Rapid development of high performance algorithms
for molecular dynamics. PLOS Computational Biology, 13(7):1–17, 07 2017.

[101] Mohammed AlQuraishi. End-to-End Differentiable Learning of Protein Structure. Cell Systems,

8(4):292–301.e3, 24 April 2019.

[102] Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure

template quality. Proteins: Structure, Function, and Bioinformatics, 57(4):702–710, 2004.

[103] Yang Zhang and Jeffrey Skolnick. TM-align: a protein structure alignment algorithm based on the

TM-score. Nucleic acids research, 33(7):2302–2309, 2005.

[104] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi-

rectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[105] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of

the International Conference on Learning Representations, 2015.

[106] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
In Proceedings of the International Conference on Machine Learning, pages 1310–1318,

networks.
2013.

[107] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efﬁcient backprop. In Neural Networks: Tricks of the

Trade. Springer, 1998.

[108] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpass-
In Proceedings of the IEEE Conference on

ing human-level performance on imagenet classiﬁcation.
Computer Vision, pages 1026–1034, 2015.

[109] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neu-
ral networks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, pages 249–256, 2010.

[110] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour.
arXiv preprint arXiv:1706.02677, 2017.

[111] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM

Journal on Control and Optimization, 30(4):838–855, 1992.

[112] Jürgen Haas, Alessandro Barbato, Dario Behringer, Gabriel Studer, Steven Roth, Martino Bertoni,
Khaled Mostaguir, Rafal Gumienny, and Torsten Schwede. Continuous automated model evaluation
(cameo) complementing the critical assessment of structure prediction in casp12. Proteins: Structure,
Function, and Bioinformatics, 86:387–398, 2018.

[113] Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of checkpointing
for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical
Software (TOMS), 26(1):19–45, 2000.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

60

[114] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory

cost. arXiv preprint arXiv:1604.06174, 2016.

[115] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In Proceed-

ings of the International Conference on Learning Representations, 2020.

[116] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasu-
vunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon
A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav
Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger,
Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet
Kohli, and Demis Hassabis. High accuracy protein structure prediction using deep learning. In Four-
teenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book), pages 22–24,
2020.

[117] Jianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and David
Improved protein structure prediction using predicted interresidue orientations. Proceedings
Baker.
of the National Academy of Sciences of the United States of America, 117(3):1496–1503, 21 January
2020.

[118] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Cc-
In Proceedings of the IEEE/CVF International

net: Criss-cross attention for semantic segmentation.
Conference on Computer Vision, pages 603–612, 2019.

[119] Adam Zemla. LGA – a method for ﬁnding 3d similarities in protein structures. Nucleic Acids Research,

31:3370–4, 08 2003.

[120] Lisa Kinch, Andriy Kryshtafovych, and Nick Grishin. Target classiﬁcation in the 14th round of the
critical assessment of protein structure prediction (CASP14). https://predictioncenter.
org/casp14/doc/presentations/2020_11_30_TargetClassification_Kinch_
Updated.pdf, 2020. [Online; accessed 08-June-2021].

[121] David M. Kern, Ben Sorum, Sonali S. Mali, Christopher M. Hoel, Savitha Sridharan, Jonathan P.
Remis, Daniel B. Toso, Abhay Kotecha, Diana M. Bautista, and Stephen G. Brohawn. Cryo-
bioRxiv preprint
EM structure of
bioRxiv:10.1101/2020.06.17.156554v3, 2021.

the SARS-CoV-2 3a ion channel

in lipid nanodiscs.

[122] Jose M Dana, Aleksandras Gutmanas, Nidhi Tyagi, Guoying Qi, Claire O’Donovan, Maria Martin, and
Sameer Velankar. SIFTS: updated structure integration with function, taxonomy and sequences resource
allows 40-fold increase in coverage of structure-based annotations for proteins. Nucleic acids research,
47(D1):D482–D489, 2019.

[123] Jaina Mistry, Sara Chuguransky, Lowri Williams, Matloob Qureshi, Gustavo A Salazar, Erik LL
Sonnhammer, Silvio CE Tosatto, Lisanna Paladin, Shriya Raj, Lorna J Richardson, et al. Pfam: The
protein families database in 2021. Nucleic Acids Research, 49(D1):D412–D419, 2021.

[124] Antonina Andreeva, Eugene Kulesha, Julian Gough, and Alexey G Murzin. The SCOP database in
2020: expanded classiﬁcation of representative family and superfamily domains of known protein struc-
tures. Nucleic acids research, 48(D1):D376–D382, 2020.

[125] Ilya N Shindyalov and Philip E Bourne. Protein structure alignment by incremental combinatorial

extension (CE) of the optimal path. Protein engineering, 11(9):739–747, 1998.

Suppl. Material for Jumper et al. (2021): Highly accurate protein structure prediction with AlphaFold

61

[126] Sarthak Jain and Byron C Wallace. Attention is not Explanation. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pages 3543–3556, Minneapolis, Minnesota, June 2019.
Association for Computational Linguistics.

[127] Soﬁa Serrano and Noah A Smith.

In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pages 2931–2951, Florence, Italy, July 2019.
Association for Computational Linguistics.

Is attention interpretable?

[128] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pages 11–20, Hong Kong, China, November 2019.
Association for Computational Linguistics.

